{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCoCOwCk6aZQ",
        "outputId": "95bdb6a0-3824-4bee-8916-e46e1336f0b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/My Drive/Session_12/input.txt\"\n",
        "\n",
        "with open(path, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "#print(text)"
      ],
      "metadata": {
        "id": "Eog18MHl6emQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solving for residual std scaling issue\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "kZec8ePh6efI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0 # Here config.n_embd means the embedding size of the model for GPT 2 it's 1024 and it has to be divided by the NUmber of\n",
        "        # heads we have created\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) #context Vector\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "Y-IKv6Ta66tY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "NuJxKntu66j4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n"
      ],
      "metadata": {
        "id": "1EHGaFQD7IG2"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n"
      ],
      "metadata": {
        "id": "pbiKuvnW7H_K"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        #print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        #print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "# model = GPT.from_pretrained('gpt2')\n"
      ],
      "metadata": {
        "id": "DDlKD5tp7TKM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NpmtQ6qmID0B"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn.functional as F # Added this import\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# SEED\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# STOP\n",
        "num_return_sequences = 4 # Changed from 5 to 4 to match batch size\n",
        "max_length = 30\n",
        "\n",
        "# Define enc globally so it's accessible for decoding\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # at init load tokens from disk and store them in memory\n",
        "        with open('/content/drive/My Drive/Session_12/input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "        # enc is now defined globally, no need to redefine here\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f'loaded {len(self.tokens)} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
        "\n",
        "        # state\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T) # inputs\n",
        "        y = (buf[1:]).view(B, T) # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B*T\n",
        "        # if loading the next batch would be out of bounds, reset\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n",
        "\n",
        "\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "\n",
        "train_loader = DataLoaderLite(B = 8, T = 256)\n",
        "\n",
        "learning_rate = 6e-4 # max learning rate\n",
        "max_iters = 6000 # total number of training iterations\n",
        "weight_decay = 0.1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "\n",
        "# NEW CODE\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device)\n",
        "\n",
        "max_lr = 6e-4\n",
        "warmup_steps = 300\n",
        "max_steps = 6000\n",
        "#min_lr = max_lr * 0.1   # don't kill learning completel\n",
        "min_lr = 3e-5\n",
        "\n",
        "def get_lr(step):\n",
        "    if step < warmup_steps:\n",
        "        return max_lr * (step + 1) / warmup_steps\n",
        "\n",
        "    if step > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "\n",
        "    progress = (step - warmup_steps) / (max_steps - warmup_steps)\n",
        "    return min_lr + 0.5 * (base_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "\n",
        "\n",
        "TARGET_LOSS = 0.0999\n",
        "#MAX_STEPS = 5000\n",
        "\n",
        "for step in range(max_steps):\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(x, y)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.3)\n",
        "    optimizer.step()\n",
        "    #scheduler.step(loss)\n",
        "\n",
        "    #print(f'step{i}, loss: {loss.item()}')\n",
        "    print(\n",
        "        f\"step {step} | loss: {loss.item():.6f} | lr: {optimizer.param_groups[0]['lr']:.6e}\"\n",
        "    )\n",
        "\n",
        "    if loss.item() <= TARGET_LOSS:\n",
        "        print(f\"\\nâœ… Target loss {TARGET_LOSS} reached at step {step}\")\n",
        "        break\n",
        "\n",
        "\n",
        "print(\"The Final loss is : \", loss)\n",
        "# import sys; sys.exit(0) # Commented out the exit statement to allow text generation\n",
        "\n",
        "# Removed torch.manual_seed(42) and torch.cuda.manual_seed(42) as they are set above.\n",
        "while x.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)[0] # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        # note: multinomial does not demand the input to sum to 1\n",
        "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequence\n",
        "        x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpbd2nlG79SP",
        "outputId": "9260e85c-3eec-4684-c19c-088eaa1df797"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "loaded 338025 tokens\n",
            "1 epoch = 165 batches\n",
            "using fused AdamW: True\n",
            "step 0 | loss: 10.952678 | lr: 2.000000e-06\n",
            "step 1 | loss: 10.735683 | lr: 4.000000e-06\n",
            "step 2 | loss: 10.394582 | lr: 6.000000e-06\n",
            "step 3 | loss: 9.997029 | lr: 8.000000e-06\n",
            "step 4 | loss: 9.763290 | lr: 1.000000e-05\n",
            "step 5 | loss: 9.547617 | lr: 1.200000e-05\n",
            "step 6 | loss: 9.279687 | lr: 1.400000e-05\n",
            "step 7 | loss: 9.256462 | lr: 1.600000e-05\n",
            "step 8 | loss: 9.056387 | lr: 1.800000e-05\n",
            "step 9 | loss: 8.919899 | lr: 2.000000e-05\n",
            "step 10 | loss: 8.816993 | lr: 2.200000e-05\n",
            "step 11 | loss: 8.514141 | lr: 2.400000e-05\n",
            "step 12 | loss: 8.760580 | lr: 2.600000e-05\n",
            "step 13 | loss: 8.661093 | lr: 2.800000e-05\n",
            "step 14 | loss: 8.753067 | lr: 3.000000e-05\n",
            "step 15 | loss: 8.537802 | lr: 3.200000e-05\n",
            "step 16 | loss: 8.878621 | lr: 3.400000e-05\n",
            "step 17 | loss: 8.432484 | lr: 3.600000e-05\n",
            "step 18 | loss: 8.579928 | lr: 3.800000e-05\n",
            "step 19 | loss: 8.432632 | lr: 4.000000e-05\n",
            "step 20 | loss: 8.682561 | lr: 4.200000e-05\n",
            "step 21 | loss: 8.476622 | lr: 4.400000e-05\n",
            "step 22 | loss: 8.329946 | lr: 4.600000e-05\n",
            "step 23 | loss: 8.630370 | lr: 4.800000e-05\n",
            "step 24 | loss: 8.508455 | lr: 5.000000e-05\n",
            "step 25 | loss: 8.378172 | lr: 5.200000e-05\n",
            "step 26 | loss: 8.367620 | lr: 5.400000e-05\n",
            "step 27 | loss: 8.419027 | lr: 5.600000e-05\n",
            "step 28 | loss: 8.229296 | lr: 5.800000e-05\n",
            "step 29 | loss: 8.077944 | lr: 6.000000e-05\n",
            "step 30 | loss: 8.128019 | lr: 6.200000e-05\n",
            "step 31 | loss: 8.110236 | lr: 6.400000e-05\n",
            "step 32 | loss: 7.896372 | lr: 6.600000e-05\n",
            "step 33 | loss: 7.760972 | lr: 6.800000e-05\n",
            "step 34 | loss: 7.803289 | lr: 7.000000e-05\n",
            "step 35 | loss: 7.680116 | lr: 7.200000e-05\n",
            "step 36 | loss: 7.743730 | lr: 7.400000e-05\n",
            "step 37 | loss: 7.730922 | lr: 7.600000e-05\n",
            "step 38 | loss: 7.625343 | lr: 7.800000e-05\n",
            "step 39 | loss: 7.592612 | lr: 8.000000e-05\n",
            "step 40 | loss: 7.887467 | lr: 8.200000e-05\n",
            "step 41 | loss: 7.639399 | lr: 8.400000e-05\n",
            "step 42 | loss: 7.488070 | lr: 8.600000e-05\n",
            "step 43 | loss: 7.308876 | lr: 8.800000e-05\n",
            "step 44 | loss: 7.495938 | lr: 9.000000e-05\n",
            "step 45 | loss: 7.332554 | lr: 9.200000e-05\n",
            "step 46 | loss: 7.377761 | lr: 9.400000e-05\n",
            "step 47 | loss: 7.554124 | lr: 9.600000e-05\n",
            "step 48 | loss: 7.431621 | lr: 9.800000e-05\n",
            "step 49 | loss: 7.421218 | lr: 1.000000e-04\n",
            "step 50 | loss: 7.294879 | lr: 1.020000e-04\n",
            "step 51 | loss: 7.245803 | lr: 1.040000e-04\n",
            "step 52 | loss: 7.165772 | lr: 1.060000e-04\n",
            "step 53 | loss: 6.884074 | lr: 1.080000e-04\n",
            "step 54 | loss: 6.957965 | lr: 1.100000e-04\n",
            "step 55 | loss: 7.068331 | lr: 1.120000e-04\n",
            "step 56 | loss: 6.972507 | lr: 1.140000e-04\n",
            "step 57 | loss: 6.815949 | lr: 1.160000e-04\n",
            "step 58 | loss: 6.714267 | lr: 1.180000e-04\n",
            "step 59 | loss: 6.662627 | lr: 1.200000e-04\n",
            "step 60 | loss: 6.509266 | lr: 1.220000e-04\n",
            "step 61 | loss: 6.480451 | lr: 1.240000e-04\n",
            "step 62 | loss: 6.229714 | lr: 1.260000e-04\n",
            "step 63 | loss: 6.454751 | lr: 1.280000e-04\n",
            "step 64 | loss: 6.350942 | lr: 1.300000e-04\n",
            "step 65 | loss: 6.471335 | lr: 1.320000e-04\n",
            "step 66 | loss: 6.555218 | lr: 1.340000e-04\n",
            "step 67 | loss: 6.438116 | lr: 1.360000e-04\n",
            "step 68 | loss: 6.592320 | lr: 1.380000e-04\n",
            "step 69 | loss: 6.292768 | lr: 1.400000e-04\n",
            "step 70 | loss: 6.072538 | lr: 1.420000e-04\n",
            "step 71 | loss: 6.654796 | lr: 1.440000e-04\n",
            "step 72 | loss: 6.084942 | lr: 1.460000e-04\n",
            "step 73 | loss: 6.095981 | lr: 1.480000e-04\n",
            "step 74 | loss: 6.256935 | lr: 1.500000e-04\n",
            "step 75 | loss: 6.358267 | lr: 1.520000e-04\n",
            "step 76 | loss: 6.227060 | lr: 1.540000e-04\n",
            "step 77 | loss: 5.925667 | lr: 1.560000e-04\n",
            "step 78 | loss: 5.825319 | lr: 1.580000e-04\n",
            "step 79 | loss: 5.808476 | lr: 1.600000e-04\n",
            "step 80 | loss: 5.931949 | lr: 1.620000e-04\n",
            "step 81 | loss: 6.055044 | lr: 1.640000e-04\n",
            "step 82 | loss: 6.240872 | lr: 1.660000e-04\n",
            "step 83 | loss: 5.951040 | lr: 1.680000e-04\n",
            "step 84 | loss: 6.001912 | lr: 1.700000e-04\n",
            "step 85 | loss: 6.000422 | lr: 1.720000e-04\n",
            "step 86 | loss: 5.893530 | lr: 1.740000e-04\n",
            "step 87 | loss: 6.088363 | lr: 1.760000e-04\n",
            "step 88 | loss: 6.102476 | lr: 1.780000e-04\n",
            "step 89 | loss: 6.146380 | lr: 1.800000e-04\n",
            "step 90 | loss: 5.887933 | lr: 1.820000e-04\n",
            "step 91 | loss: 6.027860 | lr: 1.840000e-04\n",
            "step 92 | loss: 5.917430 | lr: 1.860000e-04\n",
            "step 93 | loss: 5.670720 | lr: 1.880000e-04\n",
            "step 94 | loss: 5.609944 | lr: 1.900000e-04\n",
            "step 95 | loss: 6.140987 | lr: 1.920000e-04\n",
            "step 96 | loss: 5.839609 | lr: 1.940000e-04\n",
            "step 97 | loss: 5.861505 | lr: 1.960000e-04\n",
            "step 98 | loss: 5.709266 | lr: 1.980000e-04\n",
            "step 99 | loss: 5.633712 | lr: 2.000000e-04\n",
            "step 100 | loss: 5.711755 | lr: 2.020000e-04\n",
            "step 101 | loss: 5.814464 | lr: 2.040000e-04\n",
            "step 102 | loss: 5.384031 | lr: 2.060000e-04\n",
            "step 103 | loss: 5.854998 | lr: 2.080000e-04\n",
            "step 104 | loss: 6.327729 | lr: 2.100000e-04\n",
            "step 105 | loss: 6.264600 | lr: 2.120000e-04\n",
            "step 106 | loss: 6.256178 | lr: 2.140000e-04\n",
            "step 107 | loss: 6.058600 | lr: 2.160000e-04\n",
            "step 108 | loss: 6.035411 | lr: 2.180000e-04\n",
            "step 109 | loss: 5.844415 | lr: 2.200000e-04\n",
            "step 110 | loss: 5.988154 | lr: 2.220000e-04\n",
            "step 111 | loss: 5.963978 | lr: 2.240000e-04\n",
            "step 112 | loss: 6.236854 | lr: 2.260000e-04\n",
            "step 113 | loss: 6.409603 | lr: 2.280000e-04\n",
            "step 114 | loss: 6.161629 | lr: 2.300000e-04\n",
            "step 115 | loss: 6.324376 | lr: 2.320000e-04\n",
            "step 116 | loss: 5.904494 | lr: 2.340000e-04\n",
            "step 117 | loss: 5.821459 | lr: 2.360000e-04\n",
            "step 118 | loss: 6.017590 | lr: 2.380000e-04\n",
            "step 119 | loss: 5.871311 | lr: 2.400000e-04\n",
            "step 120 | loss: 5.625677 | lr: 2.420000e-04\n",
            "step 121 | loss: 6.046941 | lr: 2.440000e-04\n",
            "step 122 | loss: 5.761626 | lr: 2.460000e-04\n",
            "step 123 | loss: 5.746159 | lr: 2.480000e-04\n",
            "step 124 | loss: 6.063465 | lr: 2.500000e-04\n",
            "step 125 | loss: 6.139040 | lr: 2.520000e-04\n",
            "step 126 | loss: 5.891066 | lr: 2.540000e-04\n",
            "step 127 | loss: 5.766724 | lr: 2.560000e-04\n",
            "step 128 | loss: 5.440093 | lr: 2.580000e-04\n",
            "step 129 | loss: 5.812539 | lr: 2.600000e-04\n",
            "step 130 | loss: 5.574929 | lr: 2.620000e-04\n",
            "step 131 | loss: 5.925515 | lr: 2.640000e-04\n",
            "step 132 | loss: 6.209690 | lr: 2.660000e-04\n",
            "step 133 | loss: 5.805369 | lr: 2.680000e-04\n",
            "step 134 | loss: 5.946664 | lr: 2.700000e-04\n",
            "step 135 | loss: 5.582045 | lr: 2.720000e-04\n",
            "step 136 | loss: 5.816128 | lr: 2.740000e-04\n",
            "step 137 | loss: 5.534107 | lr: 2.760000e-04\n",
            "step 138 | loss: 5.363279 | lr: 2.780000e-04\n",
            "step 139 | loss: 5.408572 | lr: 2.800000e-04\n",
            "step 140 | loss: 5.194471 | lr: 2.820000e-04\n",
            "step 141 | loss: 5.269840 | lr: 2.840000e-04\n",
            "step 142 | loss: 6.144345 | lr: 2.860000e-04\n",
            "step 143 | loss: 5.829527 | lr: 2.880000e-04\n",
            "step 144 | loss: 5.873281 | lr: 2.900000e-04\n",
            "step 145 | loss: 5.934974 | lr: 2.920000e-04\n",
            "step 146 | loss: 5.627017 | lr: 2.940000e-04\n",
            "step 147 | loss: 5.686011 | lr: 2.960000e-04\n",
            "step 148 | loss: 5.519984 | lr: 2.980000e-04\n",
            "step 149 | loss: 5.704781 | lr: 3.000000e-04\n",
            "step 150 | loss: 5.834864 | lr: 3.020000e-04\n",
            "step 151 | loss: 5.646695 | lr: 3.040000e-04\n",
            "step 152 | loss: 5.599175 | lr: 3.060000e-04\n",
            "step 153 | loss: 5.530397 | lr: 3.080000e-04\n",
            "step 154 | loss: 5.223468 | lr: 3.100000e-04\n",
            "step 155 | loss: 5.199944 | lr: 3.120000e-04\n",
            "step 156 | loss: 5.243874 | lr: 3.140000e-04\n",
            "step 157 | loss: 5.090852 | lr: 3.160000e-04\n",
            "step 158 | loss: 4.724879 | lr: 3.180000e-04\n",
            "step 159 | loss: 5.502460 | lr: 3.200000e-04\n",
            "step 160 | loss: 6.090932 | lr: 3.220000e-04\n",
            "step 161 | loss: 6.091752 | lr: 3.240000e-04\n",
            "step 162 | loss: 6.057486 | lr: 3.260000e-04\n",
            "step 163 | loss: 5.639350 | lr: 3.280000e-04\n",
            "step 164 | loss: 5.716042 | lr: 3.300000e-04\n",
            "step 165 | loss: 5.862036 | lr: 3.320000e-04\n",
            "step 166 | loss: 5.982337 | lr: 3.340000e-04\n",
            "step 167 | loss: 5.877460 | lr: 3.360000e-04\n",
            "step 168 | loss: 5.759141 | lr: 3.380000e-04\n",
            "step 169 | loss: 5.731028 | lr: 3.400000e-04\n",
            "step 170 | loss: 5.812415 | lr: 3.420000e-04\n",
            "step 171 | loss: 5.655187 | lr: 3.440000e-04\n",
            "step 172 | loss: 5.684257 | lr: 3.460000e-04\n",
            "step 173 | loss: 5.376641 | lr: 3.480000e-04\n",
            "step 174 | loss: 5.323598 | lr: 3.500000e-04\n",
            "step 175 | loss: 5.248651 | lr: 3.520000e-04\n",
            "step 176 | loss: 4.863637 | lr: 3.540000e-04\n",
            "step 177 | loss: 5.117144 | lr: 3.560000e-04\n",
            "step 178 | loss: 4.993553 | lr: 3.580000e-04\n",
            "step 179 | loss: 5.058407 | lr: 3.600000e-04\n",
            "step 180 | loss: 5.112479 | lr: 3.620000e-04\n",
            "step 181 | loss: 5.424302 | lr: 3.640000e-04\n",
            "step 182 | loss: 5.087461 | lr: 3.660000e-04\n",
            "step 183 | loss: 5.132483 | lr: 3.680000e-04\n",
            "step 184 | loss: 5.062881 | lr: 3.700000e-04\n",
            "step 185 | loss: 5.403605 | lr: 3.720000e-04\n",
            "step 186 | loss: 5.284188 | lr: 3.740000e-04\n",
            "step 187 | loss: 5.278692 | lr: 3.760000e-04\n",
            "step 188 | loss: 5.783587 | lr: 3.780000e-04\n",
            "step 189 | loss: 5.782982 | lr: 3.800000e-04\n",
            "step 190 | loss: 5.533468 | lr: 3.820000e-04\n",
            "step 191 | loss: 5.589943 | lr: 3.840000e-04\n",
            "step 192 | loss: 5.500801 | lr: 3.860000e-04\n",
            "step 193 | loss: 5.583146 | lr: 3.880000e-04\n",
            "step 194 | loss: 5.261830 | lr: 3.900000e-04\n",
            "step 195 | loss: 5.380318 | lr: 3.920000e-04\n",
            "step 196 | loss: 5.379190 | lr: 3.940000e-04\n",
            "step 197 | loss: 5.176537 | lr: 3.960000e-04\n",
            "step 198 | loss: 4.921242 | lr: 3.980000e-04\n",
            "step 199 | loss: 5.111841 | lr: 4.000000e-04\n",
            "step 200 | loss: 5.007124 | lr: 4.020000e-04\n",
            "step 201 | loss: 5.321759 | lr: 4.040000e-04\n",
            "step 202 | loss: 5.327573 | lr: 4.060000e-04\n",
            "step 203 | loss: 5.113592 | lr: 4.080000e-04\n",
            "step 204 | loss: 5.218825 | lr: 4.100000e-04\n",
            "step 205 | loss: 5.603867 | lr: 4.120000e-04\n",
            "step 206 | loss: 5.219559 | lr: 4.140000e-04\n",
            "step 207 | loss: 4.939181 | lr: 4.160000e-04\n",
            "step 208 | loss: 5.032782 | lr: 4.180000e-04\n",
            "step 209 | loss: 5.546731 | lr: 4.200000e-04\n",
            "step 210 | loss: 5.348804 | lr: 4.220000e-04\n",
            "step 211 | loss: 5.368443 | lr: 4.240000e-04\n",
            "step 212 | loss: 5.680755 | lr: 4.260000e-04\n",
            "step 213 | loss: 5.678948 | lr: 4.280000e-04\n",
            "step 214 | loss: 5.652284 | lr: 4.300000e-04\n",
            "step 215 | loss: 5.528341 | lr: 4.320000e-04\n",
            "step 216 | loss: 5.561277 | lr: 4.340000e-04\n",
            "step 217 | loss: 5.627516 | lr: 4.360000e-04\n",
            "step 218 | loss: 5.323837 | lr: 4.380000e-04\n",
            "step 219 | loss: 5.276708 | lr: 4.400000e-04\n",
            "step 220 | loss: 5.589654 | lr: 4.420000e-04\n",
            "step 221 | loss: 5.417095 | lr: 4.440000e-04\n",
            "step 222 | loss: 5.296833 | lr: 4.460000e-04\n",
            "step 223 | loss: 5.332132 | lr: 4.480000e-04\n",
            "step 224 | loss: 5.203313 | lr: 4.500000e-04\n",
            "step 225 | loss: 5.031508 | lr: 4.520000e-04\n",
            "step 226 | loss: 5.002678 | lr: 4.540000e-04\n",
            "step 227 | loss: 4.479985 | lr: 4.560000e-04\n",
            "step 228 | loss: 5.239563 | lr: 4.580000e-04\n",
            "step 229 | loss: 5.260144 | lr: 4.600000e-04\n",
            "step 230 | loss: 5.501053 | lr: 4.620000e-04\n",
            "step 231 | loss: 5.697698 | lr: 4.640000e-04\n",
            "step 232 | loss: 5.451143 | lr: 4.660000e-04\n",
            "step 233 | loss: 5.703076 | lr: 4.680000e-04\n",
            "step 234 | loss: 5.319320 | lr: 4.700000e-04\n",
            "step 235 | loss: 5.069613 | lr: 4.720000e-04\n",
            "step 236 | loss: 5.717848 | lr: 4.740000e-04\n",
            "step 237 | loss: 4.984662 | lr: 4.760000e-04\n",
            "step 238 | loss: 5.202773 | lr: 4.780000e-04\n",
            "step 239 | loss: 5.161480 | lr: 4.800000e-04\n",
            "step 240 | loss: 5.541761 | lr: 4.820000e-04\n",
            "step 241 | loss: 5.243393 | lr: 4.840000e-04\n",
            "step 242 | loss: 5.052619 | lr: 4.860000e-04\n",
            "step 243 | loss: 4.887836 | lr: 4.880000e-04\n",
            "step 244 | loss: 4.930421 | lr: 4.900000e-04\n",
            "step 245 | loss: 5.005321 | lr: 4.920000e-04\n",
            "step 246 | loss: 5.232244 | lr: 4.940000e-04\n",
            "step 247 | loss: 5.443367 | lr: 4.960000e-04\n",
            "step 248 | loss: 5.117636 | lr: 4.980000e-04\n",
            "step 249 | loss: 5.224896 | lr: 5.000000e-04\n",
            "step 250 | loss: 5.223982 | lr: 5.020000e-04\n",
            "step 251 | loss: 5.063502 | lr: 5.040000e-04\n",
            "step 252 | loss: 5.343163 | lr: 5.060000e-04\n",
            "step 253 | loss: 5.414174 | lr: 5.080000e-04\n",
            "step 254 | loss: 5.433360 | lr: 5.100000e-04\n",
            "step 255 | loss: 5.046571 | lr: 5.120000e-04\n",
            "step 256 | loss: 5.353241 | lr: 5.140000e-04\n",
            "step 257 | loss: 5.138692 | lr: 5.160000e-04\n",
            "step 258 | loss: 4.904943 | lr: 5.180000e-04\n",
            "step 259 | loss: 4.749393 | lr: 5.200000e-04\n",
            "step 260 | loss: 5.364451 | lr: 5.220000e-04\n",
            "step 261 | loss: 4.986231 | lr: 5.240000e-04\n",
            "step 262 | loss: 5.075374 | lr: 5.260000e-04\n",
            "step 263 | loss: 4.911083 | lr: 5.280000e-04\n",
            "step 264 | loss: 4.778922 | lr: 5.300000e-04\n",
            "step 265 | loss: 4.836297 | lr: 5.320000e-04\n",
            "step 266 | loss: 5.027276 | lr: 5.340000e-04\n",
            "step 267 | loss: 4.550742 | lr: 5.360000e-04\n",
            "step 268 | loss: 5.114238 | lr: 5.380000e-04\n",
            "step 269 | loss: 5.706759 | lr: 5.400000e-04\n",
            "step 270 | loss: 5.677655 | lr: 5.420000e-04\n",
            "step 271 | loss: 5.582932 | lr: 5.440000e-04\n",
            "step 272 | loss: 5.457557 | lr: 5.460000e-04\n",
            "step 273 | loss: 5.367589 | lr: 5.480000e-04\n",
            "step 274 | loss: 5.092689 | lr: 5.500000e-04\n",
            "step 275 | loss: 5.309432 | lr: 5.520000e-04\n",
            "step 276 | loss: 5.286133 | lr: 5.540000e-04\n",
            "step 277 | loss: 5.595828 | lr: 5.560000e-04\n",
            "step 278 | loss: 5.752708 | lr: 5.580000e-04\n",
            "step 279 | loss: 5.412883 | lr: 5.600000e-04\n",
            "step 280 | loss: 5.647662 | lr: 5.620000e-04\n",
            "step 281 | loss: 5.242655 | lr: 5.640000e-04\n",
            "step 282 | loss: 5.168973 | lr: 5.660000e-04\n",
            "step 283 | loss: 5.206013 | lr: 5.680000e-04\n",
            "step 284 | loss: 5.136674 | lr: 5.700000e-04\n",
            "step 285 | loss: 5.095152 | lr: 5.720000e-04\n",
            "step 286 | loss: 5.374798 | lr: 5.740000e-04\n",
            "step 287 | loss: 5.149177 | lr: 5.760000e-04\n",
            "step 288 | loss: 5.131660 | lr: 5.780000e-04\n",
            "step 289 | loss: 5.321884 | lr: 5.800000e-04\n",
            "step 290 | loss: 5.481638 | lr: 5.820000e-04\n",
            "step 291 | loss: 5.257964 | lr: 5.840000e-04\n",
            "step 292 | loss: 5.040663 | lr: 5.860000e-04\n",
            "step 293 | loss: 4.696957 | lr: 5.880000e-04\n",
            "step 294 | loss: 5.035401 | lr: 5.900000e-04\n",
            "step 295 | loss: 4.867445 | lr: 5.920000e-04\n",
            "step 296 | loss: 5.015998 | lr: 5.940000e-04\n",
            "step 297 | loss: 5.273002 | lr: 5.960000e-04\n",
            "step 298 | loss: 4.802442 | lr: 5.980000e-04\n",
            "step 299 | loss: 5.117000 | lr: 6.000000e-04\n",
            "step 300 | loss: 4.658666 | lr: 6.000000e-04\n",
            "step 301 | loss: 5.012308 | lr: 6.000000e-04\n",
            "step 302 | loss: 4.637500 | lr: 5.999998e-04\n",
            "step 303 | loss: 4.581676 | lr: 5.999996e-04\n",
            "step 304 | loss: 4.676496 | lr: 5.999993e-04\n",
            "step 305 | loss: 4.434052 | lr: 5.999989e-04\n",
            "step 306 | loss: 4.612995 | lr: 5.999984e-04\n",
            "step 307 | loss: 5.581578 | lr: 5.999979e-04\n",
            "step 308 | loss: 5.292567 | lr: 5.999972e-04\n",
            "step 309 | loss: 5.296542 | lr: 5.999965e-04\n",
            "step 310 | loss: 5.281308 | lr: 5.999957e-04\n",
            "step 311 | loss: 4.978997 | lr: 5.999948e-04\n",
            "step 312 | loss: 4.991499 | lr: 5.999938e-04\n",
            "step 313 | loss: 4.734147 | lr: 5.999927e-04\n",
            "step 314 | loss: 4.980953 | lr: 5.999915e-04\n",
            "step 315 | loss: 5.093201 | lr: 5.999903e-04\n",
            "step 316 | loss: 4.988653 | lr: 5.999889e-04\n",
            "step 317 | loss: 4.965282 | lr: 5.999875e-04\n",
            "step 318 | loss: 4.933850 | lr: 5.999860e-04\n",
            "step 319 | loss: 4.578345 | lr: 5.999844e-04\n",
            "step 320 | loss: 4.577216 | lr: 5.999827e-04\n",
            "step 321 | loss: 4.546173 | lr: 5.999809e-04\n",
            "step 322 | loss: 4.454613 | lr: 5.999790e-04\n",
            "step 323 | loss: 4.013460 | lr: 5.999771e-04\n",
            "step 324 | loss: 4.994479 | lr: 5.999751e-04\n",
            "step 325 | loss: 5.654404 | lr: 5.999729e-04\n",
            "step 326 | loss: 5.607135 | lr: 5.999707e-04\n",
            "step 327 | loss: 5.563916 | lr: 5.999684e-04\n",
            "step 328 | loss: 5.065544 | lr: 5.999661e-04\n",
            "step 329 | loss: 5.173495 | lr: 5.999636e-04\n",
            "step 330 | loss: 5.400322 | lr: 5.999610e-04\n",
            "step 331 | loss: 5.472873 | lr: 5.999584e-04\n",
            "step 332 | loss: 5.379848 | lr: 5.999557e-04\n",
            "step 333 | loss: 5.248615 | lr: 5.999529e-04\n",
            "step 334 | loss: 5.184804 | lr: 5.999500e-04\n",
            "step 335 | loss: 5.302231 | lr: 5.999470e-04\n",
            "step 336 | loss: 5.122108 | lr: 5.999439e-04\n",
            "step 337 | loss: 5.169485 | lr: 5.999407e-04\n",
            "step 338 | loss: 4.916116 | lr: 5.999375e-04\n",
            "step 339 | loss: 4.854741 | lr: 5.999342e-04\n",
            "step 340 | loss: 4.710150 | lr: 5.999307e-04\n",
            "step 341 | loss: 4.251538 | lr: 5.999272e-04\n",
            "step 342 | loss: 4.658781 | lr: 5.999236e-04\n",
            "step 343 | loss: 4.498149 | lr: 5.999200e-04\n",
            "step 344 | loss: 4.617873 | lr: 5.999162e-04\n",
            "step 345 | loss: 4.625330 | lr: 5.999123e-04\n",
            "step 346 | loss: 4.972850 | lr: 5.999084e-04\n",
            "step 347 | loss: 4.661441 | lr: 5.999044e-04\n",
            "step 348 | loss: 4.711975 | lr: 5.999003e-04\n",
            "step 349 | loss: 4.692771 | lr: 5.998961e-04\n",
            "step 350 | loss: 5.028154 | lr: 5.998918e-04\n",
            "step 351 | loss: 4.913657 | lr: 5.998874e-04\n",
            "step 352 | loss: 4.827686 | lr: 5.998830e-04\n",
            "step 353 | loss: 5.392556 | lr: 5.998784e-04\n",
            "step 354 | loss: 5.262955 | lr: 5.998738e-04\n",
            "step 355 | loss: 4.953738 | lr: 5.998691e-04\n",
            "step 356 | loss: 5.015412 | lr: 5.998643e-04\n",
            "step 357 | loss: 4.922174 | lr: 5.998594e-04\n",
            "step 358 | loss: 5.057289 | lr: 5.998544e-04\n",
            "step 359 | loss: 4.781013 | lr: 5.998493e-04\n",
            "step 360 | loss: 4.882976 | lr: 5.998442e-04\n",
            "step 361 | loss: 4.887497 | lr: 5.998389e-04\n",
            "step 362 | loss: 4.635169 | lr: 5.998336e-04\n",
            "step 363 | loss: 4.432678 | lr: 5.998282e-04\n",
            "step 364 | loss: 4.622334 | lr: 5.998227e-04\n",
            "step 365 | loss: 4.508612 | lr: 5.998171e-04\n",
            "step 366 | loss: 4.872981 | lr: 5.998115e-04\n",
            "step 367 | loss: 4.909531 | lr: 5.998057e-04\n",
            "step 368 | loss: 4.635381 | lr: 5.997999e-04\n",
            "step 369 | loss: 4.639636 | lr: 5.997939e-04\n",
            "step 370 | loss: 5.160342 | lr: 5.997879e-04\n",
            "step 371 | loss: 4.694170 | lr: 5.997818e-04\n",
            "step 372 | loss: 4.394574 | lr: 5.997756e-04\n",
            "step 373 | loss: 4.448500 | lr: 5.997694e-04\n",
            "step 374 | loss: 5.113389 | lr: 5.997630e-04\n",
            "step 375 | loss: 4.962481 | lr: 5.997565e-04\n",
            "step 376 | loss: 4.926606 | lr: 5.997500e-04\n",
            "step 377 | loss: 5.267240 | lr: 5.997434e-04\n",
            "step 378 | loss: 5.220201 | lr: 5.997367e-04\n",
            "step 379 | loss: 5.153769 | lr: 5.997299e-04\n",
            "step 380 | loss: 5.076191 | lr: 5.997230e-04\n",
            "step 381 | loss: 5.121898 | lr: 5.997160e-04\n",
            "step 382 | loss: 5.192817 | lr: 5.997090e-04\n",
            "step 383 | loss: 4.901828 | lr: 5.997018e-04\n",
            "step 384 | loss: 4.821679 | lr: 5.996946e-04\n",
            "step 385 | loss: 5.137235 | lr: 5.996873e-04\n",
            "step 386 | loss: 4.969596 | lr: 5.996799e-04\n",
            "step 387 | loss: 4.888055 | lr: 5.996724e-04\n",
            "step 388 | loss: 4.892828 | lr: 5.996648e-04\n",
            "step 389 | loss: 4.854280 | lr: 5.996572e-04\n",
            "step 390 | loss: 4.664345 | lr: 5.996494e-04\n",
            "step 391 | loss: 4.608262 | lr: 5.996416e-04\n",
            "step 392 | loss: 4.101043 | lr: 5.996337e-04\n",
            "step 393 | loss: 4.897879 | lr: 5.996257e-04\n",
            "step 394 | loss: 4.867441 | lr: 5.996176e-04\n",
            "step 395 | loss: 5.118936 | lr: 5.996094e-04\n",
            "step 396 | loss: 5.295734 | lr: 5.996012e-04\n",
            "step 397 | loss: 5.011253 | lr: 5.995928e-04\n",
            "step 398 | loss: 5.294657 | lr: 5.995844e-04\n",
            "step 399 | loss: 4.937944 | lr: 5.995758e-04\n",
            "step 400 | loss: 4.712692 | lr: 5.995672e-04\n",
            "step 401 | loss: 5.263085 | lr: 5.995585e-04\n",
            "step 402 | loss: 4.526267 | lr: 5.995498e-04\n",
            "step 403 | loss: 4.828461 | lr: 5.995409e-04\n",
            "step 404 | loss: 4.708157 | lr: 5.995319e-04\n",
            "step 405 | loss: 5.096903 | lr: 5.995229e-04\n",
            "step 406 | loss: 4.792796 | lr: 5.995138e-04\n",
            "step 407 | loss: 4.667343 | lr: 5.995045e-04\n",
            "step 408 | loss: 4.513174 | lr: 5.994952e-04\n",
            "step 409 | loss: 4.558558 | lr: 5.994859e-04\n",
            "step 410 | loss: 4.643139 | lr: 5.994764e-04\n",
            "step 411 | loss: 4.843386 | lr: 5.994668e-04\n",
            "step 412 | loss: 5.126187 | lr: 5.994572e-04\n",
            "step 413 | loss: 4.749754 | lr: 5.994474e-04\n",
            "step 414 | loss: 4.842394 | lr: 5.994376e-04\n",
            "step 415 | loss: 4.821321 | lr: 5.994277e-04\n",
            "step 416 | loss: 4.663131 | lr: 5.994177e-04\n",
            "step 417 | loss: 4.991634 | lr: 5.994076e-04\n",
            "step 418 | loss: 5.047935 | lr: 5.993975e-04\n",
            "step 419 | loss: 5.009458 | lr: 5.993872e-04\n",
            "step 420 | loss: 4.618254 | lr: 5.993769e-04\n",
            "step 421 | loss: 4.996948 | lr: 5.993665e-04\n",
            "step 422 | loss: 4.763178 | lr: 5.993559e-04\n",
            "step 423 | loss: 4.565133 | lr: 5.993454e-04\n",
            "step 424 | loss: 4.358282 | lr: 5.993347e-04\n",
            "step 425 | loss: 5.002654 | lr: 5.993239e-04\n",
            "step 426 | loss: 4.623263 | lr: 5.993130e-04\n",
            "step 427 | loss: 4.703363 | lr: 5.993021e-04\n",
            "step 428 | loss: 4.545344 | lr: 5.992911e-04\n",
            "step 429 | loss: 4.458916 | lr: 5.992800e-04\n",
            "step 430 | loss: 4.522642 | lr: 5.992688e-04\n",
            "step 431 | loss: 4.725199 | lr: 5.992575e-04\n",
            "step 432 | loss: 4.230652 | lr: 5.992461e-04\n",
            "step 433 | loss: 4.808413 | lr: 5.992346e-04\n",
            "step 434 | loss: 5.356832 | lr: 5.992231e-04\n",
            "step 435 | loss: 5.344895 | lr: 5.992114e-04\n",
            "step 436 | loss: 5.255796 | lr: 5.991997e-04\n",
            "step 437 | loss: 5.023620 | lr: 5.991879e-04\n",
            "step 438 | loss: 4.993529 | lr: 5.991760e-04\n",
            "step 439 | loss: 4.718415 | lr: 5.991640e-04\n",
            "step 440 | loss: 4.933125 | lr: 5.991520e-04\n",
            "step 441 | loss: 4.942283 | lr: 5.991398e-04\n",
            "step 442 | loss: 5.261930 | lr: 5.991276e-04\n",
            "step 443 | loss: 5.380725 | lr: 5.991153e-04\n",
            "step 444 | loss: 4.994031 | lr: 5.991029e-04\n",
            "step 445 | loss: 5.253841 | lr: 5.990904e-04\n",
            "step 446 | loss: 4.876458 | lr: 5.990778e-04\n",
            "step 447 | loss: 4.825417 | lr: 5.990651e-04\n",
            "step 448 | loss: 4.822629 | lr: 5.990524e-04\n",
            "step 449 | loss: 4.836799 | lr: 5.990395e-04\n",
            "step 450 | loss: 4.840152 | lr: 5.990266e-04\n",
            "step 451 | loss: 5.063837 | lr: 5.990136e-04\n",
            "step 452 | loss: 4.832246 | lr: 5.990005e-04\n",
            "step 453 | loss: 4.818902 | lr: 5.989873e-04\n",
            "step 454 | loss: 4.935371 | lr: 5.989740e-04\n",
            "step 455 | loss: 5.096291 | lr: 5.989606e-04\n",
            "step 456 | loss: 4.873305 | lr: 5.989472e-04\n",
            "step 457 | loss: 4.634361 | lr: 5.989337e-04\n",
            "step 458 | loss: 4.358215 | lr: 5.989200e-04\n",
            "step 459 | loss: 4.671786 | lr: 5.989063e-04\n",
            "step 460 | loss: 4.578166 | lr: 5.988926e-04\n",
            "step 461 | loss: 4.646292 | lr: 5.988787e-04\n",
            "step 462 | loss: 4.892392 | lr: 5.988647e-04\n",
            "step 463 | loss: 4.430390 | lr: 5.988507e-04\n",
            "step 464 | loss: 4.779668 | lr: 5.988365e-04\n",
            "step 465 | loss: 4.291699 | lr: 5.988223e-04\n",
            "step 466 | loss: 4.631931 | lr: 5.988080e-04\n",
            "step 467 | loss: 4.335953 | lr: 5.987936e-04\n",
            "step 468 | loss: 4.311231 | lr: 5.987791e-04\n",
            "step 469 | loss: 4.360898 | lr: 5.987646e-04\n",
            "step 470 | loss: 4.140295 | lr: 5.987499e-04\n",
            "step 471 | loss: 4.317271 | lr: 5.987352e-04\n",
            "step 472 | loss: 5.264078 | lr: 5.987203e-04\n",
            "step 473 | loss: 4.980205 | lr: 5.987054e-04\n",
            "step 474 | loss: 4.967743 | lr: 5.986904e-04\n",
            "step 475 | loss: 4.963028 | lr: 5.986753e-04\n",
            "step 476 | loss: 4.620531 | lr: 5.986602e-04\n",
            "step 477 | loss: 4.570940 | lr: 5.986449e-04\n",
            "step 478 | loss: 4.359666 | lr: 5.986296e-04\n",
            "step 479 | loss: 4.587790 | lr: 5.986141e-04\n",
            "step 480 | loss: 4.678762 | lr: 5.985986e-04\n",
            "step 481 | loss: 4.561409 | lr: 5.985830e-04\n",
            "step 482 | loss: 4.558652 | lr: 5.985673e-04\n",
            "step 483 | loss: 4.597033 | lr: 5.985516e-04\n",
            "step 484 | loss: 4.192976 | lr: 5.985357e-04\n",
            "step 485 | loss: 4.308516 | lr: 5.985198e-04\n",
            "step 486 | loss: 4.249106 | lr: 5.985037e-04\n",
            "step 487 | loss: 4.115325 | lr: 5.984876e-04\n",
            "step 488 | loss: 3.715891 | lr: 5.984714e-04\n",
            "step 489 | loss: 4.769547 | lr: 5.984551e-04\n",
            "step 490 | loss: 5.383583 | lr: 5.984387e-04\n",
            "step 491 | loss: 5.291311 | lr: 5.984223e-04\n",
            "step 492 | loss: 5.231156 | lr: 5.984057e-04\n",
            "step 493 | loss: 4.821031 | lr: 5.983891e-04\n",
            "step 494 | loss: 4.884347 | lr: 5.983724e-04\n",
            "step 495 | loss: 5.176094 | lr: 5.983556e-04\n",
            "step 496 | loss: 5.208894 | lr: 5.983387e-04\n",
            "step 497 | loss: 5.065845 | lr: 5.983217e-04\n",
            "step 498 | loss: 4.913234 | lr: 5.983046e-04\n",
            "step 499 | loss: 4.845361 | lr: 5.982875e-04\n",
            "step 500 | loss: 4.996843 | lr: 5.982702e-04\n",
            "step 501 | loss: 4.753530 | lr: 5.982529e-04\n",
            "step 502 | loss: 4.793054 | lr: 5.982355e-04\n",
            "step 503 | loss: 4.566353 | lr: 5.982180e-04\n",
            "step 504 | loss: 4.517593 | lr: 5.982004e-04\n",
            "step 505 | loss: 4.368895 | lr: 5.981828e-04\n",
            "step 506 | loss: 3.934100 | lr: 5.981650e-04\n",
            "step 507 | loss: 4.391562 | lr: 5.981472e-04\n",
            "step 508 | loss: 4.217476 | lr: 5.981292e-04\n",
            "step 509 | loss: 4.350540 | lr: 5.981112e-04\n",
            "step 510 | loss: 4.355165 | lr: 5.980931e-04\n",
            "step 511 | loss: 4.678823 | lr: 5.980750e-04\n",
            "step 512 | loss: 4.447124 | lr: 5.980567e-04\n",
            "step 513 | loss: 4.473836 | lr: 5.980383e-04\n",
            "step 514 | loss: 4.473904 | lr: 5.980199e-04\n",
            "step 515 | loss: 4.744802 | lr: 5.980014e-04\n",
            "step 516 | loss: 4.663659 | lr: 5.979828e-04\n",
            "step 517 | loss: 4.556458 | lr: 5.979641e-04\n",
            "step 518 | loss: 5.120706 | lr: 5.979453e-04\n",
            "step 519 | loss: 4.919767 | lr: 5.979264e-04\n",
            "step 520 | loss: 4.621796 | lr: 5.979074e-04\n",
            "step 521 | loss: 4.682969 | lr: 5.978884e-04\n",
            "step 522 | loss: 4.596260 | lr: 5.978693e-04\n",
            "step 523 | loss: 4.713571 | lr: 5.978501e-04\n",
            "step 524 | loss: 4.432068 | lr: 5.978308e-04\n",
            "step 525 | loss: 4.542984 | lr: 5.978114e-04\n",
            "step 526 | loss: 4.568047 | lr: 5.977919e-04\n",
            "step 527 | loss: 4.258688 | lr: 5.977723e-04\n",
            "step 528 | loss: 4.135210 | lr: 5.977527e-04\n",
            "step 529 | loss: 4.293588 | lr: 5.977330e-04\n",
            "step 530 | loss: 4.242567 | lr: 5.977131e-04\n",
            "step 531 | loss: 4.629762 | lr: 5.976932e-04\n",
            "step 532 | loss: 4.655989 | lr: 5.976733e-04\n",
            "step 533 | loss: 4.373441 | lr: 5.976532e-04\n",
            "step 534 | loss: 4.306466 | lr: 5.976330e-04\n",
            "step 535 | loss: 4.838969 | lr: 5.976128e-04\n",
            "step 536 | loss: 4.344756 | lr: 5.975924e-04\n",
            "step 537 | loss: 4.117053 | lr: 5.975720e-04\n",
            "step 538 | loss: 4.136505 | lr: 5.975515e-04\n",
            "step 539 | loss: 4.849019 | lr: 5.975309e-04\n",
            "step 540 | loss: 4.712935 | lr: 5.975103e-04\n",
            "step 541 | loss: 4.662378 | lr: 5.974895e-04\n",
            "step 542 | loss: 5.018667 | lr: 5.974687e-04\n",
            "step 543 | loss: 4.895925 | lr: 5.974477e-04\n",
            "step 544 | loss: 4.809504 | lr: 5.974267e-04\n",
            "step 545 | loss: 4.724468 | lr: 5.974056e-04\n",
            "step 546 | loss: 4.850906 | lr: 5.973844e-04\n",
            "step 547 | loss: 4.811771 | lr: 5.973631e-04\n",
            "step 548 | loss: 4.549627 | lr: 5.973418e-04\n",
            "step 549 | loss: 4.509012 | lr: 5.973203e-04\n",
            "step 550 | loss: 4.877890 | lr: 5.972988e-04\n",
            "step 551 | loss: 4.659452 | lr: 5.972772e-04\n",
            "step 552 | loss: 4.622684 | lr: 5.972555e-04\n",
            "step 553 | loss: 4.620432 | lr: 5.972337e-04\n",
            "step 554 | loss: 4.567402 | lr: 5.972118e-04\n",
            "step 555 | loss: 4.434029 | lr: 5.971898e-04\n",
            "step 556 | loss: 4.352407 | lr: 5.971678e-04\n",
            "step 557 | loss: 3.888445 | lr: 5.971457e-04\n",
            "step 558 | loss: 4.638665 | lr: 5.971234e-04\n",
            "step 559 | loss: 4.613441 | lr: 5.971011e-04\n",
            "step 560 | loss: 4.824478 | lr: 5.970788e-04\n",
            "step 561 | loss: 4.986267 | lr: 5.970563e-04\n",
            "step 562 | loss: 4.675588 | lr: 5.970337e-04\n",
            "step 563 | loss: 4.973951 | lr: 5.970111e-04\n",
            "step 564 | loss: 4.588517 | lr: 5.969883e-04\n",
            "step 565 | loss: 4.395077 | lr: 5.969655e-04\n",
            "step 566 | loss: 4.975244 | lr: 5.969426e-04\n",
            "step 567 | loss: 4.210008 | lr: 5.969196e-04\n",
            "step 568 | loss: 4.543462 | lr: 5.968966e-04\n",
            "step 569 | loss: 4.395122 | lr: 5.968734e-04\n",
            "step 570 | loss: 4.834702 | lr: 5.968501e-04\n",
            "step 571 | loss: 4.486978 | lr: 5.968268e-04\n",
            "step 572 | loss: 4.423696 | lr: 5.968034e-04\n",
            "step 573 | loss: 4.264300 | lr: 5.967799e-04\n",
            "step 574 | loss: 4.292880 | lr: 5.967563e-04\n",
            "step 575 | loss: 4.369107 | lr: 5.967326e-04\n",
            "step 576 | loss: 4.574523 | lr: 5.967089e-04\n",
            "step 577 | loss: 4.863789 | lr: 5.966850e-04\n",
            "step 578 | loss: 4.489967 | lr: 5.966611e-04\n",
            "step 579 | loss: 4.630847 | lr: 5.966371e-04\n",
            "step 580 | loss: 4.467381 | lr: 5.966130e-04\n",
            "step 581 | loss: 4.373118 | lr: 5.965888e-04\n",
            "step 582 | loss: 4.756555 | lr: 5.965645e-04\n",
            "step 583 | loss: 4.814297 | lr: 5.965402e-04\n",
            "step 584 | loss: 4.772004 | lr: 5.965157e-04\n",
            "step 585 | loss: 4.375929 | lr: 5.964912e-04\n",
            "step 586 | loss: 4.763803 | lr: 5.964666e-04\n",
            "step 587 | loss: 4.561654 | lr: 5.964419e-04\n",
            "step 588 | loss: 4.362312 | lr: 5.964171e-04\n",
            "step 589 | loss: 4.091907 | lr: 5.963922e-04\n",
            "step 590 | loss: 4.765993 | lr: 5.963672e-04\n",
            "step 591 | loss: 4.394658 | lr: 5.963422e-04\n",
            "step 592 | loss: 4.495256 | lr: 5.963171e-04\n",
            "step 593 | loss: 4.318264 | lr: 5.962919e-04\n",
            "step 594 | loss: 4.202842 | lr: 5.962666e-04\n",
            "step 595 | loss: 4.252332 | lr: 5.962412e-04\n",
            "step 596 | loss: 4.497201 | lr: 5.962157e-04\n",
            "step 597 | loss: 3.967624 | lr: 5.961902e-04\n",
            "step 598 | loss: 4.566086 | lr: 5.961645e-04\n",
            "step 599 | loss: 5.131227 | lr: 5.961388e-04\n",
            "step 600 | loss: 5.073615 | lr: 5.961130e-04\n",
            "step 601 | loss: 4.924582 | lr: 5.960871e-04\n",
            "step 602 | loss: 4.789513 | lr: 5.960611e-04\n",
            "step 603 | loss: 4.738461 | lr: 5.960350e-04\n",
            "step 604 | loss: 4.485874 | lr: 5.960089e-04\n",
            "step 605 | loss: 4.757937 | lr: 5.959826e-04\n",
            "step 606 | loss: 4.736996 | lr: 5.959563e-04\n",
            "step 607 | loss: 5.075396 | lr: 5.959299e-04\n",
            "step 608 | loss: 5.124657 | lr: 5.959034e-04\n",
            "step 609 | loss: 4.785480 | lr: 5.958768e-04\n",
            "step 610 | loss: 5.002852 | lr: 5.958502e-04\n",
            "step 611 | loss: 4.569917 | lr: 5.958234e-04\n",
            "step 612 | loss: 4.540111 | lr: 5.957966e-04\n",
            "step 613 | loss: 4.559588 | lr: 5.957697e-04\n",
            "step 614 | loss: 4.596222 | lr: 5.957426e-04\n",
            "step 615 | loss: 4.653781 | lr: 5.957156e-04\n",
            "step 616 | loss: 4.831266 | lr: 5.956884e-04\n",
            "step 617 | loss: 4.611449 | lr: 5.956611e-04\n",
            "step 618 | loss: 4.579547 | lr: 5.956338e-04\n",
            "step 619 | loss: 4.595268 | lr: 5.956063e-04\n",
            "step 620 | loss: 4.804621 | lr: 5.955788e-04\n",
            "step 621 | loss: 4.518393 | lr: 5.955512e-04\n",
            "step 622 | loss: 4.296665 | lr: 5.955235e-04\n",
            "step 623 | loss: 4.044971 | lr: 5.954957e-04\n",
            "step 624 | loss: 4.422826 | lr: 5.954679e-04\n",
            "step 625 | loss: 4.319420 | lr: 5.954399e-04\n",
            "step 626 | loss: 4.380788 | lr: 5.954119e-04\n",
            "step 627 | loss: 4.666404 | lr: 5.953838e-04\n",
            "step 628 | loss: 4.218968 | lr: 5.953556e-04\n",
            "step 629 | loss: 4.533521 | lr: 5.953273e-04\n",
            "step 630 | loss: 4.043791 | lr: 5.952989e-04\n",
            "step 631 | loss: 4.383650 | lr: 5.952705e-04\n",
            "step 632 | loss: 4.128718 | lr: 5.952420e-04\n",
            "step 633 | loss: 4.096473 | lr: 5.952133e-04\n",
            "step 634 | loss: 4.149462 | lr: 5.951846e-04\n",
            "step 635 | loss: 3.955741 | lr: 5.951558e-04\n",
            "step 636 | loss: 4.120214 | lr: 5.951269e-04\n",
            "step 637 | loss: 5.029527 | lr: 5.950980e-04\n",
            "step 638 | loss: 4.747750 | lr: 5.950689e-04\n",
            "step 639 | loss: 4.711418 | lr: 5.950398e-04\n",
            "step 640 | loss: 4.671264 | lr: 5.950106e-04\n",
            "step 641 | loss: 4.290777 | lr: 5.949813e-04\n",
            "step 642 | loss: 4.287476 | lr: 5.949519e-04\n",
            "step 643 | loss: 4.052077 | lr: 5.949224e-04\n",
            "step 644 | loss: 4.341313 | lr: 5.948928e-04\n",
            "step 645 | loss: 4.355852 | lr: 5.948632e-04\n",
            "step 646 | loss: 4.261035 | lr: 5.948335e-04\n",
            "step 647 | loss: 4.274323 | lr: 5.948036e-04\n",
            "step 648 | loss: 4.342854 | lr: 5.947737e-04\n",
            "step 649 | loss: 3.939009 | lr: 5.947437e-04\n",
            "step 650 | loss: 4.094542 | lr: 5.947137e-04\n",
            "step 651 | loss: 3.999033 | lr: 5.946835e-04\n",
            "step 652 | loss: 3.817122 | lr: 5.946533e-04\n",
            "step 653 | loss: 3.452621 | lr: 5.946230e-04\n",
            "step 654 | loss: 4.557157 | lr: 5.945925e-04\n",
            "step 655 | loss: 5.104774 | lr: 5.945620e-04\n",
            "step 656 | loss: 5.020302 | lr: 5.945315e-04\n",
            "step 657 | loss: 4.979137 | lr: 5.945008e-04\n",
            "step 658 | loss: 4.457096 | lr: 5.944700e-04\n",
            "step 659 | loss: 4.563350 | lr: 5.944392e-04\n",
            "step 660 | loss: 4.866992 | lr: 5.944083e-04\n",
            "step 661 | loss: 4.842481 | lr: 5.943773e-04\n",
            "step 662 | loss: 4.685724 | lr: 5.943462e-04\n",
            "step 663 | loss: 4.572207 | lr: 5.943150e-04\n",
            "step 664 | loss: 4.564067 | lr: 5.942838e-04\n",
            "step 665 | loss: 4.669144 | lr: 5.942524e-04\n",
            "step 666 | loss: 4.419030 | lr: 5.942210e-04\n",
            "step 667 | loss: 4.554400 | lr: 5.941895e-04\n",
            "step 668 | loss: 4.301614 | lr: 5.941579e-04\n",
            "step 669 | loss: 4.239168 | lr: 5.941262e-04\n",
            "step 670 | loss: 4.134645 | lr: 5.940944e-04\n",
            "step 671 | loss: 3.748990 | lr: 5.940626e-04\n",
            "step 672 | loss: 4.190855 | lr: 5.940306e-04\n",
            "step 673 | loss: 4.040313 | lr: 5.939986e-04\n",
            "step 674 | loss: 4.092641 | lr: 5.939665e-04\n",
            "step 675 | loss: 4.137874 | lr: 5.939343e-04\n",
            "step 676 | loss: 4.438941 | lr: 5.939020e-04\n",
            "step 677 | loss: 4.194823 | lr: 5.938697e-04\n",
            "step 678 | loss: 4.300713 | lr: 5.938372e-04\n",
            "step 679 | loss: 4.271221 | lr: 5.938047e-04\n",
            "step 680 | loss: 4.514759 | lr: 5.937721e-04\n",
            "step 681 | loss: 4.444900 | lr: 5.937394e-04\n",
            "step 682 | loss: 4.310332 | lr: 5.937066e-04\n",
            "step 683 | loss: 4.911526 | lr: 5.936737e-04\n",
            "step 684 | loss: 4.655320 | lr: 5.936408e-04\n",
            "step 685 | loss: 4.343216 | lr: 5.936077e-04\n",
            "step 686 | loss: 4.415798 | lr: 5.935746e-04\n",
            "step 687 | loss: 4.344255 | lr: 5.935414e-04\n",
            "step 688 | loss: 4.419135 | lr: 5.935081e-04\n",
            "step 689 | loss: 4.124572 | lr: 5.934747e-04\n",
            "step 690 | loss: 4.255336 | lr: 5.934412e-04\n",
            "step 691 | loss: 4.363113 | lr: 5.934077e-04\n",
            "step 692 | loss: 4.061208 | lr: 5.933741e-04\n",
            "step 693 | loss: 3.944036 | lr: 5.933404e-04\n",
            "step 694 | loss: 4.089872 | lr: 5.933066e-04\n",
            "step 695 | loss: 4.001553 | lr: 5.932727e-04\n",
            "step 696 | loss: 4.436655 | lr: 5.932387e-04\n",
            "step 697 | loss: 4.445861 | lr: 5.932046e-04\n",
            "step 698 | loss: 4.148339 | lr: 5.931705e-04\n",
            "step 699 | loss: 4.060738 | lr: 5.931363e-04\n",
            "step 700 | loss: 4.613801 | lr: 5.931020e-04\n",
            "step 701 | loss: 4.116003 | lr: 5.930676e-04\n",
            "step 702 | loss: 3.873547 | lr: 5.930331e-04\n",
            "step 703 | loss: 3.868288 | lr: 5.929985e-04\n",
            "step 704 | loss: 4.562482 | lr: 5.929639e-04\n",
            "step 705 | loss: 4.459261 | lr: 5.929292e-04\n",
            "step 706 | loss: 4.399444 | lr: 5.928943e-04\n",
            "step 707 | loss: 4.766870 | lr: 5.928594e-04\n",
            "step 708 | loss: 4.632314 | lr: 5.928245e-04\n",
            "step 709 | loss: 4.592518 | lr: 5.927894e-04\n",
            "step 710 | loss: 4.448527 | lr: 5.927542e-04\n",
            "step 711 | loss: 4.607611 | lr: 5.927190e-04\n",
            "step 712 | loss: 4.563516 | lr: 5.926837e-04\n",
            "step 713 | loss: 4.287493 | lr: 5.926483e-04\n",
            "step 714 | loss: 4.258721 | lr: 5.926128e-04\n",
            "step 715 | loss: 4.636173 | lr: 5.925772e-04\n",
            "step 716 | loss: 4.400983 | lr: 5.925416e-04\n",
            "step 717 | loss: 4.402800 | lr: 5.925058e-04\n",
            "step 718 | loss: 4.429197 | lr: 5.924700e-04\n",
            "step 719 | loss: 4.345898 | lr: 5.924341e-04\n",
            "step 720 | loss: 4.213733 | lr: 5.923981e-04\n",
            "step 721 | loss: 4.146272 | lr: 5.923620e-04\n",
            "step 722 | loss: 3.698814 | lr: 5.923258e-04\n",
            "step 723 | loss: 4.431798 | lr: 5.922896e-04\n",
            "step 724 | loss: 4.358550 | lr: 5.922533e-04\n",
            "step 725 | loss: 4.570064 | lr: 5.922168e-04\n",
            "step 726 | loss: 4.661626 | lr: 5.921803e-04\n",
            "step 727 | loss: 4.414411 | lr: 5.921438e-04\n",
            "step 728 | loss: 4.684299 | lr: 5.921071e-04\n",
            "step 729 | loss: 4.298330 | lr: 5.920703e-04\n",
            "step 730 | loss: 4.115219 | lr: 5.920335e-04\n",
            "step 731 | loss: 4.729251 | lr: 5.919966e-04\n",
            "step 732 | loss: 3.998541 | lr: 5.919596e-04\n",
            "step 733 | loss: 4.279616 | lr: 5.919225e-04\n",
            "step 734 | loss: 4.146261 | lr: 5.918853e-04\n",
            "step 735 | loss: 4.609858 | lr: 5.918480e-04\n",
            "step 736 | loss: 4.245973 | lr: 5.918107e-04\n",
            "step 737 | loss: 4.245999 | lr: 5.917733e-04\n",
            "step 738 | loss: 4.071265 | lr: 5.917358e-04\n",
            "step 739 | loss: 4.075408 | lr: 5.916982e-04\n",
            "step 740 | loss: 4.122667 | lr: 5.916605e-04\n",
            "step 741 | loss: 4.352383 | lr: 5.916227e-04\n",
            "step 742 | loss: 4.652331 | lr: 5.915849e-04\n",
            "step 743 | loss: 4.298284 | lr: 5.915469e-04\n",
            "step 744 | loss: 4.393988 | lr: 5.915089e-04\n",
            "step 745 | loss: 4.240986 | lr: 5.914708e-04\n",
            "step 746 | loss: 4.156326 | lr: 5.914326e-04\n",
            "step 747 | loss: 4.546044 | lr: 5.913944e-04\n",
            "step 748 | loss: 4.611055 | lr: 5.913560e-04\n",
            "step 749 | loss: 4.521360 | lr: 5.913176e-04\n",
            "step 750 | loss: 4.213644 | lr: 5.912791e-04\n",
            "step 751 | loss: 4.657636 | lr: 5.912405e-04\n",
            "step 752 | loss: 4.365907 | lr: 5.912018e-04\n",
            "step 753 | loss: 4.148195 | lr: 5.911630e-04\n",
            "step 754 | loss: 3.854635 | lr: 5.911242e-04\n",
            "step 755 | loss: 4.537493 | lr: 5.910852e-04\n",
            "step 756 | loss: 4.211356 | lr: 5.910462e-04\n",
            "step 757 | loss: 4.253666 | lr: 5.910071e-04\n",
            "step 758 | loss: 4.122717 | lr: 5.909679e-04\n",
            "step 759 | loss: 3.989248 | lr: 5.909286e-04\n",
            "step 760 | loss: 4.074657 | lr: 5.908893e-04\n",
            "step 761 | loss: 4.309865 | lr: 5.908498e-04\n",
            "step 762 | loss: 3.786324 | lr: 5.908103e-04\n",
            "step 763 | loss: 4.352500 | lr: 5.907707e-04\n",
            "step 764 | loss: 4.850949 | lr: 5.907310e-04\n",
            "step 765 | loss: 4.800198 | lr: 5.906912e-04\n",
            "step 766 | loss: 4.658580 | lr: 5.906514e-04\n",
            "step 767 | loss: 4.558826 | lr: 5.906114e-04\n",
            "step 768 | loss: 4.444712 | lr: 5.905714e-04\n",
            "step 769 | loss: 4.239305 | lr: 5.905313e-04\n",
            "step 770 | loss: 4.549979 | lr: 5.904911e-04\n",
            "step 771 | loss: 4.505763 | lr: 5.904508e-04\n",
            "step 772 | loss: 4.843560 | lr: 5.904104e-04\n",
            "step 773 | loss: 4.905933 | lr: 5.903700e-04\n",
            "step 774 | loss: 4.616218 | lr: 5.903295e-04\n",
            "step 775 | loss: 4.799912 | lr: 5.902889e-04\n",
            "step 776 | loss: 4.347093 | lr: 5.902482e-04\n",
            "step 777 | loss: 4.364683 | lr: 5.902074e-04\n",
            "step 778 | loss: 4.389875 | lr: 5.901665e-04\n",
            "step 779 | loss: 4.419888 | lr: 5.901256e-04\n",
            "step 780 | loss: 4.476471 | lr: 5.900845e-04\n",
            "step 781 | loss: 4.692509 | lr: 5.900434e-04\n",
            "step 782 | loss: 4.401820 | lr: 5.900022e-04\n",
            "step 783 | loss: 4.354040 | lr: 5.899609e-04\n",
            "step 784 | loss: 4.329037 | lr: 5.899196e-04\n",
            "step 785 | loss: 4.594377 | lr: 5.898781e-04\n",
            "step 786 | loss: 4.327222 | lr: 5.898366e-04\n",
            "step 787 | loss: 4.114586 | lr: 5.897950e-04\n",
            "step 788 | loss: 3.901515 | lr: 5.897533e-04\n",
            "step 789 | loss: 4.207253 | lr: 5.897115e-04\n",
            "step 790 | loss: 4.119706 | lr: 5.896696e-04\n",
            "step 791 | loss: 4.123344 | lr: 5.896277e-04\n",
            "step 792 | loss: 4.419800 | lr: 5.895856e-04\n",
            "step 793 | loss: 3.984472 | lr: 5.895435e-04\n",
            "step 794 | loss: 4.296303 | lr: 5.895013e-04\n",
            "step 795 | loss: 3.817579 | lr: 5.894590e-04\n",
            "step 796 | loss: 4.164681 | lr: 5.894167e-04\n",
            "step 797 | loss: 3.911669 | lr: 5.893742e-04\n",
            "step 798 | loss: 3.883869 | lr: 5.893317e-04\n",
            "step 799 | loss: 3.937841 | lr: 5.892891e-04\n",
            "step 800 | loss: 3.767837 | lr: 5.892464e-04\n",
            "step 801 | loss: 3.920482 | lr: 5.892036e-04\n",
            "step 802 | loss: 4.805792 | lr: 5.891607e-04\n",
            "step 803 | loss: 4.473009 | lr: 5.891178e-04\n",
            "step 804 | loss: 4.492403 | lr: 5.890747e-04\n",
            "step 805 | loss: 4.376534 | lr: 5.890316e-04\n",
            "step 806 | loss: 4.026642 | lr: 5.889884e-04\n",
            "step 807 | loss: 4.039721 | lr: 5.889451e-04\n",
            "step 808 | loss: 3.811977 | lr: 5.889018e-04\n",
            "step 809 | loss: 4.112514 | lr: 5.888583e-04\n",
            "step 810 | loss: 4.164088 | lr: 5.888148e-04\n",
            "step 811 | loss: 4.094048 | lr: 5.887712e-04\n",
            "step 812 | loss: 4.084775 | lr: 5.887275e-04\n",
            "step 813 | loss: 4.161136 | lr: 5.886837e-04\n",
            "step 814 | loss: 3.715829 | lr: 5.886398e-04\n",
            "step 815 | loss: 3.880104 | lr: 5.885959e-04\n",
            "step 816 | loss: 3.806651 | lr: 5.885519e-04\n",
            "step 817 | loss: 3.617762 | lr: 5.885077e-04\n",
            "step 818 | loss: 3.280047 | lr: 5.884635e-04\n",
            "step 819 | loss: 4.309463 | lr: 5.884193e-04\n",
            "step 820 | loss: 4.803297 | lr: 5.883749e-04\n",
            "step 821 | loss: 4.748154 | lr: 5.883305e-04\n",
            "step 822 | loss: 4.735723 | lr: 5.882859e-04\n",
            "step 823 | loss: 4.134731 | lr: 5.882413e-04\n",
            "step 824 | loss: 4.279825 | lr: 5.881966e-04\n",
            "step 825 | loss: 4.691145 | lr: 5.881518e-04\n",
            "step 826 | loss: 4.605963 | lr: 5.881070e-04\n",
            "step 827 | loss: 4.443660 | lr: 5.880620e-04\n",
            "step 828 | loss: 4.362319 | lr: 5.880170e-04\n",
            "step 829 | loss: 4.351417 | lr: 5.879719e-04\n",
            "step 830 | loss: 4.433734 | lr: 5.879267e-04\n",
            "step 831 | loss: 4.179678 | lr: 5.878814e-04\n",
            "step 832 | loss: 4.380385 | lr: 5.878361e-04\n",
            "step 833 | loss: 4.125151 | lr: 5.877906e-04\n",
            "step 834 | loss: 4.047695 | lr: 5.877451e-04\n",
            "step 835 | loss: 3.977933 | lr: 5.876995e-04\n",
            "step 836 | loss: 3.610805 | lr: 5.876538e-04\n",
            "step 837 | loss: 4.006503 | lr: 5.876080e-04\n",
            "step 838 | loss: 3.867204 | lr: 5.875622e-04\n",
            "step 839 | loss: 3.894340 | lr: 5.875162e-04\n",
            "step 840 | loss: 3.961837 | lr: 5.874702e-04\n",
            "step 841 | loss: 4.236678 | lr: 5.874241e-04\n",
            "step 842 | loss: 3.968730 | lr: 5.873779e-04\n",
            "step 843 | loss: 4.103793 | lr: 5.873316e-04\n",
            "step 844 | loss: 4.087473 | lr: 5.872853e-04\n",
            "step 845 | loss: 4.311655 | lr: 5.872388e-04\n",
            "step 846 | loss: 4.255140 | lr: 5.871923e-04\n",
            "step 847 | loss: 4.131481 | lr: 5.871457e-04\n",
            "step 848 | loss: 4.674026 | lr: 5.870990e-04\n",
            "step 849 | loss: 4.384734 | lr: 5.870523e-04\n",
            "step 850 | loss: 4.076907 | lr: 5.870054e-04\n",
            "step 851 | loss: 4.197828 | lr: 5.869585e-04\n",
            "step 852 | loss: 4.133747 | lr: 5.869115e-04\n",
            "step 853 | loss: 4.198603 | lr: 5.868644e-04\n",
            "step 854 | loss: 3.980471 | lr: 5.868172e-04\n",
            "step 855 | loss: 4.113807 | lr: 5.867699e-04\n",
            "step 856 | loss: 4.167060 | lr: 5.867226e-04\n",
            "step 857 | loss: 3.865929 | lr: 5.866752e-04\n",
            "step 858 | loss: 3.743836 | lr: 5.866277e-04\n",
            "step 859 | loss: 3.879479 | lr: 5.865801e-04\n",
            "step 860 | loss: 3.813686 | lr: 5.865324e-04\n",
            "step 861 | loss: 4.234390 | lr: 5.864846e-04\n",
            "step 862 | loss: 4.237152 | lr: 5.864368e-04\n",
            "step 863 | loss: 3.941062 | lr: 5.863889e-04\n",
            "step 864 | loss: 3.828321 | lr: 5.863409e-04\n",
            "step 865 | loss: 4.399329 | lr: 5.862928e-04\n",
            "step 866 | loss: 3.906899 | lr: 5.862446e-04\n",
            "step 867 | loss: 3.719100 | lr: 5.861964e-04\n",
            "step 868 | loss: 3.677535 | lr: 5.861480e-04\n",
            "step 869 | loss: 4.368958 | lr: 5.860996e-04\n",
            "step 870 | loss: 4.243426 | lr: 5.860511e-04\n",
            "step 871 | loss: 4.219045 | lr: 5.860025e-04\n",
            "step 872 | loss: 4.571016 | lr: 5.859539e-04\n",
            "step 873 | loss: 4.434928 | lr: 5.859051e-04\n",
            "step 874 | loss: 4.390650 | lr: 5.858563e-04\n",
            "step 875 | loss: 4.293050 | lr: 5.858074e-04\n",
            "step 876 | loss: 4.444814 | lr: 5.857584e-04\n",
            "step 877 | loss: 4.329081 | lr: 5.857093e-04\n",
            "step 878 | loss: 4.065051 | lr: 5.856602e-04\n",
            "step 879 | loss: 4.099792 | lr: 5.856109e-04\n",
            "step 880 | loss: 4.470398 | lr: 5.855616e-04\n",
            "step 881 | loss: 4.235409 | lr: 5.855122e-04\n",
            "step 882 | loss: 4.214280 | lr: 5.854627e-04\n",
            "step 883 | loss: 4.194713 | lr: 5.854131e-04\n",
            "step 884 | loss: 4.192974 | lr: 5.853635e-04\n",
            "step 885 | loss: 4.027385 | lr: 5.853137e-04\n",
            "step 886 | loss: 3.983789 | lr: 5.852639e-04\n",
            "step 887 | loss: 3.537069 | lr: 5.852140e-04\n",
            "step 888 | loss: 4.257649 | lr: 5.851641e-04\n",
            "step 889 | loss: 4.192830 | lr: 5.851140e-04\n",
            "step 890 | loss: 4.444390 | lr: 5.850639e-04\n",
            "step 891 | loss: 4.517187 | lr: 5.850136e-04\n",
            "step 892 | loss: 4.274776 | lr: 5.849633e-04\n",
            "step 893 | loss: 4.457138 | lr: 5.849129e-04\n",
            "step 894 | loss: 4.117229 | lr: 5.848625e-04\n",
            "step 895 | loss: 3.932501 | lr: 5.848119e-04\n",
            "step 896 | loss: 4.521746 | lr: 5.847613e-04\n",
            "step 897 | loss: 3.842260 | lr: 5.847106e-04\n",
            "step 898 | loss: 4.142515 | lr: 5.846598e-04\n",
            "step 899 | loss: 3.954331 | lr: 5.846089e-04\n",
            "step 900 | loss: 4.406388 | lr: 5.845579e-04\n",
            "step 901 | loss: 4.088140 | lr: 5.845069e-04\n",
            "step 902 | loss: 4.048271 | lr: 5.844557e-04\n",
            "step 903 | loss: 3.891231 | lr: 5.844045e-04\n",
            "step 904 | loss: 3.907529 | lr: 5.843532e-04\n",
            "step 905 | loss: 3.901854 | lr: 5.843019e-04\n",
            "step 906 | loss: 4.156207 | lr: 5.842504e-04\n",
            "step 907 | loss: 4.421923 | lr: 5.841989e-04\n",
            "step 908 | loss: 4.088009 | lr: 5.841473e-04\n",
            "step 909 | loss: 4.168940 | lr: 5.840956e-04\n",
            "step 910 | loss: 4.058850 | lr: 5.840438e-04\n",
            "step 911 | loss: 3.978862 | lr: 5.839919e-04\n",
            "step 912 | loss: 4.332708 | lr: 5.839400e-04\n",
            "step 913 | loss: 4.442065 | lr: 5.838880e-04\n",
            "step 914 | loss: 4.354162 | lr: 5.838358e-04\n",
            "step 915 | loss: 3.991497 | lr: 5.837837e-04\n",
            "step 916 | loss: 4.457538 | lr: 5.837314e-04\n",
            "step 917 | loss: 4.162886 | lr: 5.836790e-04\n",
            "step 918 | loss: 3.984786 | lr: 5.836266e-04\n",
            "step 919 | loss: 3.612548 | lr: 5.835741e-04\n",
            "step 920 | loss: 4.274038 | lr: 5.835215e-04\n",
            "step 921 | loss: 4.019419 | lr: 5.834688e-04\n",
            "step 922 | loss: 4.051369 | lr: 5.834160e-04\n",
            "step 923 | loss: 3.922207 | lr: 5.833632e-04\n",
            "step 924 | loss: 3.798656 | lr: 5.833103e-04\n",
            "step 925 | loss: 3.858225 | lr: 5.832573e-04\n",
            "step 926 | loss: 4.100465 | lr: 5.832042e-04\n",
            "step 927 | loss: 3.589841 | lr: 5.831510e-04\n",
            "step 928 | loss: 4.201237 | lr: 5.830978e-04\n",
            "step 929 | loss: 4.633874 | lr: 5.830444e-04\n",
            "step 930 | loss: 4.573033 | lr: 5.829910e-04\n",
            "step 931 | loss: 4.414282 | lr: 5.829375e-04\n",
            "step 932 | loss: 4.321939 | lr: 5.828840e-04\n",
            "step 933 | loss: 4.264655 | lr: 5.828303e-04\n",
            "step 934 | loss: 4.017685 | lr: 5.827766e-04\n",
            "step 935 | loss: 4.407631 | lr: 5.827227e-04\n",
            "step 936 | loss: 4.345225 | lr: 5.826688e-04\n",
            "step 937 | loss: 4.693272 | lr: 5.826149e-04\n",
            "step 938 | loss: 4.743752 | lr: 5.825608e-04\n",
            "step 939 | loss: 4.408089 | lr: 5.825067e-04\n",
            "step 940 | loss: 4.662333 | lr: 5.824524e-04\n",
            "step 941 | loss: 4.210601 | lr: 5.823981e-04\n",
            "step 942 | loss: 4.252148 | lr: 5.823437e-04\n",
            "step 943 | loss: 4.289720 | lr: 5.822893e-04\n",
            "step 944 | loss: 4.254681 | lr: 5.822347e-04\n",
            "step 945 | loss: 4.314602 | lr: 5.821801e-04\n",
            "step 946 | loss: 4.493409 | lr: 5.821254e-04\n",
            "step 947 | loss: 4.229937 | lr: 5.820706e-04\n",
            "step 948 | loss: 4.224504 | lr: 5.820157e-04\n",
            "step 949 | loss: 4.148954 | lr: 5.819607e-04\n",
            "step 950 | loss: 4.426541 | lr: 5.819057e-04\n",
            "step 951 | loss: 4.163106 | lr: 5.818506e-04\n",
            "step 952 | loss: 3.901269 | lr: 5.817954e-04\n",
            "step 953 | loss: 3.768146 | lr: 5.817401e-04\n",
            "step 954 | loss: 4.022953 | lr: 5.816847e-04\n",
            "step 955 | loss: 3.983862 | lr: 5.816293e-04\n",
            "step 956 | loss: 3.946314 | lr: 5.815738e-04\n",
            "step 957 | loss: 4.183481 | lr: 5.815182e-04\n",
            "step 958 | loss: 3.739975 | lr: 5.814625e-04\n",
            "step 959 | loss: 4.104127 | lr: 5.814067e-04\n",
            "step 960 | loss: 3.605783 | lr: 5.813509e-04\n",
            "step 961 | loss: 3.979636 | lr: 5.812949e-04\n",
            "step 962 | loss: 3.744254 | lr: 5.812389e-04\n",
            "step 963 | loss: 3.688790 | lr: 5.811828e-04\n",
            "step 964 | loss: 3.794096 | lr: 5.811267e-04\n",
            "step 965 | loss: 3.582244 | lr: 5.810704e-04\n",
            "step 966 | loss: 3.739036 | lr: 5.810141e-04\n",
            "step 967 | loss: 4.580500 | lr: 5.809577e-04\n",
            "step 968 | loss: 4.300688 | lr: 5.809012e-04\n",
            "step 969 | loss: 4.300128 | lr: 5.808446e-04\n",
            "step 970 | loss: 4.163789 | lr: 5.807880e-04\n",
            "step 971 | loss: 3.813839 | lr: 5.807312e-04\n",
            "step 972 | loss: 3.833721 | lr: 5.806744e-04\n",
            "step 973 | loss: 3.650426 | lr: 5.806175e-04\n",
            "step 974 | loss: 3.953119 | lr: 5.805605e-04\n",
            "step 975 | loss: 3.985427 | lr: 5.805035e-04\n",
            "step 976 | loss: 3.917965 | lr: 5.804463e-04\n",
            "step 977 | loss: 3.906811 | lr: 5.803891e-04\n",
            "step 978 | loss: 3.992570 | lr: 5.803318e-04\n",
            "step 979 | loss: 3.527843 | lr: 5.802744e-04\n",
            "step 980 | loss: 3.698291 | lr: 5.802170e-04\n",
            "step 981 | loss: 3.640929 | lr: 5.801594e-04\n",
            "step 982 | loss: 3.488701 | lr: 5.801018e-04\n",
            "step 983 | loss: 3.153355 | lr: 5.800441e-04\n",
            "step 984 | loss: 4.059901 | lr: 5.799863e-04\n",
            "step 985 | loss: 4.544763 | lr: 5.799284e-04\n",
            "step 986 | loss: 4.491437 | lr: 5.798705e-04\n",
            "step 987 | loss: 4.485952 | lr: 5.798125e-04\n",
            "step 988 | loss: 3.907336 | lr: 5.797544e-04\n",
            "step 989 | loss: 4.034613 | lr: 5.796962e-04\n",
            "step 990 | loss: 4.554683 | lr: 5.796379e-04\n",
            "step 991 | loss: 4.413468 | lr: 5.795796e-04\n",
            "step 992 | loss: 4.264469 | lr: 5.795211e-04\n",
            "step 993 | loss: 4.181331 | lr: 5.794626e-04\n",
            "step 994 | loss: 4.170487 | lr: 5.794040e-04\n",
            "step 995 | loss: 4.264349 | lr: 5.793454e-04\n",
            "step 996 | loss: 4.018919 | lr: 5.792866e-04\n",
            "step 997 | loss: 4.238951 | lr: 5.792278e-04\n",
            "step 998 | loss: 3.951892 | lr: 5.791689e-04\n",
            "step 999 | loss: 3.870750 | lr: 5.791099e-04\n",
            "step 1000 | loss: 3.804646 | lr: 5.790508e-04\n",
            "step 1001 | loss: 3.448138 | lr: 5.789917e-04\n",
            "step 1002 | loss: 3.820370 | lr: 5.789324e-04\n",
            "step 1003 | loss: 3.710980 | lr: 5.788731e-04\n",
            "step 1004 | loss: 3.734190 | lr: 5.788137e-04\n",
            "step 1005 | loss: 3.743098 | lr: 5.787543e-04\n",
            "step 1006 | loss: 4.055462 | lr: 5.786947e-04\n",
            "step 1007 | loss: 3.789482 | lr: 5.786351e-04\n",
            "step 1008 | loss: 3.940352 | lr: 5.785754e-04\n",
            "step 1009 | loss: 3.886795 | lr: 5.785156e-04\n",
            "step 1010 | loss: 4.153651 | lr: 5.784557e-04\n",
            "step 1011 | loss: 4.102542 | lr: 5.783957e-04\n",
            "step 1012 | loss: 3.947847 | lr: 5.783357e-04\n",
            "step 1013 | loss: 4.447884 | lr: 5.782756e-04\n",
            "step 1014 | loss: 4.130473 | lr: 5.782154e-04\n",
            "step 1015 | loss: 3.862626 | lr: 5.781551e-04\n",
            "step 1016 | loss: 4.046204 | lr: 5.780948e-04\n",
            "step 1017 | loss: 3.915540 | lr: 5.780344e-04\n",
            "step 1018 | loss: 4.035043 | lr: 5.779738e-04\n",
            "step 1019 | loss: 3.848789 | lr: 5.779133e-04\n",
            "step 1020 | loss: 3.950635 | lr: 5.778526e-04\n",
            "step 1021 | loss: 4.013051 | lr: 5.777918e-04\n",
            "step 1022 | loss: 3.727127 | lr: 5.777310e-04\n",
            "step 1023 | loss: 3.585578 | lr: 5.776701e-04\n",
            "step 1024 | loss: 3.716728 | lr: 5.776091e-04\n",
            "step 1025 | loss: 3.668813 | lr: 5.775480e-04\n",
            "step 1026 | loss: 4.077325 | lr: 5.774869e-04\n",
            "step 1027 | loss: 4.067021 | lr: 5.774256e-04\n",
            "step 1028 | loss: 3.750827 | lr: 5.773643e-04\n",
            "step 1029 | loss: 3.686045 | lr: 5.773029e-04\n",
            "step 1030 | loss: 4.187713 | lr: 5.772415e-04\n",
            "step 1031 | loss: 3.734484 | lr: 5.771799e-04\n",
            "step 1032 | loss: 3.540632 | lr: 5.771183e-04\n",
            "step 1033 | loss: 3.521007 | lr: 5.770566e-04\n",
            "step 1034 | loss: 4.166011 | lr: 5.769948e-04\n",
            "step 1035 | loss: 4.041934 | lr: 5.769329e-04\n",
            "step 1036 | loss: 4.008240 | lr: 5.768710e-04\n",
            "step 1037 | loss: 4.362882 | lr: 5.768090e-04\n",
            "step 1038 | loss: 4.221824 | lr: 5.767469e-04\n",
            "step 1039 | loss: 4.182767 | lr: 5.766847e-04\n",
            "step 1040 | loss: 4.106398 | lr: 5.766224e-04\n",
            "step 1041 | loss: 4.259361 | lr: 5.765601e-04\n",
            "step 1042 | loss: 4.158519 | lr: 5.764976e-04\n",
            "step 1043 | loss: 3.906415 | lr: 5.764351e-04\n",
            "step 1044 | loss: 3.898642 | lr: 5.763726e-04\n",
            "step 1045 | loss: 4.247213 | lr: 5.763099e-04\n",
            "step 1046 | loss: 4.056480 | lr: 5.762472e-04\n",
            "step 1047 | loss: 4.021353 | lr: 5.761843e-04\n",
            "step 1048 | loss: 4.045678 | lr: 5.761214e-04\n",
            "step 1049 | loss: 4.015718 | lr: 5.760585e-04\n",
            "step 1050 | loss: 3.846607 | lr: 5.759954e-04\n",
            "step 1051 | loss: 3.790682 | lr: 5.759323e-04\n",
            "step 1052 | loss: 3.398058 | lr: 5.758690e-04\n",
            "step 1053 | loss: 4.127631 | lr: 5.758057e-04\n",
            "step 1054 | loss: 4.046519 | lr: 5.757424e-04\n",
            "step 1055 | loss: 4.210487 | lr: 5.756789e-04\n",
            "step 1056 | loss: 4.299765 | lr: 5.756154e-04\n",
            "step 1057 | loss: 4.062703 | lr: 5.755518e-04\n",
            "step 1058 | loss: 4.246297 | lr: 5.754881e-04\n",
            "step 1059 | loss: 3.936556 | lr: 5.754243e-04\n",
            "step 1060 | loss: 3.750909 | lr: 5.753605e-04\n",
            "step 1061 | loss: 4.300765 | lr: 5.752965e-04\n",
            "step 1062 | loss: 3.655412 | lr: 5.752325e-04\n",
            "step 1063 | loss: 3.948596 | lr: 5.751684e-04\n",
            "step 1064 | loss: 3.724960 | lr: 5.751043e-04\n",
            "step 1065 | loss: 4.238548 | lr: 5.750400e-04\n",
            "step 1066 | loss: 3.928931 | lr: 5.749757e-04\n",
            "step 1067 | loss: 3.901883 | lr: 5.749113e-04\n",
            "step 1068 | loss: 3.710593 | lr: 5.748468e-04\n",
            "step 1069 | loss: 3.702365 | lr: 5.747822e-04\n",
            "step 1070 | loss: 3.728585 | lr: 5.747176e-04\n",
            "step 1071 | loss: 3.936698 | lr: 5.746529e-04\n",
            "step 1072 | loss: 4.201946 | lr: 5.745881e-04\n",
            "step 1073 | loss: 3.910222 | lr: 5.745232e-04\n",
            "step 1074 | loss: 3.980462 | lr: 5.744583e-04\n",
            "step 1075 | loss: 3.768315 | lr: 5.743932e-04\n",
            "step 1076 | loss: 3.708708 | lr: 5.743281e-04\n",
            "step 1077 | loss: 4.125299 | lr: 5.742629e-04\n",
            "step 1078 | loss: 4.231565 | lr: 5.741976e-04\n",
            "step 1079 | loss: 4.153378 | lr: 5.741323e-04\n",
            "step 1080 | loss: 3.806905 | lr: 5.740669e-04\n",
            "step 1081 | loss: 4.284371 | lr: 5.740014e-04\n",
            "step 1082 | loss: 4.003606 | lr: 5.739358e-04\n",
            "step 1083 | loss: 3.833543 | lr: 5.738701e-04\n",
            "step 1084 | loss: 3.495020 | lr: 5.738044e-04\n",
            "step 1085 | loss: 4.055490 | lr: 5.737385e-04\n",
            "step 1086 | loss: 3.869936 | lr: 5.736726e-04\n",
            "step 1087 | loss: 3.889779 | lr: 5.736067e-04\n",
            "step 1088 | loss: 3.781338 | lr: 5.735406e-04\n",
            "step 1089 | loss: 3.668314 | lr: 5.734745e-04\n",
            "step 1090 | loss: 3.676215 | lr: 5.734083e-04\n",
            "step 1091 | loss: 3.936955 | lr: 5.733420e-04\n",
            "step 1092 | loss: 3.461825 | lr: 5.732756e-04\n",
            "step 1093 | loss: 4.047766 | lr: 5.732091e-04\n",
            "step 1094 | loss: 4.488153 | lr: 5.731426e-04\n",
            "step 1095 | loss: 4.413556 | lr: 5.730760e-04\n",
            "step 1096 | loss: 4.293519 | lr: 5.730093e-04\n",
            "step 1097 | loss: 4.167739 | lr: 5.729426e-04\n",
            "step 1098 | loss: 4.151402 | lr: 5.728757e-04\n",
            "step 1099 | loss: 3.908352 | lr: 5.728088e-04\n",
            "step 1100 | loss: 4.283084 | lr: 5.727418e-04\n",
            "step 1101 | loss: 4.208731 | lr: 5.726747e-04\n",
            "step 1102 | loss: 4.550642 | lr: 5.726076e-04\n",
            "step 1103 | loss: 4.626740 | lr: 5.725403e-04\n",
            "step 1104 | loss: 4.313248 | lr: 5.724730e-04\n",
            "step 1105 | loss: 4.531610 | lr: 5.724056e-04\n",
            "step 1106 | loss: 4.086013 | lr: 5.723382e-04\n",
            "step 1107 | loss: 4.086690 | lr: 5.722706e-04\n",
            "step 1108 | loss: 4.101481 | lr: 5.722030e-04\n",
            "step 1109 | loss: 4.144229 | lr: 5.721353e-04\n",
            "step 1110 | loss: 4.159835 | lr: 5.720675e-04\n",
            "step 1111 | loss: 4.300029 | lr: 5.719996e-04\n",
            "step 1112 | loss: 4.066831 | lr: 5.719317e-04\n",
            "step 1113 | loss: 4.052146 | lr: 5.718637e-04\n",
            "step 1114 | loss: 3.934983 | lr: 5.717956e-04\n",
            "step 1115 | loss: 4.181524 | lr: 5.717274e-04\n",
            "step 1116 | loss: 4.007767 | lr: 5.716592e-04\n",
            "step 1117 | loss: 3.707439 | lr: 5.715908e-04\n",
            "step 1118 | loss: 3.574522 | lr: 5.715224e-04\n",
            "step 1119 | loss: 3.830349 | lr: 5.714540e-04\n",
            "step 1120 | loss: 3.853944 | lr: 5.713854e-04\n",
            "step 1121 | loss: 3.757545 | lr: 5.713168e-04\n",
            "step 1122 | loss: 4.121292 | lr: 5.712480e-04\n",
            "step 1123 | loss: 3.627996 | lr: 5.711793e-04\n",
            "step 1124 | loss: 3.940228 | lr: 5.711104e-04\n",
            "step 1125 | loss: 3.483830 | lr: 5.710414e-04\n",
            "step 1126 | loss: 3.828735 | lr: 5.709724e-04\n",
            "step 1127 | loss: 3.634578 | lr: 5.709033e-04\n",
            "step 1128 | loss: 3.576455 | lr: 5.708341e-04\n",
            "step 1129 | loss: 3.668639 | lr: 5.707649e-04\n",
            "step 1130 | loss: 3.464385 | lr: 5.706955e-04\n",
            "step 1131 | loss: 3.617789 | lr: 5.706261e-04\n",
            "step 1132 | loss: 4.425079 | lr: 5.705566e-04\n",
            "step 1133 | loss: 4.134158 | lr: 5.704870e-04\n",
            "step 1134 | loss: 4.113883 | lr: 5.704174e-04\n",
            "step 1135 | loss: 4.007809 | lr: 5.703477e-04\n",
            "step 1136 | loss: 3.662176 | lr: 5.702779e-04\n",
            "step 1137 | loss: 3.708487 | lr: 5.702080e-04\n",
            "step 1138 | loss: 3.534516 | lr: 5.701380e-04\n",
            "step 1139 | loss: 3.780134 | lr: 5.700680e-04\n",
            "step 1140 | loss: 3.788925 | lr: 5.699979e-04\n",
            "step 1141 | loss: 3.762758 | lr: 5.699277e-04\n",
            "step 1142 | loss: 3.744277 | lr: 5.698574e-04\n",
            "step 1143 | loss: 3.829547 | lr: 5.697871e-04\n",
            "step 1144 | loss: 3.372587 | lr: 5.697166e-04\n",
            "step 1145 | loss: 3.498956 | lr: 5.696461e-04\n",
            "step 1146 | loss: 3.438035 | lr: 5.695755e-04\n",
            "step 1147 | loss: 3.318847 | lr: 5.695049e-04\n",
            "step 1148 | loss: 3.002935 | lr: 5.694342e-04\n",
            "step 1149 | loss: 3.882859 | lr: 5.693633e-04\n",
            "step 1150 | loss: 4.332545 | lr: 5.692925e-04\n",
            "step 1151 | loss: 4.261068 | lr: 5.692215e-04\n",
            "step 1152 | loss: 4.263720 | lr: 5.691505e-04\n",
            "step 1153 | loss: 3.682999 | lr: 5.690793e-04\n",
            "step 1154 | loss: 3.824739 | lr: 5.690081e-04\n",
            "step 1155 | loss: 4.367939 | lr: 5.689369e-04\n",
            "step 1156 | loss: 4.208230 | lr: 5.688655e-04\n",
            "step 1157 | loss: 4.111609 | lr: 5.687941e-04\n",
            "step 1158 | loss: 3.979151 | lr: 5.687226e-04\n",
            "step 1159 | loss: 4.020236 | lr: 5.686510e-04\n",
            "step 1160 | loss: 4.067512 | lr: 5.685793e-04\n",
            "step 1161 | loss: 3.804758 | lr: 5.685076e-04\n",
            "step 1162 | loss: 4.037354 | lr: 5.684358e-04\n",
            "step 1163 | loss: 3.779315 | lr: 5.683639e-04\n",
            "step 1164 | loss: 3.726597 | lr: 5.682919e-04\n",
            "step 1165 | loss: 3.654880 | lr: 5.682199e-04\n",
            "step 1166 | loss: 3.298178 | lr: 5.681478e-04\n",
            "step 1167 | loss: 3.664111 | lr: 5.680756e-04\n",
            "step 1168 | loss: 3.544970 | lr: 5.680033e-04\n",
            "step 1169 | loss: 3.588402 | lr: 5.679309e-04\n",
            "step 1170 | loss: 3.595665 | lr: 5.678585e-04\n",
            "step 1171 | loss: 3.877315 | lr: 5.677860e-04\n",
            "step 1172 | loss: 3.632683 | lr: 5.677134e-04\n",
            "step 1173 | loss: 3.770339 | lr: 5.676408e-04\n",
            "step 1174 | loss: 3.714724 | lr: 5.675680e-04\n",
            "step 1175 | loss: 3.956594 | lr: 5.674952e-04\n",
            "step 1176 | loss: 3.907170 | lr: 5.674223e-04\n",
            "step 1177 | loss: 3.804456 | lr: 5.673494e-04\n",
            "step 1178 | loss: 4.310465 | lr: 5.672763e-04\n",
            "step 1179 | loss: 3.966532 | lr: 5.672032e-04\n",
            "step 1180 | loss: 3.693561 | lr: 5.671300e-04\n",
            "step 1181 | loss: 3.854812 | lr: 5.670567e-04\n",
            "step 1182 | loss: 3.754688 | lr: 5.669834e-04\n",
            "step 1183 | loss: 3.883758 | lr: 5.669099e-04\n",
            "step 1184 | loss: 3.675715 | lr: 5.668364e-04\n",
            "step 1185 | loss: 3.807927 | lr: 5.667629e-04\n",
            "step 1186 | loss: 3.851177 | lr: 5.666892e-04\n",
            "step 1187 | loss: 3.529279 | lr: 5.666155e-04\n",
            "step 1188 | loss: 3.459917 | lr: 5.665417e-04\n",
            "step 1189 | loss: 3.574962 | lr: 5.664678e-04\n",
            "step 1190 | loss: 3.547797 | lr: 5.663938e-04\n",
            "step 1191 | loss: 3.888762 | lr: 5.663198e-04\n",
            "step 1192 | loss: 3.900083 | lr: 5.662457e-04\n",
            "step 1193 | loss: 3.614609 | lr: 5.661715e-04\n",
            "step 1194 | loss: 3.507102 | lr: 5.660972e-04\n",
            "step 1195 | loss: 4.033226 | lr: 5.660229e-04\n",
            "step 1196 | loss: 3.630633 | lr: 5.659485e-04\n",
            "step 1197 | loss: 3.402727 | lr: 5.658740e-04\n",
            "step 1198 | loss: 3.359626 | lr: 5.657994e-04\n",
            "step 1199 | loss: 3.952955 | lr: 5.657247e-04\n",
            "step 1200 | loss: 3.848994 | lr: 5.656500e-04\n",
            "step 1201 | loss: 3.827429 | lr: 5.655752e-04\n",
            "step 1202 | loss: 4.191442 | lr: 5.655003e-04\n",
            "step 1203 | loss: 4.029387 | lr: 5.654254e-04\n",
            "step 1204 | loss: 4.013004 | lr: 5.653504e-04\n",
            "step 1205 | loss: 3.891798 | lr: 5.652753e-04\n",
            "step 1206 | loss: 4.052723 | lr: 5.652001e-04\n",
            "step 1207 | loss: 3.964976 | lr: 5.651248e-04\n",
            "step 1208 | loss: 3.740883 | lr: 5.650495e-04\n",
            "step 1209 | loss: 3.733296 | lr: 5.649741e-04\n",
            "step 1210 | loss: 4.076257 | lr: 5.648986e-04\n",
            "step 1211 | loss: 3.887686 | lr: 5.648230e-04\n",
            "step 1212 | loss: 3.836195 | lr: 5.647474e-04\n",
            "step 1213 | loss: 3.865477 | lr: 5.646717e-04\n",
            "step 1214 | loss: 3.818641 | lr: 5.645959e-04\n",
            "step 1215 | loss: 3.687173 | lr: 5.645200e-04\n",
            "step 1216 | loss: 3.644540 | lr: 5.644441e-04\n",
            "step 1217 | loss: 3.225704 | lr: 5.643681e-04\n",
            "step 1218 | loss: 3.958253 | lr: 5.642920e-04\n",
            "step 1219 | loss: 3.893138 | lr: 5.642158e-04\n",
            "step 1220 | loss: 3.954288 | lr: 5.641396e-04\n",
            "step 1221 | loss: 4.039805 | lr: 5.640633e-04\n",
            "step 1222 | loss: 3.831726 | lr: 5.639869e-04\n",
            "step 1223 | loss: 4.052884 | lr: 5.639104e-04\n",
            "step 1224 | loss: 3.746277 | lr: 5.638339e-04\n",
            "step 1225 | loss: 3.597920 | lr: 5.637572e-04\n",
            "step 1226 | loss: 4.101899 | lr: 5.636805e-04\n",
            "step 1227 | loss: 3.475596 | lr: 5.636038e-04\n",
            "step 1228 | loss: 3.755461 | lr: 5.635269e-04\n",
            "step 1229 | loss: 3.541653 | lr: 5.634500e-04\n",
            "step 1230 | loss: 4.061376 | lr: 5.633730e-04\n",
            "step 1231 | loss: 3.759663 | lr: 5.632959e-04\n",
            "step 1232 | loss: 3.718164 | lr: 5.632188e-04\n",
            "step 1233 | loss: 3.546980 | lr: 5.631416e-04\n",
            "step 1234 | loss: 3.531732 | lr: 5.630643e-04\n",
            "step 1235 | loss: 3.555943 | lr: 5.629869e-04\n",
            "step 1236 | loss: 3.809990 | lr: 5.629094e-04\n",
            "step 1237 | loss: 4.055218 | lr: 5.628319e-04\n",
            "step 1238 | loss: 3.741359 | lr: 5.627543e-04\n",
            "step 1239 | loss: 3.829970 | lr: 5.626766e-04\n",
            "step 1240 | loss: 3.744181 | lr: 5.625989e-04\n",
            "step 1241 | loss: 3.613880 | lr: 5.625211e-04\n",
            "step 1242 | loss: 3.965168 | lr: 5.624432e-04\n",
            "step 1243 | loss: 4.099328 | lr: 5.623652e-04\n",
            "step 1244 | loss: 4.009023 | lr: 5.622871e-04\n",
            "step 1245 | loss: 3.670369 | lr: 5.622090e-04\n",
            "step 1246 | loss: 4.119421 | lr: 5.621308e-04\n",
            "step 1247 | loss: 3.862127 | lr: 5.620525e-04\n",
            "step 1248 | loss: 3.685395 | lr: 5.619742e-04\n",
            "step 1249 | loss: 3.285646 | lr: 5.618957e-04\n",
            "step 1250 | loss: 3.922855 | lr: 5.618172e-04\n",
            "step 1251 | loss: 3.697882 | lr: 5.617387e-04\n",
            "step 1252 | loss: 3.773863 | lr: 5.616600e-04\n",
            "step 1253 | loss: 3.622507 | lr: 5.615813e-04\n",
            "step 1254 | loss: 3.529646 | lr: 5.615025e-04\n",
            "step 1255 | loss: 3.486950 | lr: 5.614236e-04\n",
            "step 1256 | loss: 3.803982 | lr: 5.613447e-04\n",
            "step 1257 | loss: 3.339206 | lr: 5.612656e-04\n",
            "step 1258 | loss: 3.785361 | lr: 5.611865e-04\n",
            "step 1259 | loss: 4.260077 | lr: 5.611073e-04\n",
            "step 1260 | loss: 4.153549 | lr: 5.610281e-04\n",
            "step 1261 | loss: 4.060224 | lr: 5.609488e-04\n",
            "step 1262 | loss: 3.917980 | lr: 5.608694e-04\n",
            "step 1263 | loss: 3.930681 | lr: 5.607899e-04\n",
            "step 1264 | loss: 3.726393 | lr: 5.607103e-04\n",
            "step 1265 | loss: 4.028282 | lr: 5.606307e-04\n",
            "step 1266 | loss: 3.972326 | lr: 5.605510e-04\n",
            "step 1267 | loss: 4.332129 | lr: 5.604712e-04\n",
            "step 1268 | loss: 4.399802 | lr: 5.603914e-04\n",
            "step 1269 | loss: 4.079776 | lr: 5.603115e-04\n",
            "step 1270 | loss: 4.349045 | lr: 5.602315e-04\n",
            "step 1271 | loss: 3.919541 | lr: 5.601514e-04\n",
            "step 1272 | loss: 3.928600 | lr: 5.600713e-04\n",
            "step 1273 | loss: 3.863280 | lr: 5.599910e-04\n",
            "step 1274 | loss: 3.921900 | lr: 5.599107e-04\n",
            "step 1275 | loss: 3.970844 | lr: 5.598304e-04\n",
            "step 1276 | loss: 4.111572 | lr: 5.597499e-04\n",
            "step 1277 | loss: 3.901197 | lr: 5.596694e-04\n",
            "step 1278 | loss: 3.877858 | lr: 5.595888e-04\n",
            "step 1279 | loss: 3.740338 | lr: 5.595082e-04\n",
            "step 1280 | loss: 4.005701 | lr: 5.594274e-04\n",
            "step 1281 | loss: 3.787820 | lr: 5.593466e-04\n",
            "step 1282 | loss: 3.500101 | lr: 5.592657e-04\n",
            "step 1283 | loss: 3.412833 | lr: 5.591847e-04\n",
            "step 1284 | loss: 3.647839 | lr: 5.591037e-04\n",
            "step 1285 | loss: 3.694195 | lr: 5.590226e-04\n",
            "step 1286 | loss: 3.622716 | lr: 5.589414e-04\n",
            "step 1287 | loss: 3.981816 | lr: 5.588601e-04\n",
            "step 1288 | loss: 3.434557 | lr: 5.587788e-04\n",
            "step 1289 | loss: 3.810635 | lr: 5.586974e-04\n",
            "step 1290 | loss: 3.331572 | lr: 5.586159e-04\n",
            "step 1291 | loss: 3.704661 | lr: 5.585344e-04\n",
            "step 1292 | loss: 3.478362 | lr: 5.584527e-04\n",
            "step 1293 | loss: 3.439195 | lr: 5.583710e-04\n",
            "step 1294 | loss: 3.532327 | lr: 5.582893e-04\n",
            "step 1295 | loss: 3.332293 | lr: 5.582074e-04\n",
            "step 1296 | loss: 3.424367 | lr: 5.581255e-04\n",
            "step 1297 | loss: 4.222201 | lr: 5.580435e-04\n",
            "step 1298 | loss: 3.959293 | lr: 5.579614e-04\n",
            "step 1299 | loss: 3.905746 | lr: 5.578793e-04\n",
            "step 1300 | loss: 3.804888 | lr: 5.577970e-04\n",
            "step 1301 | loss: 3.471635 | lr: 5.577147e-04\n",
            "step 1302 | loss: 3.547220 | lr: 5.576324e-04\n",
            "step 1303 | loss: 3.386271 | lr: 5.575499e-04\n",
            "step 1304 | loss: 3.599262 | lr: 5.574674e-04\n",
            "step 1305 | loss: 3.602804 | lr: 5.573848e-04\n",
            "step 1306 | loss: 3.547926 | lr: 5.573022e-04\n",
            "step 1307 | loss: 3.528908 | lr: 5.572194e-04\n",
            "step 1308 | loss: 3.615997 | lr: 5.571366e-04\n",
            "step 1309 | loss: 3.221677 | lr: 5.570537e-04\n",
            "step 1310 | loss: 3.331513 | lr: 5.569708e-04\n",
            "step 1311 | loss: 3.246621 | lr: 5.568877e-04\n",
            "step 1312 | loss: 3.156038 | lr: 5.568046e-04\n",
            "step 1313 | loss: 2.843098 | lr: 5.567214e-04\n",
            "step 1314 | loss: 3.694736 | lr: 5.566382e-04\n",
            "step 1315 | loss: 4.154631 | lr: 5.565549e-04\n",
            "step 1316 | loss: 4.054492 | lr: 5.564715e-04\n",
            "step 1317 | loss: 4.082941 | lr: 5.563880e-04\n",
            "step 1318 | loss: 3.505868 | lr: 5.563045e-04\n",
            "step 1319 | loss: 3.629626 | lr: 5.562208e-04\n",
            "step 1320 | loss: 4.196362 | lr: 5.561371e-04\n",
            "step 1321 | loss: 4.004223 | lr: 5.560534e-04\n",
            "step 1322 | loss: 3.922362 | lr: 5.559695e-04\n",
            "step 1323 | loss: 3.858085 | lr: 5.558856e-04\n",
            "step 1324 | loss: 3.835234 | lr: 5.558016e-04\n",
            "step 1325 | loss: 3.864941 | lr: 5.557176e-04\n",
            "step 1326 | loss: 3.615947 | lr: 5.556335e-04\n",
            "step 1327 | loss: 3.860555 | lr: 5.555493e-04\n",
            "step 1328 | loss: 3.594249 | lr: 5.554650e-04\n",
            "step 1329 | loss: 3.562226 | lr: 5.553806e-04\n",
            "step 1330 | loss: 3.510424 | lr: 5.552962e-04\n",
            "step 1331 | loss: 3.116146 | lr: 5.552117e-04\n",
            "step 1332 | loss: 3.470758 | lr: 5.551271e-04\n",
            "step 1333 | loss: 3.347210 | lr: 5.550425e-04\n",
            "step 1334 | loss: 3.407399 | lr: 5.549578e-04\n",
            "step 1335 | loss: 3.406893 | lr: 5.548730e-04\n",
            "step 1336 | loss: 3.657133 | lr: 5.547881e-04\n",
            "step 1337 | loss: 3.437670 | lr: 5.547032e-04\n",
            "step 1338 | loss: 3.586313 | lr: 5.546182e-04\n",
            "step 1339 | loss: 3.523420 | lr: 5.545331e-04\n",
            "step 1340 | loss: 3.765793 | lr: 5.544480e-04\n",
            "step 1341 | loss: 3.678568 | lr: 5.543627e-04\n",
            "step 1342 | loss: 3.605610 | lr: 5.542774e-04\n",
            "step 1343 | loss: 4.071408 | lr: 5.541921e-04\n",
            "step 1344 | loss: 3.759243 | lr: 5.541066e-04\n",
            "step 1345 | loss: 3.475282 | lr: 5.540211e-04\n",
            "step 1346 | loss: 3.607800 | lr: 5.539355e-04\n",
            "step 1347 | loss: 3.514555 | lr: 5.538499e-04\n",
            "step 1348 | loss: 3.684253 | lr: 5.537641e-04\n",
            "step 1349 | loss: 3.484732 | lr: 5.536783e-04\n",
            "step 1350 | loss: 3.587835 | lr: 5.535924e-04\n",
            "step 1351 | loss: 3.655791 | lr: 5.535065e-04\n",
            "step 1352 | loss: 3.358848 | lr: 5.534205e-04\n",
            "step 1353 | loss: 3.275205 | lr: 5.533344e-04\n",
            "step 1354 | loss: 3.409679 | lr: 5.532482e-04\n",
            "step 1355 | loss: 3.366096 | lr: 5.531620e-04\n",
            "step 1356 | loss: 3.698746 | lr: 5.530757e-04\n",
            "step 1357 | loss: 3.740943 | lr: 5.529893e-04\n",
            "step 1358 | loss: 3.422942 | lr: 5.529028e-04\n",
            "step 1359 | loss: 3.402447 | lr: 5.528163e-04\n",
            "step 1360 | loss: 3.825751 | lr: 5.527297e-04\n",
            "step 1361 | loss: 3.416723 | lr: 5.526430e-04\n",
            "step 1362 | loss: 3.256034 | lr: 5.525563e-04\n",
            "step 1363 | loss: 3.209380 | lr: 5.524694e-04\n",
            "step 1364 | loss: 3.787248 | lr: 5.523826e-04\n",
            "step 1365 | loss: 3.690326 | lr: 5.522956e-04\n",
            "step 1366 | loss: 3.647763 | lr: 5.522086e-04\n",
            "step 1367 | loss: 3.989165 | lr: 5.521214e-04\n",
            "step 1368 | loss: 3.901735 | lr: 5.520343e-04\n",
            "step 1369 | loss: 3.819235 | lr: 5.519470e-04\n",
            "step 1370 | loss: 3.649320 | lr: 5.518597e-04\n",
            "step 1371 | loss: 3.849582 | lr: 5.517723e-04\n",
            "step 1372 | loss: 3.763288 | lr: 5.516848e-04\n",
            "step 1373 | loss: 3.533782 | lr: 5.515973e-04\n",
            "step 1374 | loss: 3.550557 | lr: 5.515097e-04\n",
            "step 1375 | loss: 3.846555 | lr: 5.514220e-04\n",
            "step 1376 | loss: 3.669657 | lr: 5.513343e-04\n",
            "step 1377 | loss: 3.635146 | lr: 5.512464e-04\n",
            "step 1378 | loss: 3.648064 | lr: 5.511585e-04\n",
            "step 1379 | loss: 3.617296 | lr: 5.510706e-04\n",
            "step 1380 | loss: 3.516257 | lr: 5.509825e-04\n",
            "step 1381 | loss: 3.513237 | lr: 5.508944e-04\n",
            "step 1382 | loss: 3.086098 | lr: 5.508062e-04\n",
            "step 1383 | loss: 3.764252 | lr: 5.507180e-04\n",
            "step 1384 | loss: 3.697333 | lr: 5.506296e-04\n",
            "step 1385 | loss: 3.809868 | lr: 5.505412e-04\n",
            "step 1386 | loss: 3.829439 | lr: 5.504528e-04\n",
            "step 1387 | loss: 3.676302 | lr: 5.503642e-04\n",
            "step 1388 | loss: 3.848459 | lr: 5.502756e-04\n",
            "step 1389 | loss: 3.575439 | lr: 5.501869e-04\n",
            "step 1390 | loss: 3.406164 | lr: 5.500982e-04\n",
            "step 1391 | loss: 3.911074 | lr: 5.500093e-04\n",
            "step 1392 | loss: 3.354003 | lr: 5.499204e-04\n",
            "step 1393 | loss: 3.595801 | lr: 5.498315e-04\n",
            "step 1394 | loss: 3.342780 | lr: 5.497424e-04\n",
            "step 1395 | loss: 3.862062 | lr: 5.496533e-04\n",
            "step 1396 | loss: 3.550338 | lr: 5.495641e-04\n",
            "step 1397 | loss: 3.551235 | lr: 5.494749e-04\n",
            "step 1398 | loss: 3.433765 | lr: 5.493855e-04\n",
            "step 1399 | loss: 3.375920 | lr: 5.492961e-04\n",
            "step 1400 | loss: 3.364721 | lr: 5.492067e-04\n",
            "step 1401 | loss: 3.590813 | lr: 5.491171e-04\n",
            "step 1402 | loss: 3.859677 | lr: 5.490275e-04\n",
            "step 1403 | loss: 3.568386 | lr: 5.489378e-04\n",
            "step 1404 | loss: 3.615433 | lr: 5.488481e-04\n",
            "step 1405 | loss: 3.491802 | lr: 5.487583e-04\n",
            "step 1406 | loss: 3.366607 | lr: 5.486684e-04\n",
            "step 1407 | loss: 3.767848 | lr: 5.485784e-04\n",
            "step 1408 | loss: 3.939001 | lr: 5.484884e-04\n",
            "step 1409 | loss: 3.823539 | lr: 5.483983e-04\n",
            "step 1410 | loss: 3.486039 | lr: 5.483081e-04\n",
            "step 1411 | loss: 3.883483 | lr: 5.482178e-04\n",
            "step 1412 | loss: 3.633147 | lr: 5.481275e-04\n",
            "step 1413 | loss: 3.512407 | lr: 5.480371e-04\n",
            "step 1414 | loss: 3.177315 | lr: 5.479466e-04\n",
            "step 1415 | loss: 3.683905 | lr: 5.478561e-04\n",
            "step 1416 | loss: 3.526824 | lr: 5.477655e-04\n",
            "step 1417 | loss: 3.557320 | lr: 5.476748e-04\n",
            "step 1418 | loss: 3.405130 | lr: 5.475841e-04\n",
            "step 1419 | loss: 3.337384 | lr: 5.474933e-04\n",
            "step 1420 | loss: 3.300590 | lr: 5.474024e-04\n",
            "step 1421 | loss: 3.616879 | lr: 5.473114e-04\n",
            "step 1422 | loss: 3.122155 | lr: 5.472204e-04\n",
            "step 1423 | loss: 3.537078 | lr: 5.471293e-04\n",
            "step 1424 | loss: 3.975630 | lr: 5.470381e-04\n",
            "step 1425 | loss: 3.870149 | lr: 5.469469e-04\n",
            "step 1426 | loss: 3.818490 | lr: 5.468556e-04\n",
            "step 1427 | loss: 3.723472 | lr: 5.467642e-04\n",
            "step 1428 | loss: 3.738954 | lr: 5.466727e-04\n",
            "step 1429 | loss: 3.521357 | lr: 5.465812e-04\n",
            "step 1430 | loss: 3.827518 | lr: 5.464896e-04\n",
            "step 1431 | loss: 3.774965 | lr: 5.463980e-04\n",
            "step 1432 | loss: 4.221540 | lr: 5.463062e-04\n",
            "step 1433 | loss: 4.297390 | lr: 5.462144e-04\n",
            "step 1434 | loss: 3.905772 | lr: 5.461226e-04\n",
            "step 1435 | loss: 4.157845 | lr: 5.460306e-04\n",
            "step 1436 | loss: 3.771441 | lr: 5.459386e-04\n",
            "step 1437 | loss: 3.811196 | lr: 5.458465e-04\n",
            "step 1438 | loss: 3.703638 | lr: 5.457544e-04\n",
            "step 1439 | loss: 3.712947 | lr: 5.456621e-04\n",
            "step 1440 | loss: 3.879812 | lr: 5.455698e-04\n",
            "step 1441 | loss: 3.914009 | lr: 5.454775e-04\n",
            "step 1442 | loss: 3.707314 | lr: 5.453850e-04\n",
            "step 1443 | loss: 3.661418 | lr: 5.452925e-04\n",
            "step 1444 | loss: 3.593158 | lr: 5.452000e-04\n",
            "step 1445 | loss: 3.786349 | lr: 5.451073e-04\n",
            "step 1446 | loss: 3.602808 | lr: 5.450146e-04\n",
            "step 1447 | loss: 3.339352 | lr: 5.449218e-04\n",
            "step 1448 | loss: 3.263268 | lr: 5.448290e-04\n",
            "step 1449 | loss: 3.452988 | lr: 5.447360e-04\n",
            "step 1450 | loss: 3.451515 | lr: 5.446431e-04\n",
            "step 1451 | loss: 3.452384 | lr: 5.445500e-04\n",
            "step 1452 | loss: 3.788148 | lr: 5.444569e-04\n",
            "step 1453 | loss: 3.231616 | lr: 5.443637e-04\n",
            "step 1454 | loss: 3.608668 | lr: 5.442704e-04\n",
            "step 1455 | loss: 3.159029 | lr: 5.441770e-04\n",
            "step 1456 | loss: 3.520106 | lr: 5.440836e-04\n",
            "step 1457 | loss: 3.302879 | lr: 5.439902e-04\n",
            "step 1458 | loss: 3.317551 | lr: 5.438966e-04\n",
            "step 1459 | loss: 3.370170 | lr: 5.438030e-04\n",
            "step 1460 | loss: 3.203521 | lr: 5.437093e-04\n",
            "step 1461 | loss: 3.274894 | lr: 5.436155e-04\n",
            "step 1462 | loss: 4.029417 | lr: 5.435217e-04\n",
            "step 1463 | loss: 3.775292 | lr: 5.434278e-04\n",
            "step 1464 | loss: 3.736529 | lr: 5.433338e-04\n",
            "step 1465 | loss: 3.624294 | lr: 5.432398e-04\n",
            "step 1466 | loss: 3.297579 | lr: 5.431457e-04\n",
            "step 1467 | loss: 3.336186 | lr: 5.430515e-04\n",
            "step 1468 | loss: 3.210669 | lr: 5.429573e-04\n",
            "step 1469 | loss: 3.443328 | lr: 5.428630e-04\n",
            "step 1470 | loss: 3.491221 | lr: 5.427686e-04\n",
            "step 1471 | loss: 3.367207 | lr: 5.426741e-04\n",
            "step 1472 | loss: 3.323998 | lr: 5.425796e-04\n",
            "step 1473 | loss: 3.426655 | lr: 5.424850e-04\n",
            "step 1474 | loss: 3.085244 | lr: 5.423904e-04\n",
            "step 1475 | loss: 3.215584 | lr: 5.422956e-04\n",
            "step 1476 | loss: 3.159128 | lr: 5.422008e-04\n",
            "step 1477 | loss: 2.998837 | lr: 5.421060e-04\n",
            "step 1478 | loss: 2.684479 | lr: 5.420110e-04\n",
            "step 1479 | loss: 3.467089 | lr: 5.419160e-04\n",
            "step 1480 | loss: 3.919041 | lr: 5.418209e-04\n",
            "step 1481 | loss: 3.830232 | lr: 5.417258e-04\n",
            "step 1482 | loss: 3.904518 | lr: 5.416306e-04\n",
            "step 1483 | loss: 3.372305 | lr: 5.415353e-04\n",
            "step 1484 | loss: 3.448910 | lr: 5.414400e-04\n",
            "step 1485 | loss: 3.962059 | lr: 5.413445e-04\n",
            "step 1486 | loss: 3.715782 | lr: 5.412491e-04\n",
            "step 1487 | loss: 3.712756 | lr: 5.411535e-04\n",
            "step 1488 | loss: 3.654361 | lr: 5.410579e-04\n",
            "step 1489 | loss: 3.606599 | lr: 5.409622e-04\n",
            "step 1490 | loss: 3.641147 | lr: 5.408664e-04\n",
            "step 1491 | loss: 3.438584 | lr: 5.407706e-04\n",
            "step 1492 | loss: 3.674376 | lr: 5.406747e-04\n",
            "step 1493 | loss: 3.429472 | lr: 5.405787e-04\n",
            "step 1494 | loss: 3.452651 | lr: 5.404827e-04\n",
            "step 1495 | loss: 3.330502 | lr: 5.403866e-04\n",
            "step 1496 | loss: 2.950014 | lr: 5.402904e-04\n",
            "step 1497 | loss: 3.313548 | lr: 5.401942e-04\n",
            "step 1498 | loss: 3.157866 | lr: 5.400979e-04\n",
            "step 1499 | loss: 3.221131 | lr: 5.400015e-04\n",
            "step 1500 | loss: 3.189598 | lr: 5.399050e-04\n",
            "step 1501 | loss: 3.457473 | lr: 5.398085e-04\n",
            "step 1502 | loss: 3.266479 | lr: 5.397119e-04\n",
            "step 1503 | loss: 3.436231 | lr: 5.396153e-04\n",
            "step 1504 | loss: 3.389572 | lr: 5.395186e-04\n",
            "step 1505 | loss: 3.601372 | lr: 5.394218e-04\n",
            "step 1506 | loss: 3.538179 | lr: 5.393249e-04\n",
            "step 1507 | loss: 3.427550 | lr: 5.392280e-04\n",
            "step 1508 | loss: 3.845129 | lr: 5.391310e-04\n",
            "step 1509 | loss: 3.517272 | lr: 5.390340e-04\n",
            "step 1510 | loss: 3.286629 | lr: 5.389368e-04\n",
            "step 1511 | loss: 3.441960 | lr: 5.388396e-04\n",
            "step 1512 | loss: 3.374882 | lr: 5.387424e-04\n",
            "step 1513 | loss: 3.521210 | lr: 5.386450e-04\n",
            "step 1514 | loss: 3.286132 | lr: 5.385476e-04\n",
            "step 1515 | loss: 3.404600 | lr: 5.384502e-04\n",
            "step 1516 | loss: 3.515526 | lr: 5.383526e-04\n",
            "step 1517 | loss: 3.168059 | lr: 5.382550e-04\n",
            "step 1518 | loss: 3.110427 | lr: 5.381574e-04\n",
            "step 1519 | loss: 3.233039 | lr: 5.380596e-04\n",
            "step 1520 | loss: 3.169846 | lr: 5.379618e-04\n",
            "step 1521 | loss: 3.488234 | lr: 5.378639e-04\n",
            "step 1522 | loss: 3.541304 | lr: 5.377660e-04\n",
            "step 1523 | loss: 3.283239 | lr: 5.376680e-04\n",
            "step 1524 | loss: 3.206952 | lr: 5.375699e-04\n",
            "step 1525 | loss: 3.604149 | lr: 5.374718e-04\n",
            "step 1526 | loss: 3.206198 | lr: 5.373736e-04\n",
            "step 1527 | loss: 3.118526 | lr: 5.372753e-04\n",
            "step 1528 | loss: 3.035510 | lr: 5.371769e-04\n",
            "step 1529 | loss: 3.566805 | lr: 5.370785e-04\n",
            "step 1530 | loss: 3.471097 | lr: 5.369800e-04\n",
            "step 1531 | loss: 3.428916 | lr: 5.368815e-04\n",
            "step 1532 | loss: 3.731042 | lr: 5.367829e-04\n",
            "step 1533 | loss: 3.661831 | lr: 5.366842e-04\n",
            "step 1534 | loss: 3.606196 | lr: 5.365854e-04\n",
            "step 1535 | loss: 3.471324 | lr: 5.364866e-04\n",
            "step 1536 | loss: 3.677570 | lr: 5.363877e-04\n",
            "step 1537 | loss: 3.541937 | lr: 5.362888e-04\n",
            "step 1538 | loss: 3.379823 | lr: 5.361897e-04\n",
            "step 1539 | loss: 3.370398 | lr: 5.360906e-04\n",
            "step 1540 | loss: 3.674712 | lr: 5.359915e-04\n",
            "step 1541 | loss: 3.459690 | lr: 5.358923e-04\n",
            "step 1542 | loss: 3.444773 | lr: 5.357930e-04\n",
            "step 1543 | loss: 3.452242 | lr: 5.356936e-04\n",
            "step 1544 | loss: 3.395902 | lr: 5.355942e-04\n",
            "step 1545 | loss: 3.394095 | lr: 5.354947e-04\n",
            "step 1546 | loss: 3.331811 | lr: 5.353951e-04\n",
            "step 1547 | loss: 2.945104 | lr: 5.352955e-04\n",
            "step 1548 | loss: 3.568463 | lr: 5.351958e-04\n",
            "step 1549 | loss: 3.474077 | lr: 5.350961e-04\n",
            "step 1550 | loss: 3.633580 | lr: 5.349962e-04\n",
            "step 1551 | loss: 3.644381 | lr: 5.348964e-04\n",
            "step 1552 | loss: 3.474863 | lr: 5.347964e-04\n",
            "step 1553 | loss: 3.606748 | lr: 5.346964e-04\n",
            "step 1554 | loss: 3.358214 | lr: 5.345963e-04\n",
            "step 1555 | loss: 3.252678 | lr: 5.344961e-04\n",
            "step 1556 | loss: 3.729669 | lr: 5.343959e-04\n",
            "step 1557 | loss: 3.176075 | lr: 5.342956e-04\n",
            "step 1558 | loss: 3.400365 | lr: 5.341952e-04\n",
            "step 1559 | loss: 3.171252 | lr: 5.340948e-04\n",
            "step 1560 | loss: 3.645160 | lr: 5.339943e-04\n",
            "step 1561 | loss: 3.364012 | lr: 5.338938e-04\n",
            "step 1562 | loss: 3.352000 | lr: 5.337931e-04\n",
            "step 1563 | loss: 3.235874 | lr: 5.336924e-04\n",
            "step 1564 | loss: 3.201656 | lr: 5.335917e-04\n",
            "step 1565 | loss: 3.186307 | lr: 5.334909e-04\n",
            "step 1566 | loss: 3.353329 | lr: 5.333900e-04\n",
            "step 1567 | loss: 3.633342 | lr: 5.332890e-04\n",
            "step 1568 | loss: 3.343649 | lr: 5.331880e-04\n",
            "step 1569 | loss: 3.453943 | lr: 5.330869e-04\n",
            "step 1570 | loss: 3.300698 | lr: 5.329857e-04\n",
            "step 1571 | loss: 3.166668 | lr: 5.328845e-04\n",
            "step 1572 | loss: 3.552279 | lr: 5.327832e-04\n",
            "step 1573 | loss: 3.708004 | lr: 5.326819e-04\n",
            "step 1574 | loss: 3.617252 | lr: 5.325804e-04\n",
            "step 1575 | loss: 3.314939 | lr: 5.324790e-04\n",
            "step 1576 | loss: 3.646328 | lr: 5.323774e-04\n",
            "step 1577 | loss: 3.390237 | lr: 5.322758e-04\n",
            "step 1578 | loss: 3.352801 | lr: 5.321741e-04\n",
            "step 1579 | loss: 2.991528 | lr: 5.320723e-04\n",
            "step 1580 | loss: 3.466625 | lr: 5.319705e-04\n",
            "step 1581 | loss: 3.354581 | lr: 5.318686e-04\n",
            "step 1582 | loss: 3.358816 | lr: 5.317667e-04\n",
            "step 1583 | loss: 3.185606 | lr: 5.316647e-04\n",
            "step 1584 | loss: 3.149544 | lr: 5.315626e-04\n",
            "step 1585 | loss: 3.128718 | lr: 5.314605e-04\n",
            "step 1586 | loss: 3.403563 | lr: 5.313582e-04\n",
            "step 1587 | loss: 2.929792 | lr: 5.312560e-04\n",
            "step 1588 | loss: 3.355501 | lr: 5.311536e-04\n",
            "step 1589 | loss: 3.785073 | lr: 5.310512e-04\n",
            "step 1590 | loss: 3.694519 | lr: 5.309487e-04\n",
            "step 1591 | loss: 3.614077 | lr: 5.308462e-04\n",
            "step 1592 | loss: 3.565777 | lr: 5.307436e-04\n",
            "step 1593 | loss: 3.574472 | lr: 5.306409e-04\n",
            "step 1594 | loss: 3.341716 | lr: 5.305382e-04\n",
            "step 1595 | loss: 3.697247 | lr: 5.304354e-04\n",
            "step 1596 | loss: 3.652389 | lr: 5.303325e-04\n",
            "step 1597 | loss: 4.032737 | lr: 5.302296e-04\n",
            "step 1598 | loss: 4.135719 | lr: 5.301266e-04\n",
            "step 1599 | loss: 3.694988 | lr: 5.300235e-04\n",
            "step 1600 | loss: 3.988504 | lr: 5.299204e-04\n",
            "step 1601 | loss: 3.631701 | lr: 5.298172e-04\n",
            "step 1602 | loss: 3.673567 | lr: 5.297139e-04\n",
            "step 1603 | loss: 3.505226 | lr: 5.296106e-04\n",
            "step 1604 | loss: 3.521132 | lr: 5.295072e-04\n",
            "step 1605 | loss: 3.744130 | lr: 5.294038e-04\n",
            "step 1606 | loss: 3.706148 | lr: 5.293002e-04\n",
            "step 1607 | loss: 3.545074 | lr: 5.291966e-04\n",
            "step 1608 | loss: 3.488217 | lr: 5.290930e-04\n",
            "step 1609 | loss: 3.346944 | lr: 5.289893e-04\n",
            "step 1610 | loss: 3.611242 | lr: 5.288855e-04\n",
            "step 1611 | loss: 3.406728 | lr: 5.287817e-04\n",
            "step 1612 | loss: 3.150066 | lr: 5.286777e-04\n",
            "step 1613 | loss: 3.140903 | lr: 5.285738e-04\n",
            "step 1614 | loss: 3.327215 | lr: 5.284697e-04\n",
            "step 1615 | loss: 3.289395 | lr: 5.283656e-04\n",
            "step 1616 | loss: 3.273243 | lr: 5.282615e-04\n",
            "step 1617 | loss: 3.588599 | lr: 5.281572e-04\n",
            "step 1618 | loss: 3.052989 | lr: 5.280529e-04\n",
            "step 1619 | loss: 3.381606 | lr: 5.279486e-04\n",
            "step 1620 | loss: 3.002802 | lr: 5.278441e-04\n",
            "step 1621 | loss: 3.320390 | lr: 5.277396e-04\n",
            "step 1622 | loss: 3.105545 | lr: 5.276351e-04\n",
            "step 1623 | loss: 3.103586 | lr: 5.275304e-04\n",
            "step 1624 | loss: 3.194579 | lr: 5.274258e-04\n",
            "step 1625 | loss: 3.057636 | lr: 5.273210e-04\n",
            "step 1626 | loss: 3.062315 | lr: 5.272162e-04\n",
            "step 1627 | loss: 3.776828 | lr: 5.271113e-04\n",
            "step 1628 | loss: 3.550224 | lr: 5.270064e-04\n",
            "step 1629 | loss: 3.487049 | lr: 5.269013e-04\n",
            "step 1630 | loss: 3.371055 | lr: 5.267963e-04\n",
            "step 1631 | loss: 3.115220 | lr: 5.266911e-04\n",
            "step 1632 | loss: 3.144022 | lr: 5.265859e-04\n",
            "step 1633 | loss: 2.975763 | lr: 5.264807e-04\n",
            "step 1634 | loss: 3.221414 | lr: 5.263753e-04\n",
            "step 1635 | loss: 3.281160 | lr: 5.262699e-04\n",
            "step 1636 | loss: 3.170935 | lr: 5.261645e-04\n",
            "step 1637 | loss: 3.153125 | lr: 5.260590e-04\n",
            "step 1638 | loss: 3.265819 | lr: 5.259534e-04\n",
            "step 1639 | loss: 2.929784 | lr: 5.258477e-04\n",
            "step 1640 | loss: 3.031930 | lr: 5.257420e-04\n",
            "step 1641 | loss: 2.981324 | lr: 5.256362e-04\n",
            "step 1642 | loss: 2.811701 | lr: 5.255304e-04\n",
            "step 1643 | loss: 2.533520 | lr: 5.254245e-04\n",
            "step 1644 | loss: 3.280626 | lr: 5.253185e-04\n",
            "step 1645 | loss: 3.690183 | lr: 5.252125e-04\n",
            "step 1646 | loss: 3.611431 | lr: 5.251064e-04\n",
            "step 1647 | loss: 3.677619 | lr: 5.250002e-04\n",
            "step 1648 | loss: 3.190873 | lr: 5.248940e-04\n",
            "step 1649 | loss: 3.272377 | lr: 5.247877e-04\n",
            "step 1650 | loss: 3.763249 | lr: 5.246813e-04\n",
            "step 1651 | loss: 3.558884 | lr: 5.245749e-04\n",
            "step 1652 | loss: 3.538174 | lr: 5.244684e-04\n",
            "step 1653 | loss: 3.482147 | lr: 5.243619e-04\n",
            "step 1654 | loss: 3.447924 | lr: 5.242553e-04\n",
            "step 1655 | loss: 3.560592 | lr: 5.241486e-04\n",
            "step 1656 | loss: 3.303226 | lr: 5.240418e-04\n",
            "step 1657 | loss: 3.508812 | lr: 5.239350e-04\n",
            "step 1658 | loss: 3.289869 | lr: 5.238282e-04\n",
            "step 1659 | loss: 3.300840 | lr: 5.237213e-04\n",
            "step 1660 | loss: 3.179294 | lr: 5.236143e-04\n",
            "step 1661 | loss: 2.812227 | lr: 5.235072e-04\n",
            "step 1662 | loss: 3.174898 | lr: 5.234001e-04\n",
            "step 1663 | loss: 3.000897 | lr: 5.232929e-04\n",
            "step 1664 | loss: 3.060080 | lr: 5.231857e-04\n",
            "step 1665 | loss: 3.070547 | lr: 5.230784e-04\n",
            "step 1666 | loss: 3.285635 | lr: 5.229710e-04\n",
            "step 1667 | loss: 3.084541 | lr: 5.228636e-04\n",
            "step 1668 | loss: 3.281193 | lr: 5.227561e-04\n",
            "step 1669 | loss: 3.239374 | lr: 5.226485e-04\n",
            "step 1670 | loss: 3.414540 | lr: 5.225409e-04\n",
            "step 1671 | loss: 3.406042 | lr: 5.224332e-04\n",
            "step 1672 | loss: 3.217678 | lr: 5.223254e-04\n",
            "step 1673 | loss: 3.652065 | lr: 5.222176e-04\n",
            "step 1674 | loss: 3.318275 | lr: 5.221098e-04\n",
            "step 1675 | loss: 3.113710 | lr: 5.220018e-04\n",
            "step 1676 | loss: 3.311591 | lr: 5.218938e-04\n",
            "step 1677 | loss: 3.235798 | lr: 5.217858e-04\n",
            "step 1678 | loss: 3.393732 | lr: 5.216776e-04\n",
            "step 1679 | loss: 3.116242 | lr: 5.215694e-04\n",
            "step 1680 | loss: 3.284856 | lr: 5.214612e-04\n",
            "step 1681 | loss: 3.337227 | lr: 5.213529e-04\n",
            "step 1682 | loss: 3.113378 | lr: 5.212445e-04\n",
            "step 1683 | loss: 3.003078 | lr: 5.211361e-04\n",
            "step 1684 | loss: 3.077699 | lr: 5.210275e-04\n",
            "step 1685 | loss: 3.019376 | lr: 5.209190e-04\n",
            "step 1686 | loss: 3.310282 | lr: 5.208104e-04\n",
            "step 1687 | loss: 3.370440 | lr: 5.207017e-04\n",
            "step 1688 | loss: 3.104099 | lr: 5.205929e-04\n",
            "step 1689 | loss: 3.033390 | lr: 5.204841e-04\n",
            "step 1690 | loss: 3.343387 | lr: 5.203752e-04\n",
            "step 1691 | loss: 3.008223 | lr: 5.202663e-04\n",
            "step 1692 | loss: 2.930366 | lr: 5.201573e-04\n",
            "step 1693 | loss: 2.849140 | lr: 5.200482e-04\n",
            "step 1694 | loss: 3.362369 | lr: 5.199391e-04\n",
            "step 1695 | loss: 3.303791 | lr: 5.198299e-04\n",
            "step 1696 | loss: 3.260175 | lr: 5.197206e-04\n",
            "step 1697 | loss: 3.564073 | lr: 5.196113e-04\n",
            "step 1698 | loss: 3.485138 | lr: 5.195020e-04\n",
            "step 1699 | loss: 3.425354 | lr: 5.193925e-04\n",
            "step 1700 | loss: 3.246755 | lr: 5.192830e-04\n",
            "step 1701 | loss: 3.442489 | lr: 5.191735e-04\n",
            "step 1702 | loss: 3.352844 | lr: 5.190638e-04\n",
            "step 1703 | loss: 3.167898 | lr: 5.189541e-04\n",
            "step 1704 | loss: 3.155363 | lr: 5.188444e-04\n",
            "step 1705 | loss: 3.419992 | lr: 5.187346e-04\n",
            "step 1706 | loss: 3.244239 | lr: 5.186247e-04\n",
            "step 1707 | loss: 3.261905 | lr: 5.185148e-04\n",
            "step 1708 | loss: 3.281888 | lr: 5.184048e-04\n",
            "step 1709 | loss: 3.213380 | lr: 5.182947e-04\n",
            "step 1710 | loss: 3.136813 | lr: 5.181846e-04\n",
            "step 1711 | loss: 3.121734 | lr: 5.180744e-04\n",
            "step 1712 | loss: 2.814423 | lr: 5.179642e-04\n",
            "step 1713 | loss: 3.410558 | lr: 5.178539e-04\n",
            "step 1714 | loss: 3.263248 | lr: 5.177435e-04\n",
            "step 1715 | loss: 3.461888 | lr: 5.176331e-04\n",
            "step 1716 | loss: 3.480612 | lr: 5.175226e-04\n",
            "step 1717 | loss: 3.326652 | lr: 5.174120e-04\n",
            "step 1718 | loss: 3.446863 | lr: 5.173014e-04\n",
            "step 1719 | loss: 3.173311 | lr: 5.171908e-04\n",
            "step 1720 | loss: 3.125124 | lr: 5.170800e-04\n",
            "step 1721 | loss: 3.568023 | lr: 5.169692e-04\n",
            "step 1722 | loss: 3.001704 | lr: 5.168584e-04\n",
            "step 1723 | loss: 3.249331 | lr: 5.167475e-04\n",
            "step 1724 | loss: 2.985220 | lr: 5.166365e-04\n",
            "step 1725 | loss: 3.468546 | lr: 5.165254e-04\n",
            "step 1726 | loss: 3.180520 | lr: 5.164143e-04\n",
            "step 1727 | loss: 3.141933 | lr: 5.163032e-04\n",
            "step 1728 | loss: 3.047009 | lr: 5.161919e-04\n",
            "step 1729 | loss: 3.048977 | lr: 5.160807e-04\n",
            "step 1730 | loss: 3.085982 | lr: 5.159693e-04\n",
            "step 1731 | loss: 3.177484 | lr: 5.158579e-04\n",
            "step 1732 | loss: 3.430006 | lr: 5.157464e-04\n",
            "step 1733 | loss: 3.196036 | lr: 5.156349e-04\n",
            "step 1734 | loss: 3.283753 | lr: 5.155233e-04\n",
            "step 1735 | loss: 3.195471 | lr: 5.154117e-04\n",
            "step 1736 | loss: 3.001183 | lr: 5.152999e-04\n",
            "step 1737 | loss: 3.390985 | lr: 5.151882e-04\n",
            "step 1738 | loss: 3.523810 | lr: 5.150763e-04\n",
            "step 1739 | loss: 3.400196 | lr: 5.149644e-04\n",
            "step 1740 | loss: 3.109921 | lr: 5.148525e-04\n",
            "step 1741 | loss: 3.473897 | lr: 5.147405e-04\n",
            "step 1742 | loss: 3.222845 | lr: 5.146284e-04\n",
            "step 1743 | loss: 3.152936 | lr: 5.145163e-04\n",
            "step 1744 | loss: 2.789693 | lr: 5.144041e-04\n",
            "step 1745 | loss: 3.232817 | lr: 5.142918e-04\n",
            "step 1746 | loss: 3.170063 | lr: 5.141795e-04\n",
            "step 1747 | loss: 3.142775 | lr: 5.140671e-04\n",
            "step 1748 | loss: 2.999782 | lr: 5.139547e-04\n",
            "step 1749 | loss: 2.984172 | lr: 5.138422e-04\n",
            "step 1750 | loss: 2.925538 | lr: 5.137296e-04\n",
            "step 1751 | loss: 3.215615 | lr: 5.136170e-04\n",
            "step 1752 | loss: 2.795889 | lr: 5.135043e-04\n",
            "step 1753 | loss: 3.183105 | lr: 5.133915e-04\n",
            "step 1754 | loss: 3.589543 | lr: 5.132787e-04\n",
            "step 1755 | loss: 3.497617 | lr: 5.131659e-04\n",
            "step 1756 | loss: 3.405272 | lr: 5.130530e-04\n",
            "step 1757 | loss: 3.378223 | lr: 5.129400e-04\n",
            "step 1758 | loss: 3.413683 | lr: 5.128269e-04\n",
            "step 1759 | loss: 3.183469 | lr: 5.127138e-04\n",
            "step 1760 | loss: 3.491052 | lr: 5.126007e-04\n",
            "step 1761 | loss: 3.407273 | lr: 5.124874e-04\n",
            "step 1762 | loss: 3.783845 | lr: 5.123741e-04\n",
            "step 1763 | loss: 3.921716 | lr: 5.122608e-04\n",
            "step 1764 | loss: 3.498439 | lr: 5.121474e-04\n",
            "step 1765 | loss: 3.666914 | lr: 5.120339e-04\n",
            "step 1766 | loss: 3.359340 | lr: 5.119204e-04\n",
            "step 1767 | loss: 3.453603 | lr: 5.118068e-04\n",
            "step 1768 | loss: 3.325952 | lr: 5.116932e-04\n",
            "step 1769 | loss: 3.320547 | lr: 5.115795e-04\n",
            "step 1770 | loss: 3.513437 | lr: 5.114657e-04\n",
            "step 1771 | loss: 3.482212 | lr: 5.113519e-04\n",
            "step 1772 | loss: 3.360825 | lr: 5.112380e-04\n",
            "step 1773 | loss: 3.304242 | lr: 5.111241e-04\n",
            "step 1774 | loss: 3.167855 | lr: 5.110101e-04\n",
            "step 1775 | loss: 3.423725 | lr: 5.108960e-04\n",
            "step 1776 | loss: 3.239932 | lr: 5.107819e-04\n",
            "step 1777 | loss: 3.003469 | lr: 5.106677e-04\n",
            "step 1778 | loss: 2.929744 | lr: 5.105535e-04\n",
            "step 1779 | loss: 3.104847 | lr: 5.104392e-04\n",
            "step 1780 | loss: 3.072006 | lr: 5.103248e-04\n",
            "step 1781 | loss: 3.044404 | lr: 5.102104e-04\n",
            "step 1782 | loss: 3.336696 | lr: 5.100959e-04\n",
            "step 1783 | loss: 2.875155 | lr: 5.099814e-04\n",
            "step 1784 | loss: 3.198166 | lr: 5.098668e-04\n",
            "step 1785 | loss: 2.819209 | lr: 5.097521e-04\n",
            "step 1786 | loss: 3.077645 | lr: 5.096374e-04\n",
            "step 1787 | loss: 2.922056 | lr: 5.095227e-04\n",
            "step 1788 | loss: 2.923068 | lr: 5.094078e-04\n",
            "step 1789 | loss: 2.987467 | lr: 5.092929e-04\n",
            "step 1790 | loss: 2.872582 | lr: 5.091780e-04\n",
            "step 1791 | loss: 2.848686 | lr: 5.090630e-04\n",
            "step 1792 | loss: 3.555697 | lr: 5.089479e-04\n",
            "step 1793 | loss: 3.397350 | lr: 5.088328e-04\n",
            "step 1794 | loss: 3.319768 | lr: 5.087176e-04\n",
            "step 1795 | loss: 3.171717 | lr: 5.086024e-04\n",
            "step 1796 | loss: 2.934752 | lr: 5.084870e-04\n",
            "step 1797 | loss: 2.922410 | lr: 5.083717e-04\n",
            "step 1798 | loss: 2.804505 | lr: 5.082563e-04\n",
            "step 1799 | loss: 3.030171 | lr: 5.081408e-04\n",
            "step 1800 | loss: 3.061220 | lr: 5.080252e-04\n",
            "step 1801 | loss: 2.989385 | lr: 5.079097e-04\n",
            "step 1802 | loss: 2.969740 | lr: 5.077940e-04\n",
            "step 1803 | loss: 3.049351 | lr: 5.076783e-04\n",
            "step 1804 | loss: 2.739526 | lr: 5.075625e-04\n",
            "step 1805 | loss: 2.788991 | lr: 5.074467e-04\n",
            "step 1806 | loss: 2.806182 | lr: 5.073308e-04\n",
            "step 1807 | loss: 2.639911 | lr: 5.072148e-04\n",
            "step 1808 | loss: 2.368838 | lr: 5.070988e-04\n",
            "step 1809 | loss: 3.103616 | lr: 5.069828e-04\n",
            "step 1810 | loss: 3.459484 | lr: 5.068666e-04\n",
            "step 1811 | loss: 3.349689 | lr: 5.067505e-04\n",
            "step 1812 | loss: 3.467566 | lr: 5.066342e-04\n",
            "step 1813 | loss: 2.993106 | lr: 5.065179e-04\n",
            "step 1814 | loss: 3.099284 | lr: 5.064016e-04\n",
            "step 1815 | loss: 3.594481 | lr: 5.062852e-04\n",
            "step 1816 | loss: 3.428198 | lr: 5.061687e-04\n",
            "step 1817 | loss: 3.319145 | lr: 5.060522e-04\n",
            "step 1818 | loss: 3.332166 | lr: 5.059356e-04\n",
            "step 1819 | loss: 3.283096 | lr: 5.058189e-04\n",
            "step 1820 | loss: 3.404002 | lr: 5.057022e-04\n",
            "step 1821 | loss: 3.119243 | lr: 5.055855e-04\n",
            "step 1822 | loss: 3.356106 | lr: 5.054686e-04\n",
            "step 1823 | loss: 3.246861 | lr: 5.053518e-04\n",
            "step 1824 | loss: 3.133117 | lr: 5.052348e-04\n",
            "step 1825 | loss: 3.023285 | lr: 5.051178e-04\n",
            "step 1826 | loss: 2.658950 | lr: 5.050008e-04\n",
            "step 1827 | loss: 2.997063 | lr: 5.048837e-04\n",
            "step 1828 | loss: 2.896056 | lr: 5.047665e-04\n",
            "step 1829 | loss: 2.927808 | lr: 5.046493e-04\n",
            "step 1830 | loss: 2.932963 | lr: 5.045320e-04\n",
            "step 1831 | loss: 3.057751 | lr: 5.044147e-04\n",
            "step 1832 | loss: 2.864035 | lr: 5.042973e-04\n",
            "step 1833 | loss: 3.068382 | lr: 5.041798e-04\n",
            "step 1834 | loss: 3.037429 | lr: 5.040623e-04\n",
            "step 1835 | loss: 3.215885 | lr: 5.039447e-04\n",
            "step 1836 | loss: 3.179641 | lr: 5.038271e-04\n",
            "step 1837 | loss: 3.000132 | lr: 5.037094e-04\n",
            "step 1838 | loss: 3.427053 | lr: 5.035917e-04\n",
            "step 1839 | loss: 3.074012 | lr: 5.034739e-04\n",
            "step 1840 | loss: 2.896483 | lr: 5.033560e-04\n",
            "step 1841 | loss: 3.059632 | lr: 5.032381e-04\n",
            "step 1842 | loss: 3.045525 | lr: 5.031201e-04\n",
            "step 1843 | loss: 3.128036 | lr: 5.030021e-04\n",
            "step 1844 | loss: 2.930433 | lr: 5.028840e-04\n",
            "step 1845 | loss: 3.081076 | lr: 5.027659e-04\n",
            "step 1846 | loss: 3.116372 | lr: 5.026477e-04\n",
            "step 1847 | loss: 2.920912 | lr: 5.025294e-04\n",
            "step 1848 | loss: 2.827261 | lr: 5.024111e-04\n",
            "step 1849 | loss: 2.861328 | lr: 5.022928e-04\n",
            "step 1850 | loss: 2.833816 | lr: 5.021743e-04\n",
            "step 1851 | loss: 3.132802 | lr: 5.020558e-04\n",
            "step 1852 | loss: 3.187398 | lr: 5.019373e-04\n",
            "step 1853 | loss: 2.901297 | lr: 5.018187e-04\n",
            "step 1854 | loss: 2.945410 | lr: 5.017001e-04\n",
            "step 1855 | loss: 3.129868 | lr: 5.015813e-04\n",
            "step 1856 | loss: 2.829863 | lr: 5.014626e-04\n",
            "step 1857 | loss: 2.767346 | lr: 5.013438e-04\n",
            "step 1858 | loss: 2.675315 | lr: 5.012249e-04\n",
            "step 1859 | loss: 3.178480 | lr: 5.011059e-04\n",
            "step 1860 | loss: 3.096570 | lr: 5.009869e-04\n",
            "step 1861 | loss: 3.008393 | lr: 5.008679e-04\n",
            "step 1862 | loss: 3.331989 | lr: 5.007488e-04\n",
            "step 1863 | loss: 3.295880 | lr: 5.006296e-04\n",
            "step 1864 | loss: 3.219234 | lr: 5.005104e-04\n",
            "step 1865 | loss: 3.071056 | lr: 5.003911e-04\n",
            "step 1866 | loss: 3.255699 | lr: 5.002718e-04\n",
            "step 1867 | loss: 3.128858 | lr: 5.001524e-04\n",
            "step 1868 | loss: 2.944975 | lr: 5.000330e-04\n",
            "step 1869 | loss: 2.922979 | lr: 4.999135e-04\n",
            "step 1870 | loss: 3.212966 | lr: 4.997939e-04\n",
            "step 1871 | loss: 3.045362 | lr: 4.996743e-04\n",
            "step 1872 | loss: 3.004323 | lr: 4.995546e-04\n",
            "step 1873 | loss: 3.012494 | lr: 4.994349e-04\n",
            "step 1874 | loss: 3.031826 | lr: 4.993151e-04\n",
            "step 1875 | loss: 2.940827 | lr: 4.991953e-04\n",
            "step 1876 | loss: 2.955688 | lr: 4.990754e-04\n",
            "step 1877 | loss: 2.620948 | lr: 4.989554e-04\n",
            "step 1878 | loss: 3.171920 | lr: 4.988354e-04\n",
            "step 1879 | loss: 3.005505 | lr: 4.987154e-04\n",
            "step 1880 | loss: 3.239208 | lr: 4.985953e-04\n",
            "step 1881 | loss: 3.310707 | lr: 4.984751e-04\n",
            "step 1882 | loss: 3.158974 | lr: 4.983549e-04\n",
            "step 1883 | loss: 3.200094 | lr: 4.982346e-04\n",
            "step 1884 | loss: 2.983229 | lr: 4.981142e-04\n",
            "step 1885 | loss: 2.904941 | lr: 4.979938e-04\n",
            "step 1886 | loss: 3.285435 | lr: 4.978734e-04\n",
            "step 1887 | loss: 2.836190 | lr: 4.977529e-04\n",
            "step 1888 | loss: 3.013841 | lr: 4.976323e-04\n",
            "step 1889 | loss: 2.772615 | lr: 4.975117e-04\n",
            "step 1890 | loss: 3.251899 | lr: 4.973910e-04\n",
            "step 1891 | loss: 2.965926 | lr: 4.972703e-04\n",
            "step 1892 | loss: 2.974396 | lr: 4.971495e-04\n",
            "step 1893 | loss: 2.889130 | lr: 4.970287e-04\n",
            "step 1894 | loss: 2.855500 | lr: 4.969078e-04\n",
            "step 1895 | loss: 2.863964 | lr: 4.967868e-04\n",
            "step 1896 | loss: 3.022322 | lr: 4.966658e-04\n",
            "step 1897 | loss: 3.248383 | lr: 4.965448e-04\n",
            "step 1898 | loss: 3.016315 | lr: 4.964237e-04\n",
            "step 1899 | loss: 3.108002 | lr: 4.963025e-04\n",
            "step 1900 | loss: 3.033776 | lr: 4.961813e-04\n",
            "step 1901 | loss: 2.851706 | lr: 4.960600e-04\n",
            "step 1902 | loss: 3.182544 | lr: 4.959387e-04\n",
            "step 1903 | loss: 3.339131 | lr: 4.958173e-04\n",
            "step 1904 | loss: 3.233846 | lr: 4.956958e-04\n",
            "step 1905 | loss: 2.972126 | lr: 4.955743e-04\n",
            "step 1906 | loss: 3.283013 | lr: 4.954528e-04\n",
            "step 1907 | loss: 3.081204 | lr: 4.953312e-04\n",
            "step 1908 | loss: 3.003487 | lr: 4.952095e-04\n",
            "step 1909 | loss: 2.627788 | lr: 4.950878e-04\n",
            "step 1910 | loss: 3.108829 | lr: 4.949660e-04\n",
            "step 1911 | loss: 3.019679 | lr: 4.948442e-04\n",
            "step 1912 | loss: 3.005960 | lr: 4.947223e-04\n",
            "step 1913 | loss: 2.827739 | lr: 4.946004e-04\n",
            "step 1914 | loss: 2.808017 | lr: 4.944784e-04\n",
            "step 1915 | loss: 2.745450 | lr: 4.943563e-04\n",
            "step 1916 | loss: 3.007943 | lr: 4.942342e-04\n",
            "step 1917 | loss: 2.590506 | lr: 4.941121e-04\n",
            "step 1918 | loss: 3.025261 | lr: 4.939898e-04\n",
            "step 1919 | loss: 3.426095 | lr: 4.938676e-04\n",
            "step 1920 | loss: 3.375240 | lr: 4.937453e-04\n",
            "step 1921 | loss: 3.267310 | lr: 4.936229e-04\n",
            "step 1922 | loss: 3.180439 | lr: 4.935005e-04\n",
            "step 1923 | loss: 3.254202 | lr: 4.933780e-04\n",
            "step 1924 | loss: 2.989124 | lr: 4.932554e-04\n",
            "step 1925 | loss: 3.296000 | lr: 4.931329e-04\n",
            "step 1926 | loss: 3.246738 | lr: 4.930102e-04\n",
            "step 1927 | loss: 3.582852 | lr: 4.928875e-04\n",
            "step 1928 | loss: 3.671261 | lr: 4.927648e-04\n",
            "step 1929 | loss: 3.243157 | lr: 4.926420e-04\n",
            "step 1930 | loss: 3.442790 | lr: 4.925191e-04\n",
            "step 1931 | loss: 3.143249 | lr: 4.923962e-04\n",
            "step 1932 | loss: 3.220317 | lr: 4.922732e-04\n",
            "step 1933 | loss: 3.100469 | lr: 4.921502e-04\n",
            "step 1934 | loss: 3.118557 | lr: 4.920271e-04\n",
            "step 1935 | loss: 3.267668 | lr: 4.919040e-04\n",
            "step 1936 | loss: 3.259027 | lr: 4.917808e-04\n",
            "step 1937 | loss: 3.117704 | lr: 4.916576e-04\n",
            "step 1938 | loss: 3.112944 | lr: 4.915343e-04\n",
            "step 1939 | loss: 2.956209 | lr: 4.914109e-04\n",
            "step 1940 | loss: 3.203642 | lr: 4.912875e-04\n",
            "step 1941 | loss: 2.997195 | lr: 4.911641e-04\n",
            "step 1942 | loss: 2.777818 | lr: 4.910406e-04\n",
            "step 1943 | loss: 2.736121 | lr: 4.909170e-04\n",
            "step 1944 | loss: 2.911139 | lr: 4.907934e-04\n",
            "step 1945 | loss: 2.895855 | lr: 4.906697e-04\n",
            "step 1946 | loss: 2.838823 | lr: 4.905460e-04\n",
            "step 1947 | loss: 3.160384 | lr: 4.904223e-04\n",
            "step 1948 | loss: 2.742592 | lr: 4.902984e-04\n",
            "step 1949 | loss: 3.039030 | lr: 4.901746e-04\n",
            "step 1950 | loss: 2.671591 | lr: 4.900506e-04\n",
            "step 1951 | loss: 2.893362 | lr: 4.899266e-04\n",
            "step 1952 | loss: 2.735301 | lr: 4.898026e-04\n",
            "step 1953 | loss: 2.762338 | lr: 4.896785e-04\n",
            "step 1954 | loss: 2.847529 | lr: 4.895544e-04\n",
            "step 1955 | loss: 2.719393 | lr: 4.894302e-04\n",
            "step 1956 | loss: 2.728711 | lr: 4.893059e-04\n",
            "step 1957 | loss: 3.376225 | lr: 4.891816e-04\n",
            "step 1958 | loss: 3.185769 | lr: 4.890573e-04\n",
            "step 1959 | loss: 3.133096 | lr: 4.889329e-04\n",
            "step 1960 | loss: 2.980387 | lr: 4.888084e-04\n",
            "step 1961 | loss: 2.767170 | lr: 4.886839e-04\n",
            "step 1962 | loss: 2.791785 | lr: 4.885593e-04\n",
            "step 1963 | loss: 2.634063 | lr: 4.884347e-04\n",
            "step 1964 | loss: 2.881096 | lr: 4.883100e-04\n",
            "step 1965 | loss: 2.910282 | lr: 4.881853e-04\n",
            "step 1966 | loss: 2.795063 | lr: 4.880605e-04\n",
            "step 1967 | loss: 2.786640 | lr: 4.879357e-04\n",
            "step 1968 | loss: 2.863409 | lr: 4.878108e-04\n",
            "step 1969 | loss: 2.595861 | lr: 4.876859e-04\n",
            "step 1970 | loss: 2.585433 | lr: 4.875609e-04\n",
            "step 1971 | loss: 2.612805 | lr: 4.874358e-04\n",
            "step 1972 | loss: 2.493146 | lr: 4.873107e-04\n",
            "step 1973 | loss: 2.158011 | lr: 4.871856e-04\n",
            "step 1974 | loss: 2.876185 | lr: 4.870604e-04\n",
            "step 1975 | loss: 3.229075 | lr: 4.869352e-04\n",
            "step 1976 | loss: 3.141646 | lr: 4.868099e-04\n",
            "step 1977 | loss: 3.258555 | lr: 4.866845e-04\n",
            "step 1978 | loss: 2.741034 | lr: 4.865591e-04\n",
            "step 1979 | loss: 2.826594 | lr: 4.864336e-04\n",
            "step 1980 | loss: 3.343385 | lr: 4.863081e-04\n",
            "step 1981 | loss: 3.217849 | lr: 4.861826e-04\n",
            "step 1982 | loss: 3.115028 | lr: 4.860570e-04\n",
            "step 1983 | loss: 3.166857 | lr: 4.859313e-04\n",
            "step 1984 | loss: 3.122308 | lr: 4.858056e-04\n",
            "step 1985 | loss: 3.230771 | lr: 4.856798e-04\n",
            "step 1986 | loss: 2.949918 | lr: 4.855540e-04\n",
            "step 1987 | loss: 3.198277 | lr: 4.854281e-04\n",
            "step 1988 | loss: 3.058242 | lr: 4.853022e-04\n",
            "step 1989 | loss: 2.906819 | lr: 4.851762e-04\n",
            "step 1990 | loss: 2.852649 | lr: 4.850502e-04\n",
            "step 1991 | loss: 2.530360 | lr: 4.849241e-04\n",
            "step 1992 | loss: 2.877413 | lr: 4.847980e-04\n",
            "step 1993 | loss: 2.693114 | lr: 4.846718e-04\n",
            "step 1994 | loss: 2.744769 | lr: 4.845455e-04\n",
            "step 1995 | loss: 2.745222 | lr: 4.844193e-04\n",
            "step 1996 | loss: 2.902007 | lr: 4.842929e-04\n",
            "step 1997 | loss: 2.688687 | lr: 4.841665e-04\n",
            "step 1998 | loss: 2.916942 | lr: 4.840401e-04\n",
            "step 1999 | loss: 2.854155 | lr: 4.839136e-04\n",
            "step 2000 | loss: 2.994763 | lr: 4.837870e-04\n",
            "step 2001 | loss: 2.927898 | lr: 4.836605e-04\n",
            "step 2002 | loss: 2.794097 | lr: 4.835338e-04\n",
            "step 2003 | loss: 3.236361 | lr: 4.834071e-04\n",
            "step 2004 | loss: 2.888723 | lr: 4.832804e-04\n",
            "step 2005 | loss: 2.706846 | lr: 4.831536e-04\n",
            "step 2006 | loss: 2.862653 | lr: 4.830267e-04\n",
            "step 2007 | loss: 2.822029 | lr: 4.828998e-04\n",
            "step 2008 | loss: 2.851329 | lr: 4.827729e-04\n",
            "step 2009 | loss: 2.693083 | lr: 4.826459e-04\n",
            "step 2010 | loss: 2.855624 | lr: 4.825188e-04\n",
            "step 2011 | loss: 2.933237 | lr: 4.823917e-04\n",
            "step 2012 | loss: 2.692066 | lr: 4.822645e-04\n",
            "step 2013 | loss: 2.600045 | lr: 4.821373e-04\n",
            "step 2014 | loss: 2.707010 | lr: 4.820101e-04\n",
            "step 2015 | loss: 2.664120 | lr: 4.818828e-04\n",
            "step 2016 | loss: 2.918376 | lr: 4.817554e-04\n",
            "step 2017 | loss: 2.972185 | lr: 4.816280e-04\n",
            "step 2018 | loss: 2.740920 | lr: 4.815005e-04\n",
            "step 2019 | loss: 2.755902 | lr: 4.813730e-04\n",
            "step 2020 | loss: 2.921435 | lr: 4.812455e-04\n",
            "step 2021 | loss: 2.613718 | lr: 4.811178e-04\n",
            "step 2022 | loss: 2.549361 | lr: 4.809902e-04\n",
            "step 2023 | loss: 2.469932 | lr: 4.808625e-04\n",
            "step 2024 | loss: 2.979378 | lr: 4.807347e-04\n",
            "step 2025 | loss: 2.911153 | lr: 4.806069e-04\n",
            "step 2026 | loss: 2.873487 | lr: 4.804790e-04\n",
            "step 2027 | loss: 3.204420 | lr: 4.803511e-04\n",
            "step 2028 | loss: 3.137358 | lr: 4.802231e-04\n",
            "step 2029 | loss: 3.023193 | lr: 4.800951e-04\n",
            "step 2030 | loss: 2.936125 | lr: 4.799671e-04\n",
            "step 2031 | loss: 3.109231 | lr: 4.798390e-04\n",
            "step 2032 | loss: 2.934750 | lr: 4.797108e-04\n",
            "step 2033 | loss: 2.823215 | lr: 4.795826e-04\n",
            "step 2034 | loss: 2.811247 | lr: 4.794543e-04\n",
            "step 2035 | loss: 3.081725 | lr: 4.793260e-04\n",
            "step 2036 | loss: 2.909381 | lr: 4.791976e-04\n",
            "step 2037 | loss: 2.892716 | lr: 4.790692e-04\n",
            "step 2038 | loss: 2.913539 | lr: 4.789407e-04\n",
            "step 2039 | loss: 2.866778 | lr: 4.788122e-04\n",
            "step 2040 | loss: 2.766923 | lr: 4.786837e-04\n",
            "step 2041 | loss: 2.765135 | lr: 4.785551e-04\n",
            "step 2042 | loss: 2.431534 | lr: 4.784264e-04\n",
            "step 2043 | loss: 3.006595 | lr: 4.782977e-04\n",
            "step 2044 | loss: 2.861395 | lr: 4.781689e-04\n",
            "step 2045 | loss: 3.100918 | lr: 4.780401e-04\n",
            "step 2046 | loss: 3.116266 | lr: 4.779112e-04\n",
            "step 2047 | loss: 2.986766 | lr: 4.777823e-04\n",
            "step 2048 | loss: 3.006169 | lr: 4.776534e-04\n",
            "step 2049 | loss: 2.846989 | lr: 4.775244e-04\n",
            "step 2050 | loss: 2.733265 | lr: 4.773953e-04\n",
            "step 2051 | loss: 3.145572 | lr: 4.772662e-04\n",
            "step 2052 | loss: 2.647981 | lr: 4.771370e-04\n",
            "step 2053 | loss: 2.839951 | lr: 4.770078e-04\n",
            "step 2054 | loss: 2.613088 | lr: 4.768786e-04\n",
            "step 2055 | loss: 3.026426 | lr: 4.767493e-04\n",
            "step 2056 | loss: 2.792187 | lr: 4.766199e-04\n",
            "step 2057 | loss: 2.782456 | lr: 4.764905e-04\n",
            "step 2058 | loss: 2.651384 | lr: 4.763610e-04\n",
            "step 2059 | loss: 2.623909 | lr: 4.762315e-04\n",
            "step 2060 | loss: 2.617408 | lr: 4.761020e-04\n",
            "step 2061 | loss: 2.796350 | lr: 4.759724e-04\n",
            "step 2062 | loss: 3.026507 | lr: 4.758427e-04\n",
            "step 2063 | loss: 2.827459 | lr: 4.757130e-04\n",
            "step 2064 | loss: 2.914207 | lr: 4.755833e-04\n",
            "step 2065 | loss: 2.858976 | lr: 4.754535e-04\n",
            "step 2066 | loss: 2.705761 | lr: 4.753237e-04\n",
            "step 2067 | loss: 3.008387 | lr: 4.751938e-04\n",
            "step 2068 | loss: 3.133845 | lr: 4.750638e-04\n",
            "step 2069 | loss: 2.983042 | lr: 4.749338e-04\n",
            "step 2070 | loss: 2.742638 | lr: 4.748038e-04\n",
            "step 2071 | loss: 3.047688 | lr: 4.746737e-04\n",
            "step 2072 | loss: 2.906879 | lr: 4.745436e-04\n",
            "step 2073 | loss: 2.811720 | lr: 4.744134e-04\n",
            "step 2074 | loss: 2.454913 | lr: 4.742832e-04\n",
            "step 2075 | loss: 2.934414 | lr: 4.741529e-04\n",
            "step 2076 | loss: 2.842922 | lr: 4.740225e-04\n",
            "step 2077 | loss: 2.817711 | lr: 4.738922e-04\n",
            "step 2078 | loss: 2.618879 | lr: 4.737617e-04\n",
            "step 2079 | loss: 2.606815 | lr: 4.736313e-04\n",
            "step 2080 | loss: 2.575238 | lr: 4.735007e-04\n",
            "step 2081 | loss: 2.835043 | lr: 4.733702e-04\n",
            "step 2082 | loss: 2.411415 | lr: 4.732395e-04\n",
            "step 2083 | loss: 2.788946 | lr: 4.731089e-04\n",
            "step 2084 | loss: 3.174910 | lr: 4.729782e-04\n",
            "step 2085 | loss: 3.175120 | lr: 4.728474e-04\n",
            "step 2086 | loss: 3.063695 | lr: 4.727166e-04\n",
            "step 2087 | loss: 2.999733 | lr: 4.725857e-04\n",
            "step 2088 | loss: 3.099133 | lr: 4.724548e-04\n",
            "step 2089 | loss: 2.812225 | lr: 4.723239e-04\n",
            "step 2090 | loss: 3.083284 | lr: 4.721929e-04\n",
            "step 2091 | loss: 3.085130 | lr: 4.720618e-04\n",
            "step 2092 | loss: 3.410270 | lr: 4.719307e-04\n",
            "step 2093 | loss: 3.453522 | lr: 4.717996e-04\n",
            "step 2094 | loss: 3.033147 | lr: 4.716684e-04\n",
            "step 2095 | loss: 3.244806 | lr: 4.715371e-04\n",
            "step 2096 | loss: 3.004368 | lr: 4.714059e-04\n",
            "step 2097 | loss: 3.022462 | lr: 4.712745e-04\n",
            "step 2098 | loss: 2.901059 | lr: 4.711431e-04\n",
            "step 2099 | loss: 3.000739 | lr: 4.710117e-04\n",
            "step 2100 | loss: 3.074969 | lr: 4.708802e-04\n",
            "step 2101 | loss: 3.065726 | lr: 4.707487e-04\n",
            "step 2102 | loss: 2.943328 | lr: 4.706171e-04\n",
            "step 2103 | loss: 2.934388 | lr: 4.704855e-04\n",
            "step 2104 | loss: 2.735421 | lr: 4.703538e-04\n",
            "step 2105 | loss: 3.022375 | lr: 4.702221e-04\n",
            "step 2106 | loss: 2.810840 | lr: 4.700904e-04\n",
            "step 2107 | loss: 2.586914 | lr: 4.699586e-04\n",
            "step 2108 | loss: 2.549172 | lr: 4.698267e-04\n",
            "step 2109 | loss: 2.700618 | lr: 4.696948e-04\n",
            "step 2110 | loss: 2.691306 | lr: 4.695628e-04\n",
            "step 2111 | loss: 2.661960 | lr: 4.694308e-04\n",
            "step 2112 | loss: 2.986691 | lr: 4.692988e-04\n",
            "step 2113 | loss: 2.537051 | lr: 4.691667e-04\n",
            "step 2114 | loss: 2.829018 | lr: 4.690346e-04\n",
            "step 2115 | loss: 2.482956 | lr: 4.689024e-04\n",
            "step 2116 | loss: 2.691018 | lr: 4.687702e-04\n",
            "step 2117 | loss: 2.590497 | lr: 4.686379e-04\n",
            "step 2118 | loss: 2.598086 | lr: 4.685056e-04\n",
            "step 2119 | loss: 2.665227 | lr: 4.683732e-04\n",
            "step 2120 | loss: 2.510987 | lr: 4.682408e-04\n",
            "step 2121 | loss: 2.538799 | lr: 4.681083e-04\n",
            "step 2122 | loss: 3.182751 | lr: 4.679758e-04\n",
            "step 2123 | loss: 2.999941 | lr: 4.678432e-04\n",
            "step 2124 | loss: 2.966044 | lr: 4.677106e-04\n",
            "step 2125 | loss: 2.838158 | lr: 4.675780e-04\n",
            "step 2126 | loss: 2.601988 | lr: 4.674453e-04\n",
            "step 2127 | loss: 2.594698 | lr: 4.673125e-04\n",
            "step 2128 | loss: 2.429476 | lr: 4.671798e-04\n",
            "step 2129 | loss: 2.632955 | lr: 4.670469e-04\n",
            "step 2130 | loss: 2.688206 | lr: 4.669140e-04\n",
            "step 2131 | loss: 2.597498 | lr: 4.667811e-04\n",
            "step 2132 | loss: 2.608669 | lr: 4.666481e-04\n",
            "step 2133 | loss: 2.672328 | lr: 4.665151e-04\n",
            "step 2134 | loss: 2.429573 | lr: 4.663821e-04\n",
            "step 2135 | loss: 2.484977 | lr: 4.662489e-04\n",
            "step 2136 | loss: 2.431262 | lr: 4.661158e-04\n",
            "step 2137 | loss: 2.346104 | lr: 4.659826e-04\n",
            "step 2138 | loss: 2.018783 | lr: 4.658493e-04\n",
            "step 2139 | loss: 2.690796 | lr: 4.657160e-04\n",
            "step 2140 | loss: 3.044677 | lr: 4.655827e-04\n",
            "step 2141 | loss: 2.918864 | lr: 4.654493e-04\n",
            "step 2142 | loss: 2.987170 | lr: 4.653159e-04\n",
            "step 2143 | loss: 2.538214 | lr: 4.651824e-04\n",
            "step 2144 | loss: 2.674923 | lr: 4.650489e-04\n",
            "step 2145 | loss: 3.155296 | lr: 4.649153e-04\n",
            "step 2146 | loss: 2.976969 | lr: 4.647817e-04\n",
            "step 2147 | loss: 2.875168 | lr: 4.646480e-04\n",
            "step 2148 | loss: 2.907665 | lr: 4.645143e-04\n",
            "step 2149 | loss: 2.900035 | lr: 4.643806e-04\n",
            "step 2150 | loss: 2.966475 | lr: 4.642468e-04\n",
            "step 2151 | loss: 2.818359 | lr: 4.641129e-04\n",
            "step 2152 | loss: 3.028468 | lr: 4.639791e-04\n",
            "step 2153 | loss: 2.901143 | lr: 4.638451e-04\n",
            "step 2154 | loss: 2.783279 | lr: 4.637111e-04\n",
            "step 2155 | loss: 2.672432 | lr: 4.635771e-04\n",
            "step 2156 | loss: 2.337143 | lr: 4.634431e-04\n",
            "step 2157 | loss: 2.648186 | lr: 4.633089e-04\n",
            "step 2158 | loss: 2.481555 | lr: 4.631748e-04\n",
            "step 2159 | loss: 2.553110 | lr: 4.630406e-04\n",
            "step 2160 | loss: 2.554489 | lr: 4.629063e-04\n",
            "step 2161 | loss: 2.674993 | lr: 4.627720e-04\n",
            "step 2162 | loss: 2.467414 | lr: 4.626377e-04\n",
            "step 2163 | loss: 2.708970 | lr: 4.625033e-04\n",
            "step 2164 | loss: 2.663677 | lr: 4.623689e-04\n",
            "step 2165 | loss: 2.803187 | lr: 4.622344e-04\n",
            "step 2166 | loss: 2.726119 | lr: 4.620999e-04\n",
            "step 2167 | loss: 2.587623 | lr: 4.619653e-04\n",
            "step 2168 | loss: 3.021666 | lr: 4.618307e-04\n",
            "step 2169 | loss: 2.688899 | lr: 4.616961e-04\n",
            "step 2170 | loss: 2.507651 | lr: 4.615614e-04\n",
            "step 2171 | loss: 2.674509 | lr: 4.614266e-04\n",
            "step 2172 | loss: 2.667321 | lr: 4.612919e-04\n",
            "step 2173 | loss: 2.717220 | lr: 4.611570e-04\n",
            "step 2174 | loss: 2.500298 | lr: 4.610222e-04\n",
            "step 2175 | loss: 2.686009 | lr: 4.608872e-04\n",
            "step 2176 | loss: 2.692896 | lr: 4.607523e-04\n",
            "step 2177 | loss: 2.474007 | lr: 4.606173e-04\n",
            "step 2178 | loss: 2.404582 | lr: 4.604822e-04\n",
            "step 2179 | loss: 2.497335 | lr: 4.603471e-04\n",
            "step 2180 | loss: 2.492906 | lr: 4.602120e-04\n",
            "step 2181 | loss: 2.701228 | lr: 4.600768e-04\n",
            "step 2182 | loss: 2.729743 | lr: 4.599416e-04\n",
            "step 2183 | loss: 2.516845 | lr: 4.598063e-04\n",
            "step 2184 | loss: 2.549780 | lr: 4.596710e-04\n",
            "step 2185 | loss: 2.708932 | lr: 4.595356e-04\n",
            "step 2186 | loss: 2.407478 | lr: 4.594002e-04\n",
            "step 2187 | loss: 2.389961 | lr: 4.592648e-04\n",
            "step 2188 | loss: 2.295001 | lr: 4.591293e-04\n",
            "step 2189 | loss: 2.763151 | lr: 4.589938e-04\n",
            "step 2190 | loss: 2.687722 | lr: 4.588582e-04\n",
            "step 2191 | loss: 2.643016 | lr: 4.587226e-04\n",
            "step 2192 | loss: 3.047953 | lr: 4.585869e-04\n",
            "step 2193 | loss: 2.992372 | lr: 4.584512e-04\n",
            "step 2194 | loss: 2.821968 | lr: 4.583154e-04\n",
            "step 2195 | loss: 2.747374 | lr: 4.581796e-04\n",
            "step 2196 | loss: 2.932388 | lr: 4.580438e-04\n",
            "step 2197 | loss: 2.816949 | lr: 4.579079e-04\n",
            "step 2198 | loss: 2.705807 | lr: 4.577720e-04\n",
            "step 2199 | loss: 2.666609 | lr: 4.576360e-04\n",
            "step 2200 | loss: 2.877278 | lr: 4.575000e-04\n",
            "step 2201 | loss: 2.756826 | lr: 4.573639e-04\n",
            "step 2202 | loss: 2.750131 | lr: 4.572278e-04\n",
            "step 2203 | loss: 2.777536 | lr: 4.570917e-04\n",
            "step 2204 | loss: 2.743412 | lr: 4.569555e-04\n",
            "step 2205 | loss: 2.613070 | lr: 4.568193e-04\n",
            "step 2206 | loss: 2.576202 | lr: 4.566830e-04\n",
            "step 2207 | loss: 2.250366 | lr: 4.565467e-04\n",
            "step 2208 | loss: 2.810491 | lr: 4.564103e-04\n",
            "step 2209 | loss: 2.564377 | lr: 4.562739e-04\n",
            "step 2210 | loss: 2.861432 | lr: 4.561375e-04\n",
            "step 2211 | loss: 2.940134 | lr: 4.560010e-04\n",
            "step 2212 | loss: 2.801058 | lr: 4.558645e-04\n",
            "step 2213 | loss: 2.809254 | lr: 4.557279e-04\n",
            "step 2214 | loss: 2.662140 | lr: 4.555913e-04\n",
            "step 2215 | loss: 2.617915 | lr: 4.554546e-04\n",
            "step 2216 | loss: 2.911807 | lr: 4.553179e-04\n",
            "step 2217 | loss: 2.422928 | lr: 4.551812e-04\n",
            "step 2218 | loss: 2.705594 | lr: 4.550444e-04\n",
            "step 2219 | loss: 2.438811 | lr: 4.549076e-04\n",
            "step 2220 | loss: 2.877193 | lr: 4.547707e-04\n",
            "step 2221 | loss: 2.602840 | lr: 4.546338e-04\n",
            "step 2222 | loss: 2.603185 | lr: 4.544968e-04\n",
            "step 2223 | loss: 2.485136 | lr: 4.543598e-04\n",
            "step 2224 | loss: 2.471613 | lr: 4.542228e-04\n",
            "step 2225 | loss: 2.444878 | lr: 4.540857e-04\n",
            "step 2226 | loss: 2.567671 | lr: 4.539486e-04\n",
            "step 2227 | loss: 2.809364 | lr: 4.538114e-04\n",
            "step 2228 | loss: 2.630833 | lr: 4.536742e-04\n",
            "step 2229 | loss: 2.709171 | lr: 4.535370e-04\n",
            "step 2230 | loss: 2.625697 | lr: 4.533997e-04\n",
            "step 2231 | loss: 2.522622 | lr: 4.532623e-04\n",
            "step 2232 | loss: 2.783156 | lr: 4.531249e-04\n",
            "step 2233 | loss: 2.937036 | lr: 4.529875e-04\n",
            "step 2234 | loss: 2.833087 | lr: 4.528501e-04\n",
            "step 2235 | loss: 2.582036 | lr: 4.527126e-04\n",
            "step 2236 | loss: 2.848986 | lr: 4.525750e-04\n",
            "step 2237 | loss: 2.646557 | lr: 4.524374e-04\n",
            "step 2238 | loss: 2.623456 | lr: 4.522998e-04\n",
            "step 2239 | loss: 2.280809 | lr: 4.521621e-04\n",
            "step 2240 | loss: 2.680391 | lr: 4.520244e-04\n",
            "step 2241 | loss: 2.596493 | lr: 4.518867e-04\n",
            "step 2242 | loss: 2.619946 | lr: 4.517489e-04\n",
            "step 2243 | loss: 2.449370 | lr: 4.516110e-04\n",
            "step 2244 | loss: 2.452040 | lr: 4.514731e-04\n",
            "step 2245 | loss: 2.357719 | lr: 4.513352e-04\n",
            "step 2246 | loss: 2.613039 | lr: 4.511973e-04\n",
            "step 2247 | loss: 2.199624 | lr: 4.510593e-04\n",
            "step 2248 | loss: 2.548348 | lr: 4.509212e-04\n",
            "step 2249 | loss: 2.929690 | lr: 4.507831e-04\n",
            "step 2250 | loss: 2.951070 | lr: 4.506450e-04\n",
            "step 2251 | loss: 2.823495 | lr: 4.505068e-04\n",
            "step 2252 | loss: 2.759963 | lr: 4.503686e-04\n",
            "step 2253 | loss: 2.854053 | lr: 4.502304e-04\n",
            "step 2254 | loss: 2.636223 | lr: 4.500921e-04\n",
            "step 2255 | loss: 2.894355 | lr: 4.499538e-04\n",
            "step 2256 | loss: 2.860888 | lr: 4.498154e-04\n",
            "step 2257 | loss: 3.154423 | lr: 4.496770e-04\n",
            "step 2258 | loss: 3.127407 | lr: 4.495385e-04\n",
            "step 2259 | loss: 2.785969 | lr: 4.494000e-04\n",
            "step 2260 | loss: 2.996533 | lr: 4.492615e-04\n",
            "step 2261 | loss: 2.760134 | lr: 4.491229e-04\n",
            "step 2262 | loss: 2.850913 | lr: 4.489843e-04\n",
            "step 2263 | loss: 2.719511 | lr: 4.488456e-04\n",
            "step 2264 | loss: 2.761049 | lr: 4.487069e-04\n",
            "step 2265 | loss: 2.940838 | lr: 4.485682e-04\n",
            "step 2266 | loss: 2.889514 | lr: 4.484294e-04\n",
            "step 2267 | loss: 2.757869 | lr: 4.482906e-04\n",
            "step 2268 | loss: 2.752778 | lr: 4.481517e-04\n",
            "step 2269 | loss: 2.585444 | lr: 4.480128e-04\n",
            "step 2270 | loss: 2.781126 | lr: 4.478739e-04\n",
            "step 2271 | loss: 2.614756 | lr: 4.477349e-04\n",
            "step 2272 | loss: 2.404756 | lr: 4.475959e-04\n",
            "step 2273 | loss: 2.385661 | lr: 4.474568e-04\n",
            "step 2274 | loss: 2.493216 | lr: 4.473177e-04\n",
            "step 2275 | loss: 2.489347 | lr: 4.471786e-04\n",
            "step 2276 | loss: 2.450820 | lr: 4.470394e-04\n",
            "step 2277 | loss: 2.757912 | lr: 4.469001e-04\n",
            "step 2278 | loss: 2.292439 | lr: 4.467609e-04\n",
            "step 2279 | loss: 2.569828 | lr: 4.466216e-04\n",
            "step 2280 | loss: 2.264115 | lr: 4.464822e-04\n",
            "step 2281 | loss: 2.510625 | lr: 4.463428e-04\n",
            "step 2282 | loss: 2.355115 | lr: 4.462034e-04\n",
            "step 2283 | loss: 2.366138 | lr: 4.460640e-04\n",
            "step 2284 | loss: 2.439374 | lr: 4.459245e-04\n",
            "step 2285 | loss: 2.262366 | lr: 4.457849e-04\n",
            "step 2286 | loss: 2.344609 | lr: 4.456453e-04\n",
            "step 2287 | loss: 2.986124 | lr: 4.455057e-04\n",
            "step 2288 | loss: 2.748210 | lr: 4.453660e-04\n",
            "step 2289 | loss: 2.775270 | lr: 4.452263e-04\n",
            "step 2290 | loss: 2.627478 | lr: 4.450866e-04\n",
            "step 2291 | loss: 2.402261 | lr: 4.449468e-04\n",
            "step 2292 | loss: 2.396814 | lr: 4.448070e-04\n",
            "step 2293 | loss: 2.247904 | lr: 4.446671e-04\n",
            "step 2294 | loss: 2.436992 | lr: 4.445272e-04\n",
            "step 2295 | loss: 2.488802 | lr: 4.443873e-04\n",
            "step 2296 | loss: 2.381675 | lr: 4.442473e-04\n",
            "step 2297 | loss: 2.412689 | lr: 4.441073e-04\n",
            "step 2298 | loss: 2.466268 | lr: 4.439672e-04\n",
            "step 2299 | loss: 2.253378 | lr: 4.438271e-04\n",
            "step 2300 | loss: 2.272756 | lr: 4.436870e-04\n",
            "step 2301 | loss: 2.223629 | lr: 4.435468e-04\n",
            "step 2302 | loss: 2.161902 | lr: 4.434066e-04\n",
            "step 2303 | loss: 1.843132 | lr: 4.432664e-04\n",
            "step 2304 | loss: 2.510781 | lr: 4.431261e-04\n",
            "step 2305 | loss: 2.871399 | lr: 4.429857e-04\n",
            "step 2306 | loss: 2.732796 | lr: 4.428454e-04\n",
            "step 2307 | loss: 2.837804 | lr: 4.427050e-04\n",
            "step 2308 | loss: 2.360994 | lr: 4.425645e-04\n",
            "step 2309 | loss: 2.487545 | lr: 4.424240e-04\n",
            "step 2310 | loss: 2.965082 | lr: 4.422835e-04\n",
            "step 2311 | loss: 2.749125 | lr: 4.421429e-04\n",
            "step 2312 | loss: 2.643883 | lr: 4.420023e-04\n",
            "step 2313 | loss: 2.677583 | lr: 4.418617e-04\n",
            "step 2314 | loss: 2.650879 | lr: 4.417210e-04\n",
            "step 2315 | loss: 2.705137 | lr: 4.415803e-04\n",
            "step 2316 | loss: 2.588347 | lr: 4.414396e-04\n",
            "step 2317 | loss: 2.834209 | lr: 4.412988e-04\n",
            "step 2318 | loss: 2.710275 | lr: 4.411579e-04\n",
            "step 2319 | loss: 2.626382 | lr: 4.410171e-04\n",
            "step 2320 | loss: 2.494704 | lr: 4.408761e-04\n",
            "step 2321 | loss: 2.152985 | lr: 4.407352e-04\n",
            "step 2322 | loss: 2.426010 | lr: 4.405942e-04\n",
            "step 2323 | loss: 2.281529 | lr: 4.404532e-04\n",
            "step 2324 | loss: 2.336944 | lr: 4.403121e-04\n",
            "step 2325 | loss: 2.319944 | lr: 4.401710e-04\n",
            "step 2326 | loss: 2.428204 | lr: 4.400299e-04\n",
            "step 2327 | loss: 2.248399 | lr: 4.398887e-04\n",
            "step 2328 | loss: 2.512119 | lr: 4.397475e-04\n",
            "step 2329 | loss: 2.408024 | lr: 4.396062e-04\n",
            "step 2330 | loss: 2.594154 | lr: 4.394650e-04\n",
            "step 2331 | loss: 2.482618 | lr: 4.393236e-04\n",
            "step 2332 | loss: 2.362901 | lr: 4.391823e-04\n",
            "step 2333 | loss: 2.717322 | lr: 4.390409e-04\n",
            "step 2334 | loss: 2.390176 | lr: 4.388994e-04\n",
            "step 2335 | loss: 2.267967 | lr: 4.387579e-04\n",
            "step 2336 | loss: 2.442121 | lr: 4.386164e-04\n",
            "step 2337 | loss: 2.396049 | lr: 4.384749e-04\n",
            "step 2338 | loss: 2.446529 | lr: 4.383333e-04\n",
            "step 2339 | loss: 2.268563 | lr: 4.381917e-04\n",
            "step 2340 | loss: 2.472579 | lr: 4.380500e-04\n",
            "step 2341 | loss: 2.484215 | lr: 4.379083e-04\n",
            "step 2342 | loss: 2.273616 | lr: 4.377665e-04\n",
            "step 2343 | loss: 2.188965 | lr: 4.376248e-04\n",
            "step 2344 | loss: 2.267927 | lr: 4.374830e-04\n",
            "step 2345 | loss: 2.291355 | lr: 4.373411e-04\n",
            "step 2346 | loss: 2.444258 | lr: 4.371992e-04\n",
            "step 2347 | loss: 2.467422 | lr: 4.370573e-04\n",
            "step 2348 | loss: 2.284380 | lr: 4.369153e-04\n",
            "step 2349 | loss: 2.324826 | lr: 4.367733e-04\n",
            "step 2350 | loss: 2.424397 | lr: 4.366313e-04\n",
            "step 2351 | loss: 2.175469 | lr: 4.364892e-04\n",
            "step 2352 | loss: 2.201259 | lr: 4.363471e-04\n",
            "step 2353 | loss: 2.119022 | lr: 4.362049e-04\n",
            "step 2354 | loss: 2.581685 | lr: 4.360628e-04\n",
            "step 2355 | loss: 2.490415 | lr: 4.359205e-04\n",
            "step 2356 | loss: 2.408802 | lr: 4.357783e-04\n",
            "step 2357 | loss: 2.792189 | lr: 4.356360e-04\n",
            "step 2358 | loss: 2.751281 | lr: 4.354937e-04\n",
            "step 2359 | loss: 2.610693 | lr: 4.353513e-04\n",
            "step 2360 | loss: 2.556736 | lr: 4.352089e-04\n",
            "step 2361 | loss: 2.744751 | lr: 4.350664e-04\n",
            "step 2362 | loss: 2.625585 | lr: 4.349240e-04\n",
            "step 2363 | loss: 2.539289 | lr: 4.347814e-04\n",
            "step 2364 | loss: 2.462485 | lr: 4.346389e-04\n",
            "step 2365 | loss: 2.627269 | lr: 4.344963e-04\n",
            "step 2366 | loss: 2.525393 | lr: 4.343537e-04\n",
            "step 2367 | loss: 2.514040 | lr: 4.342110e-04\n",
            "step 2368 | loss: 2.518448 | lr: 4.340683e-04\n",
            "step 2369 | loss: 2.501389 | lr: 4.339256e-04\n",
            "step 2370 | loss: 2.392467 | lr: 4.337828e-04\n",
            "step 2371 | loss: 2.394707 | lr: 4.336400e-04\n",
            "step 2372 | loss: 2.097227 | lr: 4.334972e-04\n",
            "step 2373 | loss: 2.586691 | lr: 4.333543e-04\n",
            "step 2374 | loss: 2.343608 | lr: 4.332114e-04\n",
            "step 2375 | loss: 2.611114 | lr: 4.330684e-04\n",
            "step 2376 | loss: 2.659670 | lr: 4.329255e-04\n",
            "step 2377 | loss: 2.530807 | lr: 4.327824e-04\n",
            "step 2378 | loss: 2.525151 | lr: 4.326394e-04\n",
            "step 2379 | loss: 2.355661 | lr: 4.324963e-04\n",
            "step 2380 | loss: 2.409485 | lr: 4.323532e-04\n",
            "step 2381 | loss: 2.641597 | lr: 4.322100e-04\n",
            "step 2382 | loss: 2.203055 | lr: 4.320668e-04\n",
            "step 2383 | loss: 2.455060 | lr: 4.319236e-04\n",
            "step 2384 | loss: 2.193187 | lr: 4.317803e-04\n",
            "step 2385 | loss: 2.652678 | lr: 4.316370e-04\n",
            "step 2386 | loss: 2.451473 | lr: 4.314937e-04\n",
            "step 2387 | loss: 2.380537 | lr: 4.313503e-04\n",
            "step 2388 | loss: 2.294288 | lr: 4.312069e-04\n",
            "step 2389 | loss: 2.284087 | lr: 4.310634e-04\n",
            "step 2390 | loss: 2.225234 | lr: 4.309199e-04\n",
            "step 2391 | loss: 2.375328 | lr: 4.307764e-04\n",
            "step 2392 | loss: 2.610028 | lr: 4.306329e-04\n",
            "step 2393 | loss: 2.424908 | lr: 4.304893e-04\n",
            "step 2394 | loss: 2.484806 | lr: 4.303457e-04\n",
            "step 2395 | loss: 2.430281 | lr: 4.302020e-04\n",
            "step 2396 | loss: 2.263797 | lr: 4.300583e-04\n",
            "step 2397 | loss: 2.460224 | lr: 4.299146e-04\n",
            "step 2398 | loss: 2.639125 | lr: 4.297708e-04\n",
            "step 2399 | loss: 2.567122 | lr: 4.296270e-04\n",
            "step 2400 | loss: 2.340240 | lr: 4.294832e-04\n",
            "step 2401 | loss: 2.616728 | lr: 4.293393e-04\n",
            "step 2402 | loss: 2.411635 | lr: 4.291954e-04\n",
            "step 2403 | loss: 2.420092 | lr: 4.290515e-04\n",
            "step 2404 | loss: 2.072045 | lr: 4.289075e-04\n",
            "step 2405 | loss: 2.529501 | lr: 4.287635e-04\n",
            "step 2406 | loss: 2.385107 | lr: 4.286195e-04\n",
            "step 2407 | loss: 2.408218 | lr: 4.284754e-04\n",
            "step 2408 | loss: 2.217854 | lr: 4.283313e-04\n",
            "step 2409 | loss: 2.215536 | lr: 4.281871e-04\n",
            "step 2410 | loss: 2.162174 | lr: 4.280430e-04\n",
            "step 2411 | loss: 2.400842 | lr: 4.278988e-04\n",
            "step 2412 | loss: 2.013462 | lr: 4.277545e-04\n",
            "step 2413 | loss: 2.346882 | lr: 4.276102e-04\n",
            "step 2414 | loss: 2.700825 | lr: 4.274659e-04\n",
            "step 2415 | loss: 2.665012 | lr: 4.273216e-04\n",
            "step 2416 | loss: 2.582535 | lr: 4.271772e-04\n",
            "step 2417 | loss: 2.587514 | lr: 4.270328e-04\n",
            "step 2418 | loss: 2.640194 | lr: 4.268883e-04\n",
            "step 2419 | loss: 2.383230 | lr: 4.267438e-04\n",
            "step 2420 | loss: 2.625378 | lr: 4.265993e-04\n",
            "step 2421 | loss: 2.604262 | lr: 4.264548e-04\n",
            "step 2422 | loss: 2.836613 | lr: 4.263102e-04\n",
            "step 2423 | loss: 2.879252 | lr: 4.261656e-04\n",
            "step 2424 | loss: 2.492405 | lr: 4.260209e-04\n",
            "step 2425 | loss: 2.670377 | lr: 4.258762e-04\n",
            "step 2426 | loss: 2.495277 | lr: 4.257315e-04\n",
            "step 2427 | loss: 2.592521 | lr: 4.255867e-04\n",
            "step 2428 | loss: 2.483509 | lr: 4.254419e-04\n",
            "step 2429 | loss: 2.598619 | lr: 4.252971e-04\n",
            "step 2430 | loss: 2.720290 | lr: 4.251523e-04\n",
            "step 2431 | loss: 2.646470 | lr: 4.250074e-04\n",
            "step 2432 | loss: 2.573758 | lr: 4.248625e-04\n",
            "step 2433 | loss: 2.555396 | lr: 4.247175e-04\n",
            "step 2434 | loss: 2.384899 | lr: 4.245725e-04\n",
            "step 2435 | loss: 2.608032 | lr: 4.244275e-04\n",
            "step 2436 | loss: 2.418584 | lr: 4.242824e-04\n",
            "step 2437 | loss: 2.195892 | lr: 4.241373e-04\n",
            "step 2438 | loss: 2.182819 | lr: 4.239922e-04\n",
            "step 2439 | loss: 2.329913 | lr: 4.238471e-04\n",
            "step 2440 | loss: 2.293248 | lr: 4.237019e-04\n",
            "step 2441 | loss: 2.221776 | lr: 4.235566e-04\n",
            "step 2442 | loss: 2.530564 | lr: 4.234114e-04\n",
            "step 2443 | loss: 2.117655 | lr: 4.232661e-04\n",
            "step 2444 | loss: 2.329716 | lr: 4.231208e-04\n",
            "step 2445 | loss: 2.034516 | lr: 4.229754e-04\n",
            "step 2446 | loss: 2.293828 | lr: 4.228300e-04\n",
            "step 2447 | loss: 2.133420 | lr: 4.226846e-04\n",
            "step 2448 | loss: 2.125214 | lr: 4.225392e-04\n",
            "step 2449 | loss: 2.231011 | lr: 4.223937e-04\n",
            "step 2450 | loss: 2.044256 | lr: 4.222482e-04\n",
            "step 2451 | loss: 2.077499 | lr: 4.221026e-04\n",
            "step 2452 | loss: 2.708242 | lr: 4.219570e-04\n",
            "step 2453 | loss: 2.500808 | lr: 4.218114e-04\n",
            "step 2454 | loss: 2.554359 | lr: 4.216658e-04\n",
            "step 2455 | loss: 2.414302 | lr: 4.215201e-04\n",
            "step 2456 | loss: 2.215360 | lr: 4.213744e-04\n",
            "step 2457 | loss: 2.217514 | lr: 4.212286e-04\n",
            "step 2458 | loss: 2.052957 | lr: 4.210829e-04\n",
            "step 2459 | loss: 2.221620 | lr: 4.209371e-04\n",
            "step 2460 | loss: 2.211082 | lr: 4.207912e-04\n",
            "step 2461 | loss: 2.156202 | lr: 4.206453e-04\n",
            "step 2462 | loss: 2.203159 | lr: 4.204994e-04\n",
            "step 2463 | loss: 2.209733 | lr: 4.203535e-04\n",
            "step 2464 | loss: 2.047996 | lr: 4.202075e-04\n",
            "step 2465 | loss: 2.070691 | lr: 4.200615e-04\n",
            "step 2466 | loss: 2.054997 | lr: 4.199155e-04\n",
            "step 2467 | loss: 2.028154 | lr: 4.197694e-04\n",
            "step 2468 | loss: 1.643758 | lr: 4.196233e-04\n",
            "step 2469 | loss: 2.248472 | lr: 4.194772e-04\n",
            "step 2470 | loss: 2.596016 | lr: 4.193310e-04\n",
            "step 2471 | loss: 2.447868 | lr: 4.191849e-04\n",
            "step 2472 | loss: 2.564545 | lr: 4.190386e-04\n",
            "step 2473 | loss: 2.151919 | lr: 4.188924e-04\n",
            "step 2474 | loss: 2.261955 | lr: 4.187461e-04\n",
            "step 2475 | loss: 2.748168 | lr: 4.185998e-04\n",
            "step 2476 | loss: 2.558367 | lr: 4.184534e-04\n",
            "step 2477 | loss: 2.423196 | lr: 4.183070e-04\n",
            "step 2478 | loss: 2.453844 | lr: 4.181606e-04\n",
            "step 2479 | loss: 2.412408 | lr: 4.180142e-04\n",
            "step 2480 | loss: 2.500007 | lr: 4.178677e-04\n",
            "step 2481 | loss: 2.349554 | lr: 4.177212e-04\n",
            "step 2482 | loss: 2.535163 | lr: 4.175747e-04\n",
            "step 2483 | loss: 2.481321 | lr: 4.174281e-04\n",
            "step 2484 | loss: 2.342117 | lr: 4.172815e-04\n",
            "step 2485 | loss: 2.250267 | lr: 4.171349e-04\n",
            "step 2486 | loss: 1.951805 | lr: 4.169882e-04\n",
            "step 2487 | loss: 2.200083 | lr: 4.168415e-04\n",
            "step 2488 | loss: 2.083319 | lr: 4.166948e-04\n",
            "step 2489 | loss: 2.134463 | lr: 4.165480e-04\n",
            "step 2490 | loss: 2.093106 | lr: 4.164012e-04\n",
            "step 2491 | loss: 2.220963 | lr: 4.162544e-04\n",
            "step 2492 | loss: 2.018104 | lr: 4.161076e-04\n",
            "step 2493 | loss: 2.274420 | lr: 4.159607e-04\n",
            "step 2494 | loss: 2.190891 | lr: 4.158138e-04\n",
            "step 2495 | loss: 2.376916 | lr: 4.156669e-04\n",
            "step 2496 | loss: 2.233250 | lr: 4.155199e-04\n",
            "step 2497 | loss: 2.126054 | lr: 4.153729e-04\n",
            "step 2498 | loss: 2.461690 | lr: 4.152259e-04\n",
            "step 2499 | loss: 2.150005 | lr: 4.150788e-04\n",
            "step 2500 | loss: 2.036071 | lr: 4.149317e-04\n",
            "step 2501 | loss: 2.183065 | lr: 4.147846e-04\n",
            "step 2502 | loss: 2.121358 | lr: 4.146374e-04\n",
            "step 2503 | loss: 2.214851 | lr: 4.144902e-04\n",
            "step 2504 | loss: 2.055080 | lr: 4.143430e-04\n",
            "step 2505 | loss: 2.253554 | lr: 4.141958e-04\n",
            "step 2506 | loss: 2.219833 | lr: 4.140485e-04\n",
            "step 2507 | loss: 1.991705 | lr: 4.139012e-04\n",
            "step 2508 | loss: 1.979166 | lr: 4.137539e-04\n",
            "step 2509 | loss: 2.053718 | lr: 4.136065e-04\n",
            "step 2510 | loss: 2.093608 | lr: 4.134591e-04\n",
            "step 2511 | loss: 2.249024 | lr: 4.133117e-04\n",
            "step 2512 | loss: 2.216585 | lr: 4.131642e-04\n",
            "step 2513 | loss: 2.055765 | lr: 4.130168e-04\n",
            "step 2514 | loss: 2.083573 | lr: 4.128693e-04\n",
            "step 2515 | loss: 2.197060 | lr: 4.127217e-04\n",
            "step 2516 | loss: 1.949514 | lr: 4.125741e-04\n",
            "step 2517 | loss: 1.985521 | lr: 4.124265e-04\n",
            "step 2518 | loss: 1.848913 | lr: 4.122789e-04\n",
            "step 2519 | loss: 2.342542 | lr: 4.121312e-04\n",
            "step 2520 | loss: 2.228264 | lr: 4.119836e-04\n",
            "step 2521 | loss: 2.188112 | lr: 4.118358e-04\n",
            "step 2522 | loss: 2.549710 | lr: 4.116881e-04\n",
            "step 2523 | loss: 2.532960 | lr: 4.115403e-04\n",
            "step 2524 | loss: 2.377107 | lr: 4.113925e-04\n",
            "step 2525 | loss: 2.262184 | lr: 4.112447e-04\n",
            "step 2526 | loss: 2.439394 | lr: 4.110968e-04\n",
            "step 2527 | loss: 2.353977 | lr: 4.109489e-04\n",
            "step 2528 | loss: 2.262641 | lr: 4.108010e-04\n",
            "step 2529 | loss: 2.214306 | lr: 4.106530e-04\n",
            "step 2530 | loss: 2.381352 | lr: 4.105050e-04\n",
            "step 2531 | loss: 2.287570 | lr: 4.103570e-04\n",
            "step 2532 | loss: 2.256654 | lr: 4.102090e-04\n",
            "step 2533 | loss: 2.256399 | lr: 4.100609e-04\n",
            "step 2534 | loss: 2.239832 | lr: 4.099128e-04\n",
            "step 2535 | loss: 2.197947 | lr: 4.097647e-04\n",
            "step 2536 | loss: 2.148611 | lr: 4.096165e-04\n",
            "step 2537 | loss: 1.863413 | lr: 4.094683e-04\n",
            "step 2538 | loss: 2.314007 | lr: 4.093201e-04\n",
            "step 2539 | loss: 2.117239 | lr: 4.091719e-04\n",
            "step 2540 | loss: 2.388246 | lr: 4.090236e-04\n",
            "step 2541 | loss: 2.425135 | lr: 4.088753e-04\n",
            "step 2542 | loss: 2.316303 | lr: 4.087270e-04\n",
            "step 2543 | loss: 2.249844 | lr: 4.085786e-04\n",
            "step 2544 | loss: 2.128600 | lr: 4.084303e-04\n",
            "step 2545 | loss: 2.105062 | lr: 4.082818e-04\n",
            "step 2546 | loss: 2.333157 | lr: 4.081334e-04\n",
            "step 2547 | loss: 1.991589 | lr: 4.079849e-04\n",
            "step 2548 | loss: 2.230179 | lr: 4.078364e-04\n",
            "step 2549 | loss: 1.911418 | lr: 4.076879e-04\n",
            "step 2550 | loss: 2.341302 | lr: 4.075393e-04\n",
            "step 2551 | loss: 2.180764 | lr: 4.073908e-04\n",
            "step 2552 | loss: 2.154298 | lr: 4.072422e-04\n",
            "step 2553 | loss: 2.069306 | lr: 4.070935e-04\n",
            "step 2554 | loss: 2.000594 | lr: 4.069448e-04\n",
            "step 2555 | loss: 1.964638 | lr: 4.067962e-04\n",
            "step 2556 | loss: 2.118739 | lr: 4.066474e-04\n",
            "step 2557 | loss: 2.339631 | lr: 4.064987e-04\n",
            "step 2558 | loss: 2.173727 | lr: 4.063499e-04\n",
            "step 2559 | loss: 2.266727 | lr: 4.062011e-04\n",
            "step 2560 | loss: 2.258798 | lr: 4.060523e-04\n",
            "step 2561 | loss: 2.034104 | lr: 4.059034e-04\n",
            "step 2562 | loss: 2.284365 | lr: 4.057545e-04\n",
            "step 2563 | loss: 2.408029 | lr: 4.056056e-04\n",
            "step 2564 | loss: 2.313158 | lr: 4.054567e-04\n",
            "step 2565 | loss: 2.115351 | lr: 4.053077e-04\n",
            "step 2566 | loss: 2.307112 | lr: 4.051587e-04\n",
            "step 2567 | loss: 2.126212 | lr: 4.050097e-04\n",
            "step 2568 | loss: 2.163756 | lr: 4.048606e-04\n",
            "step 2569 | loss: 1.834690 | lr: 4.047115e-04\n",
            "step 2570 | loss: 2.209906 | lr: 4.045624e-04\n",
            "step 2571 | loss: 2.120015 | lr: 4.044133e-04\n",
            "step 2572 | loss: 2.131987 | lr: 4.042641e-04\n",
            "step 2573 | loss: 1.955960 | lr: 4.041149e-04\n",
            "step 2574 | loss: 1.993662 | lr: 4.039657e-04\n",
            "step 2575 | loss: 1.956741 | lr: 4.038165e-04\n",
            "step 2576 | loss: 2.161487 | lr: 4.036672e-04\n",
            "step 2577 | loss: 1.834885 | lr: 4.035179e-04\n",
            "step 2578 | loss: 2.114252 | lr: 4.033686e-04\n",
            "step 2579 | loss: 2.494363 | lr: 4.032192e-04\n",
            "step 2580 | loss: 2.422462 | lr: 4.030698e-04\n",
            "step 2581 | loss: 2.293408 | lr: 4.029204e-04\n",
            "step 2582 | loss: 2.300065 | lr: 4.027710e-04\n",
            "step 2583 | loss: 2.393799 | lr: 4.026215e-04\n",
            "step 2584 | loss: 2.168785 | lr: 4.024721e-04\n",
            "step 2585 | loss: 2.401797 | lr: 4.023226e-04\n",
            "step 2586 | loss: 2.393746 | lr: 4.021730e-04\n",
            "step 2587 | loss: 2.538247 | lr: 4.020234e-04\n",
            "step 2588 | loss: 2.517004 | lr: 4.018739e-04\n",
            "step 2589 | loss: 2.255058 | lr: 4.017242e-04\n",
            "step 2590 | loss: 2.403955 | lr: 4.015746e-04\n",
            "step 2591 | loss: 2.199208 | lr: 4.014249e-04\n",
            "step 2592 | loss: 2.331211 | lr: 4.012752e-04\n",
            "step 2593 | loss: 2.181913 | lr: 4.011255e-04\n",
            "step 2594 | loss: 2.321355 | lr: 4.009758e-04\n",
            "step 2595 | loss: 2.435143 | lr: 4.008260e-04\n",
            "step 2596 | loss: 2.405944 | lr: 4.006762e-04\n",
            "step 2597 | loss: 2.271896 | lr: 4.005264e-04\n",
            "step 2598 | loss: 2.242902 | lr: 4.003765e-04\n",
            "step 2599 | loss: 2.080799 | lr: 4.002266e-04\n",
            "step 2600 | loss: 2.328161 | lr: 4.000767e-04\n",
            "step 2601 | loss: 2.181171 | lr: 3.999268e-04\n",
            "step 2602 | loss: 1.950988 | lr: 3.997768e-04\n",
            "step 2603 | loss: 1.966549 | lr: 3.996269e-04\n",
            "step 2604 | loss: 2.075158 | lr: 3.994768e-04\n",
            "step 2605 | loss: 2.023945 | lr: 3.993268e-04\n",
            "step 2606 | loss: 2.025840 | lr: 3.991768e-04\n",
            "step 2607 | loss: 2.271940 | lr: 3.990267e-04\n",
            "step 2608 | loss: 1.929871 | lr: 3.988766e-04\n",
            "step 2609 | loss: 2.130619 | lr: 3.987264e-04\n",
            "step 2610 | loss: 1.836222 | lr: 3.985763e-04\n",
            "step 2611 | loss: 2.064737 | lr: 3.984261e-04\n",
            "step 2612 | loss: 1.880236 | lr: 3.982759e-04\n",
            "step 2613 | loss: 1.890275 | lr: 3.981256e-04\n",
            "step 2614 | loss: 1.980945 | lr: 3.979754e-04\n",
            "step 2615 | loss: 1.882885 | lr: 3.978251e-04\n",
            "step 2616 | loss: 1.897487 | lr: 3.976748e-04\n",
            "step 2617 | loss: 2.433136 | lr: 3.975244e-04\n",
            "step 2618 | loss: 2.221476 | lr: 3.973741e-04\n",
            "step 2619 | loss: 2.339958 | lr: 3.972237e-04\n",
            "step 2620 | loss: 2.204201 | lr: 3.970733e-04\n",
            "step 2621 | loss: 2.058111 | lr: 3.969228e-04\n",
            "step 2622 | loss: 2.013908 | lr: 3.967724e-04\n",
            "step 2623 | loss: 1.809872 | lr: 3.966219e-04\n",
            "step 2624 | loss: 2.051946 | lr: 3.964714e-04\n",
            "step 2625 | loss: 2.011280 | lr: 3.963208e-04\n",
            "step 2626 | loss: 1.946415 | lr: 3.961703e-04\n",
            "step 2627 | loss: 1.960838 | lr: 3.960197e-04\n",
            "step 2628 | loss: 1.955120 | lr: 3.958691e-04\n",
            "step 2629 | loss: 1.873004 | lr: 3.957184e-04\n",
            "step 2630 | loss: 1.920108 | lr: 3.955678e-04\n",
            "step 2631 | loss: 1.865386 | lr: 3.954171e-04\n",
            "step 2632 | loss: 1.863634 | lr: 3.952664e-04\n",
            "step 2633 | loss: 1.488437 | lr: 3.951156e-04\n",
            "step 2634 | loss: 2.019235 | lr: 3.949649e-04\n",
            "step 2635 | loss: 2.272890 | lr: 3.948141e-04\n",
            "step 2636 | loss: 2.157951 | lr: 3.946633e-04\n",
            "step 2637 | loss: 2.277071 | lr: 3.945125e-04\n",
            "step 2638 | loss: 1.908335 | lr: 3.943616e-04\n",
            "step 2639 | loss: 2.009322 | lr: 3.942107e-04\n",
            "step 2640 | loss: 2.518093 | lr: 3.940598e-04\n",
            "step 2641 | loss: 2.274637 | lr: 3.939089e-04\n",
            "step 2642 | loss: 2.176600 | lr: 3.937580e-04\n",
            "step 2643 | loss: 2.185587 | lr: 3.936070e-04\n",
            "step 2644 | loss: 2.170473 | lr: 3.934560e-04\n",
            "step 2645 | loss: 2.220554 | lr: 3.933050e-04\n",
            "step 2646 | loss: 2.099255 | lr: 3.931539e-04\n",
            "step 2647 | loss: 2.286375 | lr: 3.930028e-04\n",
            "step 2648 | loss: 2.195527 | lr: 3.928517e-04\n",
            "step 2649 | loss: 2.099594 | lr: 3.927006e-04\n",
            "step 2650 | loss: 2.002328 | lr: 3.925495e-04\n",
            "step 2651 | loss: 1.705215 | lr: 3.923983e-04\n",
            "step 2652 | loss: 1.967562 | lr: 3.922471e-04\n",
            "step 2653 | loss: 1.855535 | lr: 3.920959e-04\n",
            "step 2654 | loss: 1.873150 | lr: 3.919447e-04\n",
            "step 2655 | loss: 1.901292 | lr: 3.917934e-04\n",
            "step 2656 | loss: 1.977380 | lr: 3.916421e-04\n",
            "step 2657 | loss: 1.798648 | lr: 3.914908e-04\n",
            "step 2658 | loss: 2.048727 | lr: 3.913395e-04\n",
            "step 2659 | loss: 1.928851 | lr: 3.911882e-04\n",
            "step 2660 | loss: 2.154794 | lr: 3.910368e-04\n",
            "step 2661 | loss: 2.017028 | lr: 3.908854e-04\n",
            "step 2662 | loss: 1.931917 | lr: 3.907340e-04\n",
            "step 2663 | loss: 2.215791 | lr: 3.905825e-04\n",
            "step 2664 | loss: 1.924963 | lr: 3.904311e-04\n",
            "step 2665 | loss: 1.799159 | lr: 3.902796e-04\n",
            "step 2666 | loss: 1.991356 | lr: 3.901281e-04\n",
            "step 2667 | loss: 1.897472 | lr: 3.899765e-04\n",
            "step 2668 | loss: 1.942577 | lr: 3.898250e-04\n",
            "step 2669 | loss: 1.820747 | lr: 3.896734e-04\n",
            "step 2670 | loss: 1.978891 | lr: 3.895218e-04\n",
            "step 2671 | loss: 1.973158 | lr: 3.893702e-04\n",
            "step 2672 | loss: 1.749892 | lr: 3.892185e-04\n",
            "step 2673 | loss: 1.808043 | lr: 3.890668e-04\n",
            "step 2674 | loss: 1.863714 | lr: 3.889151e-04\n",
            "step 2675 | loss: 1.929789 | lr: 3.887634e-04\n",
            "step 2676 | loss: 2.045215 | lr: 3.886117e-04\n",
            "step 2677 | loss: 1.965537 | lr: 3.884599e-04\n",
            "step 2678 | loss: 1.824877 | lr: 3.883081e-04\n",
            "step 2679 | loss: 1.832810 | lr: 3.881563e-04\n",
            "step 2680 | loss: 1.915586 | lr: 3.880045e-04\n",
            "step 2681 | loss: 1.700751 | lr: 3.878527e-04\n",
            "step 2682 | loss: 1.761723 | lr: 3.877008e-04\n",
            "step 2683 | loss: 1.643347 | lr: 3.875489e-04\n",
            "step 2684 | loss: 2.050211 | lr: 3.873970e-04\n",
            "step 2685 | loss: 1.965298 | lr: 3.872450e-04\n",
            "step 2686 | loss: 1.895029 | lr: 3.870931e-04\n",
            "step 2687 | loss: 2.267129 | lr: 3.869411e-04\n",
            "step 2688 | loss: 2.246280 | lr: 3.867891e-04\n",
            "step 2689 | loss: 2.093550 | lr: 3.866371e-04\n",
            "step 2690 | loss: 2.051073 | lr: 3.864850e-04\n",
            "step 2691 | loss: 2.182308 | lr: 3.863330e-04\n",
            "step 2692 | loss: 2.047376 | lr: 3.861809e-04\n",
            "step 2693 | loss: 2.003339 | lr: 3.860288e-04\n",
            "step 2694 | loss: 1.897662 | lr: 3.858766e-04\n",
            "step 2695 | loss: 2.081754 | lr: 3.857245e-04\n",
            "step 2696 | loss: 2.054483 | lr: 3.855723e-04\n",
            "step 2697 | loss: 2.010354 | lr: 3.854201e-04\n",
            "step 2698 | loss: 2.019220 | lr: 3.852679e-04\n",
            "step 2699 | loss: 2.043536 | lr: 3.851156e-04\n",
            "step 2700 | loss: 1.943779 | lr: 3.849634e-04\n",
            "step 2701 | loss: 1.926469 | lr: 3.848111e-04\n",
            "step 2702 | loss: 1.660383 | lr: 3.846588e-04\n",
            "step 2703 | loss: 2.098679 | lr: 3.845064e-04\n",
            "step 2704 | loss: 1.882870 | lr: 3.843541e-04\n",
            "step 2705 | loss: 2.121064 | lr: 3.842017e-04\n",
            "step 2706 | loss: 2.143836 | lr: 3.840493e-04\n",
            "step 2707 | loss: 2.043728 | lr: 3.838969e-04\n",
            "step 2708 | loss: 2.003723 | lr: 3.837445e-04\n",
            "step 2709 | loss: 1.945409 | lr: 3.835921e-04\n",
            "step 2710 | loss: 1.926518 | lr: 3.834396e-04\n",
            "step 2711 | loss: 2.092471 | lr: 3.832871e-04\n",
            "step 2712 | loss: 1.735527 | lr: 3.831346e-04\n",
            "step 2713 | loss: 1.988267 | lr: 3.829820e-04\n",
            "step 2714 | loss: 1.700140 | lr: 3.828295e-04\n",
            "step 2715 | loss: 2.121291 | lr: 3.826769e-04\n",
            "step 2716 | loss: 1.945029 | lr: 3.825243e-04\n",
            "step 2717 | loss: 1.905439 | lr: 3.823717e-04\n",
            "step 2718 | loss: 1.800805 | lr: 3.822191e-04\n",
            "step 2719 | loss: 1.817780 | lr: 3.820664e-04\n",
            "step 2720 | loss: 1.766219 | lr: 3.819137e-04\n",
            "step 2721 | loss: 1.922292 | lr: 3.817610e-04\n",
            "step 2722 | loss: 2.107905 | lr: 3.816083e-04\n",
            "step 2723 | loss: 1.907007 | lr: 3.814556e-04\n",
            "step 2724 | loss: 2.031580 | lr: 3.813028e-04\n",
            "step 2725 | loss: 2.037053 | lr: 3.811500e-04\n",
            "step 2726 | loss: 1.854665 | lr: 3.809972e-04\n",
            "step 2727 | loss: 2.027468 | lr: 3.808444e-04\n",
            "step 2728 | loss: 2.138348 | lr: 3.806916e-04\n",
            "step 2729 | loss: 2.071849 | lr: 3.805387e-04\n",
            "step 2730 | loss: 1.894481 | lr: 3.803858e-04\n",
            "step 2731 | loss: 2.079314 | lr: 3.802329e-04\n",
            "step 2732 | loss: 1.913255 | lr: 3.800800e-04\n",
            "step 2733 | loss: 1.946800 | lr: 3.799271e-04\n",
            "step 2734 | loss: 1.627656 | lr: 3.797741e-04\n",
            "step 2735 | loss: 1.967702 | lr: 3.796211e-04\n",
            "step 2736 | loss: 1.909826 | lr: 3.794681e-04\n",
            "step 2737 | loss: 1.902000 | lr: 3.793151e-04\n",
            "step 2738 | loss: 1.767132 | lr: 3.791621e-04\n",
            "step 2739 | loss: 1.777702 | lr: 3.790090e-04\n",
            "step 2740 | loss: 1.730053 | lr: 3.788559e-04\n",
            "step 2741 | loss: 1.906136 | lr: 3.787028e-04\n",
            "step 2742 | loss: 1.612433 | lr: 3.785497e-04\n",
            "step 2743 | loss: 1.864745 | lr: 3.783966e-04\n",
            "step 2744 | loss: 2.216267 | lr: 3.782434e-04\n",
            "step 2745 | loss: 2.158149 | lr: 3.780903e-04\n",
            "step 2746 | loss: 2.011874 | lr: 3.779371e-04\n",
            "step 2747 | loss: 2.034284 | lr: 3.777839e-04\n",
            "step 2748 | loss: 2.132660 | lr: 3.776306e-04\n",
            "step 2749 | loss: 1.897685 | lr: 3.774774e-04\n",
            "step 2750 | loss: 2.085908 | lr: 3.773241e-04\n",
            "step 2751 | loss: 2.079817 | lr: 3.771708e-04\n",
            "step 2752 | loss: 2.271645 | lr: 3.770175e-04\n",
            "step 2753 | loss: 2.295505 | lr: 3.768642e-04\n",
            "step 2754 | loss: 2.009756 | lr: 3.767108e-04\n",
            "step 2755 | loss: 2.123863 | lr: 3.765575e-04\n",
            "step 2756 | loss: 1.972226 | lr: 3.764041e-04\n",
            "step 2757 | loss: 2.146300 | lr: 3.762507e-04\n",
            "step 2758 | loss: 1.954782 | lr: 3.760973e-04\n",
            "step 2759 | loss: 2.049128 | lr: 3.759439e-04\n",
            "step 2760 | loss: 2.210352 | lr: 3.757904e-04\n",
            "step 2761 | loss: 2.159950 | lr: 3.756369e-04\n",
            "step 2762 | loss: 2.047699 | lr: 3.754834e-04\n",
            "step 2763 | loss: 2.008813 | lr: 3.753299e-04\n",
            "step 2764 | loss: 1.843897 | lr: 3.751764e-04\n",
            "step 2765 | loss: 2.085614 | lr: 3.750228e-04\n",
            "step 2766 | loss: 1.926950 | lr: 3.748693e-04\n",
            "step 2767 | loss: 1.736278 | lr: 3.747157e-04\n",
            "step 2768 | loss: 1.764074 | lr: 3.745621e-04\n",
            "step 2769 | loss: 1.830372 | lr: 3.744085e-04\n",
            "step 2770 | loss: 1.776693 | lr: 3.742548e-04\n",
            "step 2771 | loss: 1.754648 | lr: 3.741012e-04\n",
            "step 2772 | loss: 1.959730 | lr: 3.739475e-04\n",
            "step 2773 | loss: 1.640740 | lr: 3.737938e-04\n",
            "step 2774 | loss: 1.850699 | lr: 3.736401e-04\n",
            "step 2775 | loss: 1.599334 | lr: 3.734864e-04\n",
            "step 2776 | loss: 1.828225 | lr: 3.733326e-04\n",
            "step 2777 | loss: 1.686752 | lr: 3.731789e-04\n",
            "step 2778 | loss: 1.700773 | lr: 3.730251e-04\n",
            "step 2779 | loss: 1.775486 | lr: 3.728713e-04\n",
            "step 2780 | loss: 1.711341 | lr: 3.727175e-04\n",
            "step 2781 | loss: 1.699201 | lr: 3.725636e-04\n",
            "step 2782 | loss: 2.157331 | lr: 3.724098e-04\n",
            "step 2783 | loss: 2.012667 | lr: 3.722559e-04\n",
            "step 2784 | loss: 2.122871 | lr: 3.721020e-04\n",
            "step 2785 | loss: 1.973275 | lr: 3.719481e-04\n",
            "step 2786 | loss: 1.854482 | lr: 3.717942e-04\n",
            "step 2787 | loss: 1.842242 | lr: 3.716403e-04\n",
            "step 2788 | loss: 1.642363 | lr: 3.714863e-04\n",
            "step 2789 | loss: 1.853302 | lr: 3.713323e-04\n",
            "step 2790 | loss: 1.783883 | lr: 3.711784e-04\n",
            "step 2791 | loss: 1.748209 | lr: 3.710243e-04\n",
            "step 2792 | loss: 1.741465 | lr: 3.708703e-04\n",
            "step 2793 | loss: 1.769955 | lr: 3.707163e-04\n",
            "step 2794 | loss: 1.685630 | lr: 3.705622e-04\n",
            "step 2795 | loss: 1.655240 | lr: 3.704082e-04\n",
            "step 2796 | loss: 1.676786 | lr: 3.702541e-04\n",
            "step 2797 | loss: 1.683012 | lr: 3.701000e-04\n",
            "step 2798 | loss: 1.317562 | lr: 3.699458e-04\n",
            "step 2799 | loss: 1.811617 | lr: 3.697917e-04\n",
            "step 2800 | loss: 2.079221 | lr: 3.696375e-04\n",
            "step 2801 | loss: 1.982711 | lr: 3.694834e-04\n",
            "step 2802 | loss: 2.044605 | lr: 3.693292e-04\n",
            "step 2803 | loss: 1.719695 | lr: 3.691750e-04\n",
            "step 2804 | loss: 1.821343 | lr: 3.690207e-04\n",
            "step 2805 | loss: 2.313399 | lr: 3.688665e-04\n",
            "step 2806 | loss: 2.064902 | lr: 3.687122e-04\n",
            "step 2807 | loss: 2.021687 | lr: 3.685580e-04\n",
            "step 2808 | loss: 1.981617 | lr: 3.684037e-04\n",
            "step 2809 | loss: 2.005851 | lr: 3.682494e-04\n",
            "step 2810 | loss: 1.964923 | lr: 3.680950e-04\n",
            "step 2811 | loss: 1.869268 | lr: 3.679407e-04\n",
            "step 2812 | loss: 2.023390 | lr: 3.677864e-04\n",
            "step 2813 | loss: 1.981585 | lr: 3.676320e-04\n",
            "step 2814 | loss: 1.884648 | lr: 3.674776e-04\n",
            "step 2815 | loss: 1.809537 | lr: 3.673232e-04\n",
            "step 2816 | loss: 1.532331 | lr: 3.671688e-04\n",
            "step 2817 | loss: 1.719763 | lr: 3.670143e-04\n",
            "step 2818 | loss: 1.613421 | lr: 3.668599e-04\n",
            "step 2819 | loss: 1.646772 | lr: 3.667054e-04\n",
            "step 2820 | loss: 1.693904 | lr: 3.665510e-04\n",
            "step 2821 | loss: 1.715369 | lr: 3.663965e-04\n",
            "step 2822 | loss: 1.618394 | lr: 3.662419e-04\n",
            "step 2823 | loss: 1.802628 | lr: 3.660874e-04\n",
            "step 2824 | loss: 1.713615 | lr: 3.659329e-04\n",
            "step 2825 | loss: 1.837713 | lr: 3.657783e-04\n",
            "step 2826 | loss: 1.807099 | lr: 3.656237e-04\n",
            "step 2827 | loss: 1.715298 | lr: 3.654692e-04\n",
            "step 2828 | loss: 1.979395 | lr: 3.653145e-04\n",
            "step 2829 | loss: 1.735280 | lr: 3.651599e-04\n",
            "step 2830 | loss: 1.567664 | lr: 3.650053e-04\n",
            "step 2831 | loss: 1.738008 | lr: 3.648506e-04\n",
            "step 2832 | loss: 1.665362 | lr: 3.646960e-04\n",
            "step 2833 | loss: 1.730297 | lr: 3.645413e-04\n",
            "step 2834 | loss: 1.621504 | lr: 3.643866e-04\n",
            "step 2835 | loss: 1.793275 | lr: 3.642319e-04\n",
            "step 2836 | loss: 1.853534 | lr: 3.640772e-04\n",
            "step 2837 | loss: 1.571828 | lr: 3.639224e-04\n",
            "step 2838 | loss: 1.622145 | lr: 3.637677e-04\n",
            "step 2839 | loss: 1.662464 | lr: 3.636129e-04\n",
            "step 2840 | loss: 1.716325 | lr: 3.634581e-04\n",
            "step 2841 | loss: 1.809251 | lr: 3.633033e-04\n",
            "step 2842 | loss: 1.726604 | lr: 3.631485e-04\n",
            "step 2843 | loss: 1.621298 | lr: 3.629937e-04\n",
            "step 2844 | loss: 1.676458 | lr: 3.628388e-04\n",
            "step 2845 | loss: 1.670402 | lr: 3.626840e-04\n",
            "step 2846 | loss: 1.492083 | lr: 3.625291e-04\n",
            "step 2847 | loss: 1.535321 | lr: 3.623742e-04\n",
            "step 2848 | loss: 1.425354 | lr: 3.622193e-04\n",
            "step 2849 | loss: 1.822874 | lr: 3.620644e-04\n",
            "step 2850 | loss: 1.751580 | lr: 3.619095e-04\n",
            "step 2851 | loss: 1.656748 | lr: 3.617545e-04\n",
            "step 2852 | loss: 1.943257 | lr: 3.615996e-04\n",
            "step 2853 | loss: 1.979044 | lr: 3.614446e-04\n",
            "step 2854 | loss: 1.815245 | lr: 3.612896e-04\n",
            "step 2855 | loss: 1.800128 | lr: 3.611346e-04\n",
            "step 2856 | loss: 1.928739 | lr: 3.609796e-04\n",
            "step 2857 | loss: 1.824174 | lr: 3.608246e-04\n",
            "step 2858 | loss: 1.755208 | lr: 3.606695e-04\n",
            "step 2859 | loss: 1.647822 | lr: 3.605145e-04\n",
            "step 2860 | loss: 1.794781 | lr: 3.603594e-04\n",
            "step 2861 | loss: 1.760614 | lr: 3.602043e-04\n",
            "step 2862 | loss: 1.747150 | lr: 3.600492e-04\n",
            "step 2863 | loss: 1.766983 | lr: 3.598941e-04\n",
            "step 2864 | loss: 1.788523 | lr: 3.597390e-04\n",
            "step 2865 | loss: 1.701438 | lr: 3.595838e-04\n",
            "step 2866 | loss: 1.694771 | lr: 3.594287e-04\n",
            "step 2867 | loss: 1.416924 | lr: 3.592735e-04\n",
            "step 2868 | loss: 1.829687 | lr: 3.591183e-04\n",
            "step 2869 | loss: 1.620980 | lr: 3.589631e-04\n",
            "step 2870 | loss: 1.900884 | lr: 3.588079e-04\n",
            "step 2871 | loss: 1.887090 | lr: 3.586527e-04\n",
            "step 2872 | loss: 1.834059 | lr: 3.584975e-04\n",
            "step 2873 | loss: 1.771692 | lr: 3.583422e-04\n",
            "step 2874 | loss: 1.713994 | lr: 3.581870e-04\n",
            "step 2875 | loss: 1.706106 | lr: 3.580317e-04\n",
            "step 2876 | loss: 1.823896 | lr: 3.578764e-04\n",
            "step 2877 | loss: 1.516517 | lr: 3.577211e-04\n",
            "step 2878 | loss: 1.727242 | lr: 3.575658e-04\n",
            "step 2879 | loss: 1.483588 | lr: 3.574105e-04\n",
            "step 2880 | loss: 1.888053 | lr: 3.572551e-04\n",
            "step 2881 | loss: 1.747317 | lr: 3.570998e-04\n",
            "step 2882 | loss: 1.707088 | lr: 3.569444e-04\n",
            "step 2883 | loss: 1.642563 | lr: 3.567891e-04\n",
            "step 2884 | loss: 1.616349 | lr: 3.566337e-04\n",
            "step 2885 | loss: 1.507417 | lr: 3.564783e-04\n",
            "step 2886 | loss: 1.633981 | lr: 3.563228e-04\n",
            "step 2887 | loss: 1.845375 | lr: 3.561674e-04\n",
            "step 2888 | loss: 1.677930 | lr: 3.560120e-04\n",
            "step 2889 | loss: 1.821005 | lr: 3.558565e-04\n",
            "step 2890 | loss: 1.808762 | lr: 3.557011e-04\n",
            "step 2891 | loss: 1.654290 | lr: 3.555456e-04\n",
            "step 2892 | loss: 1.788684 | lr: 3.553901e-04\n",
            "step 2893 | loss: 1.886818 | lr: 3.552346e-04\n",
            "step 2894 | loss: 1.840447 | lr: 3.550791e-04\n",
            "step 2895 | loss: 1.632069 | lr: 3.549236e-04\n",
            "step 2896 | loss: 1.772398 | lr: 3.547680e-04\n",
            "step 2897 | loss: 1.666407 | lr: 3.546125e-04\n",
            "step 2898 | loss: 1.705571 | lr: 3.544569e-04\n",
            "step 2899 | loss: 1.434168 | lr: 3.543013e-04\n",
            "step 2900 | loss: 1.766827 | lr: 3.541458e-04\n",
            "step 2901 | loss: 1.683847 | lr: 3.539902e-04\n",
            "step 2902 | loss: 1.687158 | lr: 3.538346e-04\n",
            "step 2903 | loss: 1.533535 | lr: 3.536789e-04\n",
            "step 2904 | loss: 1.508416 | lr: 3.535233e-04\n",
            "step 2905 | loss: 1.495765 | lr: 3.533677e-04\n",
            "step 2906 | loss: 1.653934 | lr: 3.532120e-04\n",
            "step 2907 | loss: 1.449533 | lr: 3.530563e-04\n",
            "step 2908 | loss: 1.601425 | lr: 3.529007e-04\n",
            "step 2909 | loss: 1.994854 | lr: 3.527450e-04\n",
            "step 2910 | loss: 1.887581 | lr: 3.525893e-04\n",
            "step 2911 | loss: 1.747946 | lr: 3.524336e-04\n",
            "step 2912 | loss: 1.800594 | lr: 3.522778e-04\n",
            "step 2913 | loss: 1.884499 | lr: 3.521221e-04\n",
            "step 2914 | loss: 1.714708 | lr: 3.519663e-04\n",
            "step 2915 | loss: 1.871152 | lr: 3.518106e-04\n",
            "step 2916 | loss: 1.806771 | lr: 3.516548e-04\n",
            "step 2917 | loss: 1.986397 | lr: 3.514990e-04\n",
            "step 2918 | loss: 2.023841 | lr: 3.513432e-04\n",
            "step 2919 | loss: 1.721880 | lr: 3.511874e-04\n",
            "step 2920 | loss: 1.815902 | lr: 3.510316e-04\n",
            "step 2921 | loss: 1.668412 | lr: 3.508758e-04\n",
            "step 2922 | loss: 1.887325 | lr: 3.507200e-04\n",
            "step 2923 | loss: 1.671950 | lr: 3.505641e-04\n",
            "step 2924 | loss: 1.757429 | lr: 3.504083e-04\n",
            "step 2925 | loss: 1.982235 | lr: 3.502524e-04\n",
            "step 2926 | loss: 1.824322 | lr: 3.500965e-04\n",
            "step 2927 | loss: 1.808661 | lr: 3.499406e-04\n",
            "step 2928 | loss: 1.770857 | lr: 3.497847e-04\n",
            "step 2929 | loss: 1.575158 | lr: 3.496288e-04\n",
            "step 2930 | loss: 1.784463 | lr: 3.494729e-04\n",
            "step 2931 | loss: 1.683988 | lr: 3.493170e-04\n",
            "step 2932 | loss: 1.519375 | lr: 3.491610e-04\n",
            "step 2933 | loss: 1.513899 | lr: 3.490051e-04\n",
            "step 2934 | loss: 1.571303 | lr: 3.488491e-04\n",
            "step 2935 | loss: 1.514396 | lr: 3.486931e-04\n",
            "step 2936 | loss: 1.475349 | lr: 3.485372e-04\n",
            "step 2937 | loss: 1.753411 | lr: 3.483812e-04\n",
            "step 2938 | loss: 1.402239 | lr: 3.482252e-04\n",
            "step 2939 | loss: 1.617243 | lr: 3.480691e-04\n",
            "step 2940 | loss: 1.407776 | lr: 3.479131e-04\n",
            "step 2941 | loss: 1.542861 | lr: 3.477571e-04\n",
            "step 2942 | loss: 1.450797 | lr: 3.476010e-04\n",
            "step 2943 | loss: 1.482245 | lr: 3.474450e-04\n",
            "step 2944 | loss: 1.529212 | lr: 3.472889e-04\n",
            "step 2945 | loss: 1.462721 | lr: 3.471329e-04\n",
            "step 2946 | loss: 1.449991 | lr: 3.469768e-04\n",
            "step 2947 | loss: 1.859470 | lr: 3.468207e-04\n",
            "step 2948 | loss: 1.768533 | lr: 3.466646e-04\n",
            "step 2949 | loss: 1.873606 | lr: 3.465085e-04\n",
            "step 2950 | loss: 1.766937 | lr: 3.463523e-04\n",
            "step 2951 | loss: 1.585128 | lr: 3.461962e-04\n",
            "step 2952 | loss: 1.573183 | lr: 3.460401e-04\n",
            "step 2953 | loss: 1.402511 | lr: 3.458839e-04\n",
            "step 2954 | loss: 1.614437 | lr: 3.457278e-04\n",
            "step 2955 | loss: 1.573556 | lr: 3.455716e-04\n",
            "step 2956 | loss: 1.549907 | lr: 3.454154e-04\n",
            "step 2957 | loss: 1.514965 | lr: 3.452592e-04\n",
            "step 2958 | loss: 1.568858 | lr: 3.451030e-04\n",
            "step 2959 | loss: 1.548136 | lr: 3.449468e-04\n",
            "step 2960 | loss: 1.470957 | lr: 3.447906e-04\n",
            "step 2961 | loss: 1.473870 | lr: 3.446344e-04\n",
            "step 2962 | loss: 1.446392 | lr: 3.444782e-04\n",
            "step 2963 | loss: 1.148160 | lr: 3.443219e-04\n",
            "step 2964 | loss: 1.519172 | lr: 3.441657e-04\n",
            "step 2965 | loss: 1.755491 | lr: 3.440094e-04\n",
            "step 2966 | loss: 1.679094 | lr: 3.438531e-04\n",
            "step 2967 | loss: 1.783909 | lr: 3.436969e-04\n",
            "step 2968 | loss: 1.511710 | lr: 3.435406e-04\n",
            "step 2969 | loss: 1.595320 | lr: 3.433843e-04\n",
            "step 2970 | loss: 2.078973 | lr: 3.432280e-04\n",
            "step 2971 | loss: 1.811481 | lr: 3.430717e-04\n",
            "step 2972 | loss: 1.723332 | lr: 3.429153e-04\n",
            "step 2973 | loss: 1.684541 | lr: 3.427590e-04\n",
            "step 2974 | loss: 1.725364 | lr: 3.426027e-04\n",
            "step 2975 | loss: 1.699994 | lr: 3.424463e-04\n",
            "step 2976 | loss: 1.648990 | lr: 3.422900e-04\n",
            "step 2977 | loss: 1.810775 | lr: 3.421336e-04\n",
            "step 2978 | loss: 1.767304 | lr: 3.419772e-04\n",
            "step 2979 | loss: 1.648579 | lr: 3.418209e-04\n",
            "step 2980 | loss: 1.610925 | lr: 3.416645e-04\n",
            "step 2981 | loss: 1.336661 | lr: 3.415081e-04\n",
            "step 2982 | loss: 1.475965 | lr: 3.413517e-04\n",
            "step 2983 | loss: 1.365872 | lr: 3.411953e-04\n",
            "step 2984 | loss: 1.439144 | lr: 3.410389e-04\n",
            "step 2985 | loss: 1.438510 | lr: 3.408824e-04\n",
            "step 2986 | loss: 1.459298 | lr: 3.407260e-04\n",
            "step 2987 | loss: 1.341903 | lr: 3.405696e-04\n",
            "step 2988 | loss: 1.567283 | lr: 3.404131e-04\n",
            "step 2989 | loss: 1.454071 | lr: 3.402566e-04\n",
            "step 2990 | loss: 1.566075 | lr: 3.401002e-04\n",
            "step 2991 | loss: 1.525803 | lr: 3.399437e-04\n",
            "step 2992 | loss: 1.396438 | lr: 3.397872e-04\n",
            "step 2993 | loss: 1.721339 | lr: 3.396307e-04\n",
            "step 2994 | loss: 1.434569 | lr: 3.394742e-04\n",
            "step 2995 | loss: 1.340061 | lr: 3.393177e-04\n",
            "step 2996 | loss: 1.478855 | lr: 3.391612e-04\n",
            "step 2997 | loss: 1.433581 | lr: 3.390047e-04\n",
            "step 2998 | loss: 1.492861 | lr: 3.388482e-04\n",
            "step 2999 | loss: 1.388845 | lr: 3.386917e-04\n",
            "step 3000 | loss: 1.543944 | lr: 3.385351e-04\n",
            "step 3001 | loss: 1.632650 | lr: 3.383786e-04\n",
            "step 3002 | loss: 1.373357 | lr: 3.382220e-04\n",
            "step 3003 | loss: 1.483565 | lr: 3.380655e-04\n",
            "step 3004 | loss: 1.490391 | lr: 3.379089e-04\n",
            "step 3005 | loss: 1.512099 | lr: 3.377523e-04\n",
            "step 3006 | loss: 1.621030 | lr: 3.375957e-04\n",
            "step 3007 | loss: 1.511898 | lr: 3.374391e-04\n",
            "step 3008 | loss: 1.432070 | lr: 3.372825e-04\n",
            "step 3009 | loss: 1.474185 | lr: 3.371259e-04\n",
            "step 3010 | loss: 1.444238 | lr: 3.369693e-04\n",
            "step 3011 | loss: 1.288426 | lr: 3.368127e-04\n",
            "step 3012 | loss: 1.337388 | lr: 3.366561e-04\n",
            "step 3013 | loss: 1.255535 | lr: 3.364995e-04\n",
            "step 3014 | loss: 1.597540 | lr: 3.363428e-04\n",
            "step 3015 | loss: 1.521679 | lr: 3.361862e-04\n",
            "step 3016 | loss: 1.453793 | lr: 3.360295e-04\n",
            "step 3017 | loss: 1.700559 | lr: 3.358729e-04\n",
            "step 3018 | loss: 1.689039 | lr: 3.357162e-04\n",
            "step 3019 | loss: 1.533905 | lr: 3.355596e-04\n",
            "step 3020 | loss: 1.529556 | lr: 3.354029e-04\n",
            "step 3021 | loss: 1.608451 | lr: 3.352462e-04\n",
            "step 3022 | loss: 1.531731 | lr: 3.350895e-04\n",
            "step 3023 | loss: 1.550056 | lr: 3.349328e-04\n",
            "step 3024 | loss: 1.432140 | lr: 3.347761e-04\n",
            "step 3025 | loss: 1.544645 | lr: 3.346194e-04\n",
            "step 3026 | loss: 1.537158 | lr: 3.344627e-04\n",
            "step 3027 | loss: 1.452446 | lr: 3.343060e-04\n",
            "step 3028 | loss: 1.489958 | lr: 3.341493e-04\n",
            "step 3029 | loss: 1.499971 | lr: 3.339925e-04\n",
            "step 3030 | loss: 1.414041 | lr: 3.338358e-04\n",
            "step 3031 | loss: 1.425657 | lr: 3.336791e-04\n",
            "step 3032 | loss: 1.204618 | lr: 3.335223e-04\n",
            "step 3033 | loss: 1.552884 | lr: 3.333656e-04\n",
            "step 3034 | loss: 1.402758 | lr: 3.332088e-04\n",
            "step 3035 | loss: 1.636436 | lr: 3.330521e-04\n",
            "step 3036 | loss: 1.635040 | lr: 3.328953e-04\n",
            "step 3037 | loss: 1.533744 | lr: 3.327385e-04\n",
            "step 3038 | loss: 1.504659 | lr: 3.325817e-04\n",
            "step 3039 | loss: 1.421371 | lr: 3.324250e-04\n",
            "step 3040 | loss: 1.460162 | lr: 3.322682e-04\n",
            "step 3041 | loss: 1.552652 | lr: 3.321114e-04\n",
            "step 3042 | loss: 1.312307 | lr: 3.319546e-04\n",
            "step 3043 | loss: 1.502367 | lr: 3.317978e-04\n",
            "step 3044 | loss: 1.216776 | lr: 3.316410e-04\n",
            "step 3045 | loss: 1.624440 | lr: 3.314842e-04\n",
            "step 3046 | loss: 1.477733 | lr: 3.313273e-04\n",
            "step 3047 | loss: 1.460273 | lr: 3.311705e-04\n",
            "step 3048 | loss: 1.392645 | lr: 3.310137e-04\n",
            "step 3049 | loss: 1.375541 | lr: 3.308569e-04\n",
            "step 3050 | loss: 1.292844 | lr: 3.307000e-04\n",
            "step 3051 | loss: 1.401983 | lr: 3.305432e-04\n",
            "step 3052 | loss: 1.580770 | lr: 3.303863e-04\n",
            "step 3053 | loss: 1.434587 | lr: 3.302295e-04\n",
            "step 3054 | loss: 1.558255 | lr: 3.300726e-04\n",
            "step 3055 | loss: 1.523782 | lr: 3.299157e-04\n",
            "step 3056 | loss: 1.391832 | lr: 3.297589e-04\n",
            "step 3057 | loss: 1.556810 | lr: 3.296020e-04\n",
            "step 3058 | loss: 1.613767 | lr: 3.294451e-04\n",
            "step 3059 | loss: 1.586200 | lr: 3.292883e-04\n",
            "step 3060 | loss: 1.393957 | lr: 3.291314e-04\n",
            "step 3061 | loss: 1.556325 | lr: 3.289745e-04\n",
            "step 3062 | loss: 1.408580 | lr: 3.288176e-04\n",
            "step 3063 | loss: 1.449527 | lr: 3.286607e-04\n",
            "step 3064 | loss: 1.169703 | lr: 3.285038e-04\n",
            "step 3065 | loss: 1.501422 | lr: 3.283469e-04\n",
            "step 3066 | loss: 1.446084 | lr: 3.281900e-04\n",
            "step 3067 | loss: 1.460323 | lr: 3.280331e-04\n",
            "step 3068 | loss: 1.324118 | lr: 3.278761e-04\n",
            "step 3069 | loss: 1.314692 | lr: 3.277192e-04\n",
            "step 3070 | loss: 1.276130 | lr: 3.275623e-04\n",
            "step 3071 | loss: 1.434457 | lr: 3.274054e-04\n",
            "step 3072 | loss: 1.238457 | lr: 3.272484e-04\n",
            "step 3073 | loss: 1.328796 | lr: 3.270915e-04\n",
            "step 3074 | loss: 1.735458 | lr: 3.269346e-04\n",
            "step 3075 | loss: 1.626694 | lr: 3.267776e-04\n",
            "step 3076 | loss: 1.460790 | lr: 3.266207e-04\n",
            "step 3077 | loss: 1.507015 | lr: 3.264637e-04\n",
            "step 3078 | loss: 1.611735 | lr: 3.263068e-04\n",
            "step 3079 | loss: 1.451099 | lr: 3.261498e-04\n",
            "step 3080 | loss: 1.626994 | lr: 3.259928e-04\n",
            "step 3081 | loss: 1.527194 | lr: 3.258359e-04\n",
            "step 3082 | loss: 1.741219 | lr: 3.256789e-04\n",
            "step 3083 | loss: 1.705853 | lr: 3.255219e-04\n",
            "step 3084 | loss: 1.504379 | lr: 3.253650e-04\n",
            "step 3085 | loss: 1.561347 | lr: 3.252080e-04\n",
            "step 3086 | loss: 1.423138 | lr: 3.250510e-04\n",
            "step 3087 | loss: 1.634465 | lr: 3.248940e-04\n",
            "step 3088 | loss: 1.436835 | lr: 3.247370e-04\n",
            "step 3089 | loss: 1.493493 | lr: 3.245801e-04\n",
            "step 3090 | loss: 1.708524 | lr: 3.244231e-04\n",
            "step 3091 | loss: 1.524993 | lr: 3.242661e-04\n",
            "step 3092 | loss: 1.562828 | lr: 3.241091e-04\n",
            "step 3093 | loss: 1.500162 | lr: 3.239521e-04\n",
            "step 3094 | loss: 1.314796 | lr: 3.237951e-04\n",
            "step 3095 | loss: 1.530246 | lr: 3.236381e-04\n",
            "step 3096 | loss: 1.401018 | lr: 3.234810e-04\n",
            "step 3097 | loss: 1.240015 | lr: 3.233240e-04\n",
            "step 3098 | loss: 1.272469 | lr: 3.231670e-04\n",
            "step 3099 | loss: 1.365407 | lr: 3.230100e-04\n",
            "step 3100 | loss: 1.272476 | lr: 3.228530e-04\n",
            "step 3101 | loss: 1.297507 | lr: 3.226960e-04\n",
            "step 3102 | loss: 1.530438 | lr: 3.225389e-04\n",
            "step 3103 | loss: 1.185611 | lr: 3.223819e-04\n",
            "step 3104 | loss: 1.359092 | lr: 3.222249e-04\n",
            "step 3105 | loss: 1.198191 | lr: 3.220679e-04\n",
            "step 3106 | loss: 1.333038 | lr: 3.219108e-04\n",
            "step 3107 | loss: 1.239238 | lr: 3.217538e-04\n",
            "step 3108 | loss: 1.257416 | lr: 3.215968e-04\n",
            "step 3109 | loss: 1.315480 | lr: 3.214397e-04\n",
            "step 3110 | loss: 1.304549 | lr: 3.212827e-04\n",
            "step 3111 | loss: 1.223890 | lr: 3.211256e-04\n",
            "step 3112 | loss: 1.603988 | lr: 3.209686e-04\n",
            "step 3113 | loss: 1.519199 | lr: 3.208115e-04\n",
            "step 3114 | loss: 1.631510 | lr: 3.206545e-04\n",
            "step 3115 | loss: 1.530447 | lr: 3.204974e-04\n",
            "step 3116 | loss: 1.382629 | lr: 3.203404e-04\n",
            "step 3117 | loss: 1.369686 | lr: 3.201833e-04\n",
            "step 3118 | loss: 1.184275 | lr: 3.200263e-04\n",
            "step 3119 | loss: 1.361369 | lr: 3.198692e-04\n",
            "step 3120 | loss: 1.358028 | lr: 3.197122e-04\n",
            "step 3121 | loss: 1.269366 | lr: 3.195551e-04\n",
            "step 3122 | loss: 1.287480 | lr: 3.193981e-04\n",
            "step 3123 | loss: 1.312388 | lr: 3.192410e-04\n",
            "step 3124 | loss: 1.307813 | lr: 3.190839e-04\n",
            "step 3125 | loss: 1.226870 | lr: 3.189269e-04\n",
            "step 3126 | loss: 1.228249 | lr: 3.187698e-04\n",
            "step 3127 | loss: 1.219413 | lr: 3.186127e-04\n",
            "step 3128 | loss: 0.919990 | lr: 3.184557e-04\n",
            "step 3129 | loss: 1.296340 | lr: 3.182986e-04\n",
            "step 3130 | loss: 1.488018 | lr: 3.181415e-04\n",
            "step 3131 | loss: 1.400958 | lr: 3.179845e-04\n",
            "step 3132 | loss: 1.515105 | lr: 3.178274e-04\n",
            "step 3133 | loss: 1.249823 | lr: 3.176703e-04\n",
            "step 3134 | loss: 1.343192 | lr: 3.175132e-04\n",
            "step 3135 | loss: 1.734393 | lr: 3.173562e-04\n",
            "step 3136 | loss: 1.535655 | lr: 3.171991e-04\n",
            "step 3137 | loss: 1.473308 | lr: 3.170420e-04\n",
            "step 3138 | loss: 1.390333 | lr: 3.168849e-04\n",
            "step 3139 | loss: 1.471284 | lr: 3.167279e-04\n",
            "step 3140 | loss: 1.374335 | lr: 3.165708e-04\n",
            "step 3141 | loss: 1.351680 | lr: 3.164137e-04\n",
            "step 3142 | loss: 1.518675 | lr: 3.162566e-04\n",
            "step 3143 | loss: 1.512545 | lr: 3.160996e-04\n",
            "step 3144 | loss: 1.348347 | lr: 3.159425e-04\n",
            "step 3145 | loss: 1.318120 | lr: 3.157854e-04\n",
            "step 3146 | loss: 1.135112 | lr: 3.156283e-04\n",
            "step 3147 | loss: 1.307822 | lr: 3.154712e-04\n",
            "step 3148 | loss: 1.156307 | lr: 3.153142e-04\n",
            "step 3149 | loss: 1.210901 | lr: 3.151571e-04\n",
            "step 3150 | loss: 1.231187 | lr: 3.150000e-04\n",
            "step 3151 | loss: 1.210270 | lr: 3.148429e-04\n",
            "step 3152 | loss: 1.093887 | lr: 3.146858e-04\n",
            "step 3153 | loss: 1.334352 | lr: 3.145288e-04\n",
            "step 3154 | loss: 1.218913 | lr: 3.143717e-04\n",
            "step 3155 | loss: 1.311806 | lr: 3.142146e-04\n",
            "step 3156 | loss: 1.279717 | lr: 3.140575e-04\n",
            "step 3157 | loss: 1.167905 | lr: 3.139004e-04\n",
            "step 3158 | loss: 1.432058 | lr: 3.137434e-04\n",
            "step 3159 | loss: 1.204197 | lr: 3.135863e-04\n",
            "step 3160 | loss: 1.117256 | lr: 3.134292e-04\n",
            "step 3161 | loss: 1.222567 | lr: 3.132721e-04\n",
            "step 3162 | loss: 1.133649 | lr: 3.131151e-04\n",
            "step 3163 | loss: 1.214478 | lr: 3.129580e-04\n",
            "step 3164 | loss: 1.172823 | lr: 3.128009e-04\n",
            "step 3165 | loss: 1.322077 | lr: 3.126438e-04\n",
            "step 3166 | loss: 1.344490 | lr: 3.124868e-04\n",
            "step 3167 | loss: 1.115530 | lr: 3.123297e-04\n",
            "step 3168 | loss: 1.193882 | lr: 3.121726e-04\n",
            "step 3169 | loss: 1.187579 | lr: 3.120155e-04\n",
            "step 3170 | loss: 1.239920 | lr: 3.118585e-04\n",
            "step 3171 | loss: 1.342135 | lr: 3.117014e-04\n",
            "step 3172 | loss: 1.292756 | lr: 3.115443e-04\n",
            "step 3173 | loss: 1.251628 | lr: 3.113873e-04\n",
            "step 3174 | loss: 1.270005 | lr: 3.112302e-04\n",
            "step 3175 | loss: 1.209023 | lr: 3.110731e-04\n",
            "step 3176 | loss: 1.063891 | lr: 3.109161e-04\n",
            "step 3177 | loss: 1.114255 | lr: 3.107590e-04\n",
            "step 3178 | loss: 1.024022 | lr: 3.106019e-04\n",
            "step 3179 | loss: 1.326297 | lr: 3.104449e-04\n",
            "step 3180 | loss: 1.284089 | lr: 3.102878e-04\n",
            "step 3181 | loss: 1.232092 | lr: 3.101308e-04\n",
            "step 3182 | loss: 1.498802 | lr: 3.099737e-04\n",
            "step 3183 | loss: 1.483625 | lr: 3.098167e-04\n",
            "step 3184 | loss: 1.333969 | lr: 3.096596e-04\n",
            "step 3185 | loss: 1.353115 | lr: 3.095026e-04\n",
            "step 3186 | loss: 1.384337 | lr: 3.093455e-04\n",
            "step 3187 | loss: 1.261536 | lr: 3.091885e-04\n",
            "step 3188 | loss: 1.271110 | lr: 3.090314e-04\n",
            "step 3189 | loss: 1.227017 | lr: 3.088744e-04\n",
            "step 3190 | loss: 1.257709 | lr: 3.087173e-04\n",
            "step 3191 | loss: 1.211239 | lr: 3.085603e-04\n",
            "step 3192 | loss: 1.195499 | lr: 3.084032e-04\n",
            "step 3193 | loss: 1.232103 | lr: 3.082462e-04\n",
            "step 3194 | loss: 1.262119 | lr: 3.080892e-04\n",
            "step 3195 | loss: 1.197458 | lr: 3.079321e-04\n",
            "step 3196 | loss: 1.143983 | lr: 3.077751e-04\n",
            "step 3197 | loss: 0.957183 | lr: 3.076181e-04\n",
            "step 3198 | loss: 1.265887 | lr: 3.074611e-04\n",
            "step 3199 | loss: 1.123458 | lr: 3.073040e-04\n",
            "step 3200 | loss: 1.347209 | lr: 3.071470e-04\n",
            "step 3201 | loss: 1.336639 | lr: 3.069900e-04\n",
            "step 3202 | loss: 1.262579 | lr: 3.068330e-04\n",
            "step 3203 | loss: 1.248342 | lr: 3.066760e-04\n",
            "step 3204 | loss: 1.194665 | lr: 3.065190e-04\n",
            "step 3205 | loss: 1.235332 | lr: 3.063619e-04\n",
            "step 3206 | loss: 1.250265 | lr: 3.062049e-04\n",
            "step 3207 | loss: 1.034617 | lr: 3.060479e-04\n",
            "step 3208 | loss: 1.234827 | lr: 3.058909e-04\n",
            "step 3209 | loss: 0.998763 | lr: 3.057339e-04\n",
            "step 3210 | loss: 1.403252 | lr: 3.055769e-04\n",
            "step 3211 | loss: 1.222649 | lr: 3.054199e-04\n",
            "step 3212 | loss: 1.189452 | lr: 3.052630e-04\n",
            "step 3213 | loss: 1.146632 | lr: 3.051060e-04\n",
            "step 3214 | loss: 1.135898 | lr: 3.049490e-04\n",
            "step 3215 | loss: 1.023285 | lr: 3.047920e-04\n",
            "step 3216 | loss: 1.159286 | lr: 3.046350e-04\n",
            "step 3217 | loss: 1.334098 | lr: 3.044781e-04\n",
            "step 3218 | loss: 1.197743 | lr: 3.043211e-04\n",
            "step 3219 | loss: 1.274144 | lr: 3.041641e-04\n",
            "step 3220 | loss: 1.279077 | lr: 3.040072e-04\n",
            "step 3221 | loss: 1.136687 | lr: 3.038502e-04\n",
            "step 3222 | loss: 1.296237 | lr: 3.036932e-04\n",
            "step 3223 | loss: 1.322278 | lr: 3.035363e-04\n",
            "step 3224 | loss: 1.288044 | lr: 3.033793e-04\n",
            "step 3225 | loss: 1.123520 | lr: 3.032224e-04\n",
            "step 3226 | loss: 1.258830 | lr: 3.030654e-04\n",
            "step 3227 | loss: 1.178123 | lr: 3.029085e-04\n",
            "step 3228 | loss: 1.220363 | lr: 3.027516e-04\n",
            "step 3229 | loss: 0.934553 | lr: 3.025946e-04\n",
            "step 3230 | loss: 1.272482 | lr: 3.024377e-04\n",
            "step 3231 | loss: 1.177655 | lr: 3.022808e-04\n",
            "step 3232 | loss: 1.143051 | lr: 3.021239e-04\n",
            "step 3233 | loss: 1.038606 | lr: 3.019669e-04\n",
            "step 3234 | loss: 1.052490 | lr: 3.018100e-04\n",
            "step 3235 | loss: 1.042895 | lr: 3.016531e-04\n",
            "step 3236 | loss: 1.214243 | lr: 3.014962e-04\n",
            "step 3237 | loss: 0.999721 | lr: 3.013393e-04\n",
            "step 3238 | loss: 1.093885 | lr: 3.011824e-04\n",
            "step 3239 | loss: 1.481026 | lr: 3.010255e-04\n",
            "step 3240 | loss: 1.385423 | lr: 3.008686e-04\n",
            "step 3241 | loss: 1.226540 | lr: 3.007117e-04\n",
            "step 3242 | loss: 1.257957 | lr: 3.005549e-04\n",
            "step 3243 | loss: 1.409839 | lr: 3.003980e-04\n",
            "step 3244 | loss: 1.197692 | lr: 3.002411e-04\n",
            "step 3245 | loss: 1.362725 | lr: 3.000843e-04\n",
            "step 3246 | loss: 1.295545 | lr: 2.999274e-04\n",
            "step 3247 | loss: 1.477203 | lr: 2.997705e-04\n",
            "step 3248 | loss: 1.478503 | lr: 2.996137e-04\n",
            "step 3249 | loss: 1.309342 | lr: 2.994568e-04\n",
            "step 3250 | loss: 1.346635 | lr: 2.993000e-04\n",
            "step 3251 | loss: 1.233390 | lr: 2.991431e-04\n",
            "step 3252 | loss: 1.384290 | lr: 2.989863e-04\n",
            "step 3253 | loss: 1.250100 | lr: 2.988295e-04\n",
            "step 3254 | loss: 1.339277 | lr: 2.986727e-04\n",
            "step 3255 | loss: 1.486813 | lr: 2.985158e-04\n",
            "step 3256 | loss: 1.311914 | lr: 2.983590e-04\n",
            "step 3257 | loss: 1.285671 | lr: 2.982022e-04\n",
            "step 3258 | loss: 1.272779 | lr: 2.980454e-04\n",
            "step 3259 | loss: 1.088112 | lr: 2.978886e-04\n",
            "step 3260 | loss: 1.330249 | lr: 2.977318e-04\n",
            "step 3261 | loss: 1.209618 | lr: 2.975750e-04\n",
            "step 3262 | loss: 1.033497 | lr: 2.974183e-04\n",
            "step 3263 | loss: 1.103244 | lr: 2.972615e-04\n",
            "step 3264 | loss: 1.148107 | lr: 2.971047e-04\n",
            "step 3265 | loss: 1.100314 | lr: 2.969479e-04\n",
            "step 3266 | loss: 1.073189 | lr: 2.967912e-04\n",
            "step 3267 | loss: 1.268151 | lr: 2.966344e-04\n",
            "step 3268 | loss: 1.005139 | lr: 2.964777e-04\n",
            "step 3269 | loss: 1.164224 | lr: 2.963209e-04\n",
            "step 3270 | loss: 1.010750 | lr: 2.961642e-04\n",
            "step 3271 | loss: 1.128121 | lr: 2.960075e-04\n",
            "step 3272 | loss: 0.992111 | lr: 2.958507e-04\n",
            "step 3273 | loss: 1.037143 | lr: 2.956940e-04\n",
            "step 3274 | loss: 1.074509 | lr: 2.955373e-04\n",
            "step 3275 | loss: 1.070490 | lr: 2.953806e-04\n",
            "step 3276 | loss: 1.022739 | lr: 2.952239e-04\n",
            "step 3277 | loss: 1.327494 | lr: 2.950672e-04\n",
            "step 3278 | loss: 1.271021 | lr: 2.949105e-04\n",
            "step 3279 | loss: 1.374096 | lr: 2.947538e-04\n",
            "step 3280 | loss: 1.285094 | lr: 2.945971e-04\n",
            "step 3281 | loss: 1.178841 | lr: 2.944404e-04\n",
            "step 3282 | loss: 1.151304 | lr: 2.942838e-04\n",
            "step 3283 | loss: 0.982385 | lr: 2.941271e-04\n",
            "step 3284 | loss: 1.145344 | lr: 2.939705e-04\n",
            "step 3285 | loss: 1.129756 | lr: 2.938138e-04\n",
            "step 3286 | loss: 1.100777 | lr: 2.936572e-04\n",
            "step 3287 | loss: 1.052731 | lr: 2.935005e-04\n",
            "step 3288 | loss: 1.111908 | lr: 2.933439e-04\n",
            "step 3289 | loss: 1.102255 | lr: 2.931873e-04\n",
            "step 3290 | loss: 1.025489 | lr: 2.930307e-04\n",
            "step 3291 | loss: 1.056769 | lr: 2.928741e-04\n",
            "step 3292 | loss: 0.987834 | lr: 2.927175e-04\n",
            "step 3293 | loss: 0.747331 | lr: 2.925609e-04\n",
            "step 3294 | loss: 1.040628 | lr: 2.924043e-04\n",
            "step 3295 | loss: 1.234088 | lr: 2.922477e-04\n",
            "step 3296 | loss: 1.168763 | lr: 2.920911e-04\n",
            "step 3297 | loss: 1.225323 | lr: 2.919345e-04\n",
            "step 3298 | loss: 1.016748 | lr: 2.917780e-04\n",
            "step 3299 | loss: 1.082082 | lr: 2.916214e-04\n",
            "step 3300 | loss: 1.498686 | lr: 2.914649e-04\n",
            "step 3301 | loss: 1.299844 | lr: 2.913083e-04\n",
            "step 3302 | loss: 1.265534 | lr: 2.911518e-04\n",
            "step 3303 | loss: 1.206914 | lr: 2.909953e-04\n",
            "step 3304 | loss: 1.270034 | lr: 2.908388e-04\n",
            "step 3305 | loss: 1.236521 | lr: 2.906823e-04\n",
            "step 3306 | loss: 1.163340 | lr: 2.905258e-04\n",
            "step 3307 | loss: 1.288444 | lr: 2.903693e-04\n",
            "step 3308 | loss: 1.267097 | lr: 2.902128e-04\n",
            "step 3309 | loss: 1.121538 | lr: 2.900563e-04\n",
            "step 3310 | loss: 1.101085 | lr: 2.898998e-04\n",
            "step 3311 | loss: 0.898938 | lr: 2.897434e-04\n",
            "step 3312 | loss: 1.046023 | lr: 2.895869e-04\n",
            "step 3313 | loss: 0.932913 | lr: 2.894304e-04\n",
            "step 3314 | loss: 0.993878 | lr: 2.892740e-04\n",
            "step 3315 | loss: 1.032593 | lr: 2.891176e-04\n",
            "step 3316 | loss: 0.990977 | lr: 2.889611e-04\n",
            "step 3317 | loss: 0.886050 | lr: 2.888047e-04\n",
            "step 3318 | loss: 1.123333 | lr: 2.886483e-04\n",
            "step 3319 | loss: 0.983021 | lr: 2.884919e-04\n",
            "step 3320 | loss: 1.061563 | lr: 2.883355e-04\n",
            "step 3321 | loss: 1.044388 | lr: 2.881791e-04\n",
            "step 3322 | loss: 0.940211 | lr: 2.880228e-04\n",
            "step 3323 | loss: 1.199150 | lr: 2.878664e-04\n",
            "step 3324 | loss: 0.975169 | lr: 2.877100e-04\n",
            "step 3325 | loss: 0.868808 | lr: 2.875537e-04\n",
            "step 3326 | loss: 0.994449 | lr: 2.873973e-04\n",
            "step 3327 | loss: 0.914933 | lr: 2.872410e-04\n",
            "step 3328 | loss: 0.976473 | lr: 2.870847e-04\n",
            "step 3329 | loss: 0.947097 | lr: 2.869283e-04\n",
            "step 3330 | loss: 1.011616 | lr: 2.867720e-04\n",
            "step 3331 | loss: 1.042359 | lr: 2.866157e-04\n",
            "step 3332 | loss: 0.878754 | lr: 2.864594e-04\n",
            "step 3333 | loss: 0.919710 | lr: 2.863031e-04\n",
            "step 3334 | loss: 0.935132 | lr: 2.861469e-04\n",
            "step 3335 | loss: 0.969920 | lr: 2.859906e-04\n",
            "step 3336 | loss: 1.046917 | lr: 2.858343e-04\n",
            "step 3337 | loss: 1.041709 | lr: 2.856781e-04\n",
            "step 3338 | loss: 0.997345 | lr: 2.855218e-04\n",
            "step 3339 | loss: 1.008295 | lr: 2.853656e-04\n",
            "step 3340 | loss: 0.984844 | lr: 2.852094e-04\n",
            "step 3341 | loss: 0.851173 | lr: 2.850532e-04\n",
            "step 3342 | loss: 0.899239 | lr: 2.848970e-04\n",
            "step 3343 | loss: 0.830357 | lr: 2.847408e-04\n",
            "step 3344 | loss: 1.075788 | lr: 2.845846e-04\n",
            "step 3345 | loss: 1.046487 | lr: 2.844284e-04\n",
            "step 3346 | loss: 1.020403 | lr: 2.842722e-04\n",
            "step 3347 | loss: 1.224804 | lr: 2.841161e-04\n",
            "step 3348 | loss: 1.162861 | lr: 2.839599e-04\n",
            "step 3349 | loss: 1.071001 | lr: 2.838038e-04\n",
            "step 3350 | loss: 1.095598 | lr: 2.836477e-04\n",
            "step 3351 | loss: 1.167352 | lr: 2.834915e-04\n",
            "step 3352 | loss: 1.087861 | lr: 2.833354e-04\n",
            "step 3353 | loss: 1.140903 | lr: 2.831793e-04\n",
            "step 3354 | loss: 1.041040 | lr: 2.830232e-04\n",
            "step 3355 | loss: 1.075766 | lr: 2.828671e-04\n",
            "step 3356 | loss: 1.008934 | lr: 2.827111e-04\n",
            "step 3357 | loss: 0.997111 | lr: 2.825550e-04\n",
            "step 3358 | loss: 0.986016 | lr: 2.823990e-04\n",
            "step 3359 | loss: 1.012009 | lr: 2.822429e-04\n",
            "step 3360 | loss: 0.952676 | lr: 2.820869e-04\n",
            "step 3361 | loss: 0.925919 | lr: 2.819309e-04\n",
            "step 3362 | loss: 0.754819 | lr: 2.817748e-04\n",
            "step 3363 | loss: 1.033438 | lr: 2.816188e-04\n",
            "step 3364 | loss: 0.904578 | lr: 2.814628e-04\n",
            "step 3365 | loss: 1.088504 | lr: 2.813069e-04\n",
            "step 3366 | loss: 1.057760 | lr: 2.811509e-04\n",
            "step 3367 | loss: 1.028135 | lr: 2.809949e-04\n",
            "step 3368 | loss: 0.982940 | lr: 2.808390e-04\n",
            "step 3369 | loss: 0.956953 | lr: 2.806830e-04\n",
            "step 3370 | loss: 1.003845 | lr: 2.805271e-04\n",
            "step 3371 | loss: 0.978068 | lr: 2.803712e-04\n",
            "step 3372 | loss: 0.832796 | lr: 2.802153e-04\n",
            "step 3373 | loss: 0.967018 | lr: 2.800594e-04\n",
            "step 3374 | loss: 0.798045 | lr: 2.799035e-04\n",
            "step 3375 | loss: 1.097115 | lr: 2.797476e-04\n",
            "step 3376 | loss: 0.982054 | lr: 2.795917e-04\n",
            "step 3377 | loss: 0.957604 | lr: 2.794359e-04\n",
            "step 3378 | loss: 0.898131 | lr: 2.792800e-04\n",
            "step 3379 | loss: 0.895382 | lr: 2.791242e-04\n",
            "step 3380 | loss: 0.833062 | lr: 2.789684e-04\n",
            "step 3381 | loss: 0.925954 | lr: 2.788126e-04\n",
            "step 3382 | loss: 1.075599 | lr: 2.786568e-04\n",
            "step 3383 | loss: 0.948342 | lr: 2.785010e-04\n",
            "step 3384 | loss: 1.056487 | lr: 2.783452e-04\n",
            "step 3385 | loss: 1.042342 | lr: 2.781894e-04\n",
            "step 3386 | loss: 0.909518 | lr: 2.780337e-04\n",
            "step 3387 | loss: 1.039662 | lr: 2.778779e-04\n",
            "step 3388 | loss: 1.103348 | lr: 2.777222e-04\n",
            "step 3389 | loss: 1.024221 | lr: 2.775664e-04\n",
            "step 3390 | loss: 0.884654 | lr: 2.774107e-04\n",
            "step 3391 | loss: 1.000697 | lr: 2.772550e-04\n",
            "step 3392 | loss: 0.897881 | lr: 2.770993e-04\n",
            "step 3393 | loss: 0.976699 | lr: 2.769437e-04\n",
            "step 3394 | loss: 0.734961 | lr: 2.767880e-04\n",
            "step 3395 | loss: 0.994890 | lr: 2.766323e-04\n",
            "step 3396 | loss: 0.971775 | lr: 2.764767e-04\n",
            "step 3397 | loss: 1.016082 | lr: 2.763211e-04\n",
            "step 3398 | loss: 0.865175 | lr: 2.761654e-04\n",
            "step 3399 | loss: 0.843563 | lr: 2.760098e-04\n",
            "step 3400 | loss: 0.783140 | lr: 2.758542e-04\n",
            "step 3401 | loss: 0.922503 | lr: 2.756987e-04\n",
            "step 3402 | loss: 0.735198 | lr: 2.755431e-04\n",
            "step 3403 | loss: 0.832006 | lr: 2.753875e-04\n",
            "step 3404 | loss: 1.171385 | lr: 2.752320e-04\n",
            "step 3405 | loss: 1.100646 | lr: 2.750764e-04\n",
            "step 3406 | loss: 0.946274 | lr: 2.749209e-04\n",
            "step 3407 | loss: 1.009023 | lr: 2.747654e-04\n",
            "step 3408 | loss: 1.139713 | lr: 2.746099e-04\n",
            "step 3409 | loss: 0.987328 | lr: 2.744544e-04\n",
            "step 3410 | loss: 1.126293 | lr: 2.742989e-04\n",
            "step 3411 | loss: 1.066112 | lr: 2.741435e-04\n",
            "step 3412 | loss: 1.223026 | lr: 2.739880e-04\n",
            "step 3413 | loss: 1.177313 | lr: 2.738326e-04\n",
            "step 3414 | loss: 1.021503 | lr: 2.736772e-04\n",
            "step 3415 | loss: 1.061754 | lr: 2.735217e-04\n",
            "step 3416 | loss: 0.987444 | lr: 2.733663e-04\n",
            "step 3417 | loss: 1.159243 | lr: 2.732109e-04\n",
            "step 3418 | loss: 1.035696 | lr: 2.730556e-04\n",
            "step 3419 | loss: 1.058014 | lr: 2.729002e-04\n",
            "step 3420 | loss: 1.267473 | lr: 2.727449e-04\n",
            "step 3421 | loss: 1.148241 | lr: 2.725895e-04\n",
            "step 3422 | loss: 1.094079 | lr: 2.724342e-04\n",
            "step 3423 | loss: 1.111453 | lr: 2.722789e-04\n",
            "step 3424 | loss: 0.927636 | lr: 2.721236e-04\n",
            "step 3425 | loss: 1.106775 | lr: 2.719683e-04\n",
            "step 3426 | loss: 1.001557 | lr: 2.718130e-04\n",
            "step 3427 | loss: 0.831266 | lr: 2.716578e-04\n",
            "step 3428 | loss: 0.900923 | lr: 2.715025e-04\n",
            "step 3429 | loss: 0.883548 | lr: 2.713473e-04\n",
            "step 3430 | loss: 0.865426 | lr: 2.711921e-04\n",
            "step 3431 | loss: 0.825508 | lr: 2.710369e-04\n",
            "step 3432 | loss: 1.009792 | lr: 2.708817e-04\n",
            "step 3433 | loss: 0.787460 | lr: 2.707265e-04\n",
            "step 3434 | loss: 0.931070 | lr: 2.705713e-04\n",
            "step 3435 | loss: 0.860114 | lr: 2.704162e-04\n",
            "step 3436 | loss: 0.890963 | lr: 2.702610e-04\n",
            "step 3437 | loss: 0.807554 | lr: 2.701059e-04\n",
            "step 3438 | loss: 0.837381 | lr: 2.699508e-04\n",
            "step 3439 | loss: 0.870124 | lr: 2.697957e-04\n",
            "step 3440 | loss: 0.820454 | lr: 2.696406e-04\n",
            "step 3441 | loss: 0.812238 | lr: 2.694855e-04\n",
            "step 3442 | loss: 1.044997 | lr: 2.693305e-04\n",
            "step 3443 | loss: 1.017888 | lr: 2.691754e-04\n",
            "step 3444 | loss: 1.120006 | lr: 2.690204e-04\n",
            "step 3445 | loss: 1.020430 | lr: 2.688654e-04\n",
            "step 3446 | loss: 0.987523 | lr: 2.687104e-04\n",
            "step 3447 | loss: 0.948290 | lr: 2.685554e-04\n",
            "step 3448 | loss: 0.805973 | lr: 2.684004e-04\n",
            "step 3449 | loss: 0.931356 | lr: 2.682455e-04\n",
            "step 3450 | loss: 0.924951 | lr: 2.680905e-04\n",
            "step 3451 | loss: 0.883086 | lr: 2.679356e-04\n",
            "step 3452 | loss: 0.860500 | lr: 2.677807e-04\n",
            "step 3453 | loss: 0.883152 | lr: 2.676258e-04\n",
            "step 3454 | loss: 0.909675 | lr: 2.674709e-04\n",
            "step 3455 | loss: 0.830743 | lr: 2.673160e-04\n",
            "step 3456 | loss: 0.816038 | lr: 2.671612e-04\n",
            "step 3457 | loss: 0.792740 | lr: 2.670063e-04\n",
            "step 3458 | loss: 0.564989 | lr: 2.668515e-04\n",
            "step 3459 | loss: 0.827574 | lr: 2.666967e-04\n",
            "step 3460 | loss: 1.036217 | lr: 2.665419e-04\n",
            "step 3461 | loss: 0.960809 | lr: 2.663871e-04\n",
            "step 3462 | loss: 0.955614 | lr: 2.662323e-04\n",
            "step 3463 | loss: 0.814840 | lr: 2.660776e-04\n",
            "step 3464 | loss: 0.874504 | lr: 2.659228e-04\n",
            "step 3465 | loss: 1.250400 | lr: 2.657681e-04\n",
            "step 3466 | loss: 1.094611 | lr: 2.656134e-04\n",
            "step 3467 | loss: 1.024873 | lr: 2.654587e-04\n",
            "step 3468 | loss: 0.984615 | lr: 2.653040e-04\n",
            "step 3469 | loss: 1.060316 | lr: 2.651494e-04\n",
            "step 3470 | loss: 1.006823 | lr: 2.649947e-04\n",
            "step 3471 | loss: 0.984609 | lr: 2.648401e-04\n",
            "step 3472 | loss: 1.094471 | lr: 2.646855e-04\n",
            "step 3473 | loss: 1.045246 | lr: 2.645308e-04\n",
            "step 3474 | loss: 0.913058 | lr: 2.643763e-04\n",
            "step 3475 | loss: 0.902765 | lr: 2.642217e-04\n",
            "step 3476 | loss: 0.774816 | lr: 2.640671e-04\n",
            "step 3477 | loss: 0.824283 | lr: 2.639126e-04\n",
            "step 3478 | loss: 0.770437 | lr: 2.637581e-04\n",
            "step 3479 | loss: 0.807114 | lr: 2.636035e-04\n",
            "step 3480 | loss: 0.827260 | lr: 2.634490e-04\n",
            "step 3481 | loss: 0.797358 | lr: 2.632946e-04\n",
            "step 3482 | loss: 0.711116 | lr: 2.631401e-04\n",
            "step 3483 | loss: 0.918336 | lr: 2.629857e-04\n",
            "step 3484 | loss: 0.814028 | lr: 2.628312e-04\n",
            "step 3485 | loss: 0.913123 | lr: 2.626768e-04\n",
            "step 3486 | loss: 0.859504 | lr: 2.625224e-04\n",
            "step 3487 | loss: 0.760211 | lr: 2.623680e-04\n",
            "step 3488 | loss: 0.956539 | lr: 2.622136e-04\n",
            "step 3489 | loss: 0.768889 | lr: 2.620593e-04\n",
            "step 3490 | loss: 0.676054 | lr: 2.619050e-04\n",
            "step 3491 | loss: 0.799443 | lr: 2.617506e-04\n",
            "step 3492 | loss: 0.718183 | lr: 2.615963e-04\n",
            "step 3493 | loss: 0.756911 | lr: 2.614420e-04\n",
            "step 3494 | loss: 0.785756 | lr: 2.612878e-04\n",
            "step 3495 | loss: 0.804668 | lr: 2.611335e-04\n",
            "step 3496 | loss: 0.843313 | lr: 2.609793e-04\n",
            "step 3497 | loss: 0.739664 | lr: 2.608250e-04\n",
            "step 3498 | loss: 0.770915 | lr: 2.606708e-04\n",
            "step 3499 | loss: 0.760763 | lr: 2.605166e-04\n",
            "step 3500 | loss: 0.782176 | lr: 2.603625e-04\n",
            "step 3501 | loss: 0.812821 | lr: 2.602083e-04\n",
            "step 3502 | loss: 0.812176 | lr: 2.600542e-04\n",
            "step 3503 | loss: 0.778144 | lr: 2.599000e-04\n",
            "step 3504 | loss: 0.793825 | lr: 2.597459e-04\n",
            "step 3505 | loss: 0.790564 | lr: 2.595918e-04\n",
            "step 3506 | loss: 0.667645 | lr: 2.594378e-04\n",
            "step 3507 | loss: 0.692015 | lr: 2.592837e-04\n",
            "step 3508 | loss: 0.611018 | lr: 2.591297e-04\n",
            "step 3509 | loss: 0.846478 | lr: 2.589757e-04\n",
            "step 3510 | loss: 0.805056 | lr: 2.588216e-04\n",
            "step 3511 | loss: 0.806198 | lr: 2.586677e-04\n",
            "step 3512 | loss: 0.975605 | lr: 2.585137e-04\n",
            "step 3513 | loss: 0.919732 | lr: 2.583597e-04\n",
            "step 3514 | loss: 0.799091 | lr: 2.582058e-04\n",
            "step 3515 | loss: 0.846702 | lr: 2.580519e-04\n",
            "step 3516 | loss: 0.887285 | lr: 2.578980e-04\n",
            "step 3517 | loss: 0.843103 | lr: 2.577441e-04\n",
            "step 3518 | loss: 0.932984 | lr: 2.575902e-04\n",
            "step 3519 | loss: 0.803351 | lr: 2.574364e-04\n",
            "step 3520 | loss: 0.859794 | lr: 2.572825e-04\n",
            "step 3521 | loss: 0.863934 | lr: 2.571287e-04\n",
            "step 3522 | loss: 0.806350 | lr: 2.569749e-04\n",
            "step 3523 | loss: 0.826006 | lr: 2.568211e-04\n",
            "step 3524 | loss: 0.777190 | lr: 2.566674e-04\n",
            "step 3525 | loss: 0.733193 | lr: 2.565136e-04\n",
            "step 3526 | loss: 0.733888 | lr: 2.563599e-04\n",
            "step 3527 | loss: 0.584656 | lr: 2.562062e-04\n",
            "step 3528 | loss: 0.812880 | lr: 2.560525e-04\n",
            "step 3529 | loss: 0.738421 | lr: 2.558988e-04\n",
            "step 3530 | loss: 0.916571 | lr: 2.557452e-04\n",
            "step 3531 | loss: 0.861578 | lr: 2.555915e-04\n",
            "step 3532 | loss: 0.814728 | lr: 2.554379e-04\n",
            "step 3533 | loss: 0.749203 | lr: 2.552843e-04\n",
            "step 3534 | loss: 0.730712 | lr: 2.551307e-04\n",
            "step 3535 | loss: 0.772573 | lr: 2.549772e-04\n",
            "step 3536 | loss: 0.724111 | lr: 2.548236e-04\n",
            "step 3537 | loss: 0.604869 | lr: 2.546701e-04\n",
            "step 3538 | loss: 0.716655 | lr: 2.545166e-04\n",
            "step 3539 | loss: 0.609006 | lr: 2.543631e-04\n",
            "step 3540 | loss: 0.880796 | lr: 2.542096e-04\n",
            "step 3541 | loss: 0.779586 | lr: 2.540561e-04\n",
            "step 3542 | loss: 0.759675 | lr: 2.539027e-04\n",
            "step 3543 | loss: 0.715656 | lr: 2.537493e-04\n",
            "step 3544 | loss: 0.686827 | lr: 2.535959e-04\n",
            "step 3545 | loss: 0.641216 | lr: 2.534425e-04\n",
            "step 3546 | loss: 0.749430 | lr: 2.532892e-04\n",
            "step 3547 | loss: 0.852522 | lr: 2.531358e-04\n",
            "step 3548 | loss: 0.773987 | lr: 2.529825e-04\n",
            "step 3549 | loss: 0.855728 | lr: 2.528292e-04\n",
            "step 3550 | loss: 0.879895 | lr: 2.526759e-04\n",
            "step 3551 | loss: 0.756507 | lr: 2.525226e-04\n",
            "step 3552 | loss: 0.845832 | lr: 2.523694e-04\n",
            "step 3553 | loss: 0.881257 | lr: 2.522161e-04\n",
            "step 3554 | loss: 0.792033 | lr: 2.520629e-04\n",
            "step 3555 | loss: 0.656533 | lr: 2.519097e-04\n",
            "step 3556 | loss: 0.762552 | lr: 2.517566e-04\n",
            "step 3557 | loss: 0.684439 | lr: 2.516034e-04\n",
            "step 3558 | loss: 0.791598 | lr: 2.514503e-04\n",
            "step 3559 | loss: 0.597103 | lr: 2.512972e-04\n",
            "step 3560 | loss: 0.801688 | lr: 2.511441e-04\n",
            "step 3561 | loss: 0.737983 | lr: 2.509910e-04\n",
            "step 3562 | loss: 0.756524 | lr: 2.508379e-04\n",
            "step 3563 | loss: 0.642637 | lr: 2.506849e-04\n",
            "step 3564 | loss: 0.606580 | lr: 2.505319e-04\n",
            "step 3565 | loss: 0.565648 | lr: 2.503789e-04\n",
            "step 3566 | loss: 0.723902 | lr: 2.502259e-04\n",
            "step 3567 | loss: 0.569855 | lr: 2.500729e-04\n",
            "step 3568 | loss: 0.630367 | lr: 2.499200e-04\n",
            "step 3569 | loss: 0.900564 | lr: 2.497671e-04\n",
            "step 3570 | loss: 0.871760 | lr: 2.496142e-04\n",
            "step 3571 | loss: 0.708775 | lr: 2.494613e-04\n",
            "step 3572 | loss: 0.796105 | lr: 2.493084e-04\n",
            "step 3573 | loss: 0.878865 | lr: 2.491556e-04\n",
            "step 3574 | loss: 0.746036 | lr: 2.490028e-04\n",
            "step 3575 | loss: 0.929004 | lr: 2.488500e-04\n",
            "step 3576 | loss: 0.836538 | lr: 2.486972e-04\n",
            "step 3577 | loss: 0.973148 | lr: 2.485444e-04\n",
            "step 3578 | loss: 0.965351 | lr: 2.483917e-04\n",
            "step 3579 | loss: 0.821774 | lr: 2.482390e-04\n",
            "step 3580 | loss: 0.852734 | lr: 2.480863e-04\n",
            "step 3581 | loss: 0.773575 | lr: 2.479336e-04\n",
            "step 3582 | loss: 0.890704 | lr: 2.477809e-04\n",
            "step 3583 | loss: 0.807687 | lr: 2.476283e-04\n",
            "step 3584 | loss: 0.844276 | lr: 2.474757e-04\n",
            "step 3585 | loss: 0.994842 | lr: 2.473231e-04\n",
            "step 3586 | loss: 0.889911 | lr: 2.471705e-04\n",
            "step 3587 | loss: 0.886851 | lr: 2.470180e-04\n",
            "step 3588 | loss: 0.853137 | lr: 2.468654e-04\n",
            "step 3589 | loss: 0.763130 | lr: 2.467129e-04\n",
            "step 3590 | loss: 0.883755 | lr: 2.465604e-04\n",
            "step 3591 | loss: 0.747106 | lr: 2.464079e-04\n",
            "step 3592 | loss: 0.663248 | lr: 2.462555e-04\n",
            "step 3593 | loss: 0.741360 | lr: 2.461031e-04\n",
            "step 3594 | loss: 0.727490 | lr: 2.459507e-04\n",
            "step 3595 | loss: 0.724936 | lr: 2.457983e-04\n",
            "step 3596 | loss: 0.647550 | lr: 2.456459e-04\n",
            "step 3597 | loss: 0.822245 | lr: 2.454936e-04\n",
            "step 3598 | loss: 0.648479 | lr: 2.453412e-04\n",
            "step 3599 | loss: 0.708994 | lr: 2.451889e-04\n",
            "step 3600 | loss: 0.619126 | lr: 2.450366e-04\n",
            "step 3601 | loss: 0.701096 | lr: 2.448844e-04\n",
            "step 3602 | loss: 0.623795 | lr: 2.447321e-04\n",
            "step 3603 | loss: 0.629322 | lr: 2.445799e-04\n",
            "step 3604 | loss: 0.698199 | lr: 2.444277e-04\n",
            "step 3605 | loss: 0.670478 | lr: 2.442755e-04\n",
            "step 3606 | loss: 0.659380 | lr: 2.441234e-04\n",
            "step 3607 | loss: 0.859722 | lr: 2.439712e-04\n",
            "step 3608 | loss: 0.811841 | lr: 2.438191e-04\n",
            "step 3609 | loss: 0.926868 | lr: 2.436670e-04\n",
            "step 3610 | loss: 0.817177 | lr: 2.435150e-04\n",
            "step 3611 | loss: 0.748838 | lr: 2.433629e-04\n",
            "step 3612 | loss: 0.692939 | lr: 2.432109e-04\n",
            "step 3613 | loss: 0.598391 | lr: 2.430589e-04\n",
            "step 3614 | loss: 0.690498 | lr: 2.429069e-04\n",
            "step 3615 | loss: 0.660162 | lr: 2.427550e-04\n",
            "step 3616 | loss: 0.647381 | lr: 2.426030e-04\n",
            "step 3617 | loss: 0.645723 | lr: 2.424511e-04\n",
            "step 3618 | loss: 0.667087 | lr: 2.422992e-04\n",
            "step 3619 | loss: 0.715219 | lr: 2.421473e-04\n",
            "step 3620 | loss: 0.603412 | lr: 2.419955e-04\n",
            "step 3621 | loss: 0.641958 | lr: 2.418437e-04\n",
            "step 3622 | loss: 0.605605 | lr: 2.416919e-04\n",
            "step 3623 | loss: 0.440448 | lr: 2.415401e-04\n",
            "step 3624 | loss: 0.674227 | lr: 2.413883e-04\n",
            "step 3625 | loss: 0.765578 | lr: 2.412366e-04\n",
            "step 3626 | loss: 0.692445 | lr: 2.410849e-04\n",
            "step 3627 | loss: 0.689513 | lr: 2.409332e-04\n",
            "step 3628 | loss: 0.646349 | lr: 2.407815e-04\n",
            "step 3629 | loss: 0.713207 | lr: 2.406298e-04\n",
            "step 3630 | loss: 1.049484 | lr: 2.404782e-04\n",
            "step 3631 | loss: 0.857124 | lr: 2.403266e-04\n",
            "step 3632 | loss: 0.768929 | lr: 2.401750e-04\n",
            "step 3633 | loss: 0.731321 | lr: 2.400235e-04\n",
            "step 3634 | loss: 0.774531 | lr: 2.398719e-04\n",
            "step 3635 | loss: 0.768672 | lr: 2.397204e-04\n",
            "step 3636 | loss: 0.764645 | lr: 2.395689e-04\n",
            "step 3637 | loss: 0.878657 | lr: 2.394175e-04\n",
            "step 3638 | loss: 0.843824 | lr: 2.392660e-04\n",
            "step 3639 | loss: 0.694456 | lr: 2.391146e-04\n",
            "step 3640 | loss: 0.719248 | lr: 2.389632e-04\n",
            "step 3641 | loss: 0.621680 | lr: 2.388118e-04\n",
            "step 3642 | loss: 0.668257 | lr: 2.386605e-04\n",
            "step 3643 | loss: 0.616950 | lr: 2.385092e-04\n",
            "step 3644 | loss: 0.641363 | lr: 2.383579e-04\n",
            "step 3645 | loss: 0.611128 | lr: 2.382066e-04\n",
            "step 3646 | loss: 0.614020 | lr: 2.380553e-04\n",
            "step 3647 | loss: 0.519151 | lr: 2.379041e-04\n",
            "step 3648 | loss: 0.721085 | lr: 2.377529e-04\n",
            "step 3649 | loss: 0.629045 | lr: 2.376017e-04\n",
            "step 3650 | loss: 0.698471 | lr: 2.374505e-04\n",
            "step 3651 | loss: 0.670471 | lr: 2.372994e-04\n",
            "step 3652 | loss: 0.601210 | lr: 2.371483e-04\n",
            "step 3653 | loss: 0.795445 | lr: 2.369972e-04\n",
            "step 3654 | loss: 0.562026 | lr: 2.368461e-04\n",
            "step 3655 | loss: 0.496397 | lr: 2.366950e-04\n",
            "step 3656 | loss: 0.609803 | lr: 2.365440e-04\n",
            "step 3657 | loss: 0.529653 | lr: 2.363930e-04\n",
            "step 3658 | loss: 0.572159 | lr: 2.362420e-04\n",
            "step 3659 | loss: 0.581176 | lr: 2.360911e-04\n",
            "step 3660 | loss: 0.637214 | lr: 2.359402e-04\n",
            "step 3661 | loss: 0.670424 | lr: 2.357893e-04\n",
            "step 3662 | loss: 0.583674 | lr: 2.356384e-04\n",
            "step 3663 | loss: 0.562514 | lr: 2.354875e-04\n",
            "step 3664 | loss: 0.631842 | lr: 2.353367e-04\n",
            "step 3665 | loss: 0.638942 | lr: 2.351859e-04\n",
            "step 3666 | loss: 0.658705 | lr: 2.350351e-04\n",
            "step 3667 | loss: 0.628422 | lr: 2.348844e-04\n",
            "step 3668 | loss: 0.588083 | lr: 2.347336e-04\n",
            "step 3669 | loss: 0.657437 | lr: 2.345829e-04\n",
            "step 3670 | loss: 0.618553 | lr: 2.344322e-04\n",
            "step 3671 | loss: 0.530542 | lr: 2.342816e-04\n",
            "step 3672 | loss: 0.539558 | lr: 2.341309e-04\n",
            "step 3673 | loss: 0.469496 | lr: 2.339803e-04\n",
            "step 3674 | loss: 0.678351 | lr: 2.338297e-04\n",
            "step 3675 | loss: 0.653269 | lr: 2.336792e-04\n",
            "step 3676 | loss: 0.667296 | lr: 2.335286e-04\n",
            "step 3677 | loss: 0.823091 | lr: 2.333781e-04\n",
            "step 3678 | loss: 0.754474 | lr: 2.332276e-04\n",
            "step 3679 | loss: 0.603372 | lr: 2.330772e-04\n",
            "step 3680 | loss: 0.647654 | lr: 2.329267e-04\n",
            "step 3681 | loss: 0.655245 | lr: 2.327763e-04\n",
            "step 3682 | loss: 0.635699 | lr: 2.326259e-04\n",
            "step 3683 | loss: 0.696493 | lr: 2.324756e-04\n",
            "step 3684 | loss: 0.624814 | lr: 2.323252e-04\n",
            "step 3685 | loss: 0.675570 | lr: 2.321749e-04\n",
            "step 3686 | loss: 0.632867 | lr: 2.320246e-04\n",
            "step 3687 | loss: 0.651155 | lr: 2.318744e-04\n",
            "step 3688 | loss: 0.682722 | lr: 2.317241e-04\n",
            "step 3689 | loss: 0.656873 | lr: 2.315739e-04\n",
            "step 3690 | loss: 0.604546 | lr: 2.314237e-04\n",
            "step 3691 | loss: 0.570952 | lr: 2.312736e-04\n",
            "step 3692 | loss: 0.440878 | lr: 2.311234e-04\n",
            "step 3693 | loss: 0.651003 | lr: 2.309733e-04\n",
            "step 3694 | loss: 0.573273 | lr: 2.308232e-04\n",
            "step 3695 | loss: 0.705470 | lr: 2.306732e-04\n",
            "step 3696 | loss: 0.669483 | lr: 2.305232e-04\n",
            "step 3697 | loss: 0.644749 | lr: 2.303731e-04\n",
            "step 3698 | loss: 0.558237 | lr: 2.302232e-04\n",
            "step 3699 | loss: 0.571035 | lr: 2.300732e-04\n",
            "step 3700 | loss: 0.639604 | lr: 2.299233e-04\n",
            "step 3701 | loss: 0.552343 | lr: 2.297734e-04\n",
            "step 3702 | loss: 0.482844 | lr: 2.296235e-04\n",
            "step 3703 | loss: 0.575133 | lr: 2.294736e-04\n",
            "step 3704 | loss: 0.466369 | lr: 2.293238e-04\n",
            "step 3705 | loss: 0.648884 | lr: 2.291740e-04\n",
            "step 3706 | loss: 0.579890 | lr: 2.290242e-04\n",
            "step 3707 | loss: 0.579073 | lr: 2.288745e-04\n",
            "step 3708 | loss: 0.558808 | lr: 2.287248e-04\n",
            "step 3709 | loss: 0.541973 | lr: 2.285751e-04\n",
            "step 3710 | loss: 0.498334 | lr: 2.284254e-04\n",
            "step 3711 | loss: 0.577876 | lr: 2.282758e-04\n",
            "step 3712 | loss: 0.684691 | lr: 2.281261e-04\n",
            "step 3713 | loss: 0.614991 | lr: 2.279766e-04\n",
            "step 3714 | loss: 0.681323 | lr: 2.278270e-04\n",
            "step 3715 | loss: 0.741863 | lr: 2.276774e-04\n",
            "step 3716 | loss: 0.591567 | lr: 2.275279e-04\n",
            "step 3717 | loss: 0.651187 | lr: 2.273785e-04\n",
            "step 3718 | loss: 0.690458 | lr: 2.272290e-04\n",
            "step 3719 | loss: 0.646018 | lr: 2.270796e-04\n",
            "step 3720 | loss: 0.580957 | lr: 2.269302e-04\n",
            "step 3721 | loss: 0.645252 | lr: 2.267808e-04\n",
            "step 3722 | loss: 0.586127 | lr: 2.266314e-04\n",
            "step 3723 | loss: 0.611739 | lr: 2.264821e-04\n",
            "step 3724 | loss: 0.461424 | lr: 2.263328e-04\n",
            "step 3725 | loss: 0.612960 | lr: 2.261835e-04\n",
            "step 3726 | loss: 0.557466 | lr: 2.260343e-04\n",
            "step 3727 | loss: 0.560852 | lr: 2.258851e-04\n",
            "step 3728 | loss: 0.513377 | lr: 2.257359e-04\n",
            "step 3729 | loss: 0.480373 | lr: 2.255867e-04\n",
            "step 3730 | loss: 0.427908 | lr: 2.254376e-04\n",
            "step 3731 | loss: 0.521735 | lr: 2.252885e-04\n",
            "step 3732 | loss: 0.410270 | lr: 2.251394e-04\n",
            "step 3733 | loss: 0.454522 | lr: 2.249903e-04\n",
            "step 3734 | loss: 0.693838 | lr: 2.248413e-04\n",
            "step 3735 | loss: 0.673067 | lr: 2.246923e-04\n",
            "step 3736 | loss: 0.547236 | lr: 2.245433e-04\n",
            "step 3737 | loss: 0.602588 | lr: 2.243944e-04\n",
            "step 3738 | loss: 0.727032 | lr: 2.242455e-04\n",
            "step 3739 | loss: 0.567398 | lr: 2.240966e-04\n",
            "step 3740 | loss: 0.691657 | lr: 2.239477e-04\n",
            "step 3741 | loss: 0.608023 | lr: 2.237989e-04\n",
            "step 3742 | loss: 0.707862 | lr: 2.236501e-04\n",
            "step 3743 | loss: 0.661924 | lr: 2.235013e-04\n",
            "step 3744 | loss: 0.618724 | lr: 2.233526e-04\n",
            "step 3745 | loss: 0.596408 | lr: 2.232038e-04\n",
            "step 3746 | loss: 0.578147 | lr: 2.230552e-04\n",
            "step 3747 | loss: 0.663991 | lr: 2.229065e-04\n",
            "step 3748 | loss: 0.628608 | lr: 2.227578e-04\n",
            "step 3749 | loss: 0.636197 | lr: 2.226092e-04\n",
            "step 3750 | loss: 0.755078 | lr: 2.224607e-04\n",
            "step 3751 | loss: 0.708724 | lr: 2.223121e-04\n",
            "step 3752 | loss: 0.706736 | lr: 2.221636e-04\n",
            "step 3753 | loss: 0.650436 | lr: 2.220151e-04\n",
            "step 3754 | loss: 0.605288 | lr: 2.218666e-04\n",
            "step 3755 | loss: 0.740348 | lr: 2.217182e-04\n",
            "step 3756 | loss: 0.566099 | lr: 2.215697e-04\n",
            "step 3757 | loss: 0.521307 | lr: 2.214214e-04\n",
            "step 3758 | loss: 0.567489 | lr: 2.212730e-04\n",
            "step 3759 | loss: 0.500370 | lr: 2.211247e-04\n",
            "step 3760 | loss: 0.534199 | lr: 2.209764e-04\n",
            "step 3761 | loss: 0.487529 | lr: 2.208281e-04\n",
            "step 3762 | loss: 0.657462 | lr: 2.206799e-04\n",
            "step 3763 | loss: 0.477818 | lr: 2.205317e-04\n",
            "step 3764 | loss: 0.520672 | lr: 2.203835e-04\n",
            "step 3765 | loss: 0.459889 | lr: 2.202353e-04\n",
            "step 3766 | loss: 0.521781 | lr: 2.200872e-04\n",
            "step 3767 | loss: 0.450949 | lr: 2.199391e-04\n",
            "step 3768 | loss: 0.440238 | lr: 2.197910e-04\n",
            "step 3769 | loss: 0.527387 | lr: 2.196430e-04\n",
            "step 3770 | loss: 0.531494 | lr: 2.194950e-04\n",
            "step 3771 | loss: 0.479991 | lr: 2.193470e-04\n",
            "step 3772 | loss: 0.647390 | lr: 2.191990e-04\n",
            "step 3773 | loss: 0.617117 | lr: 2.190511e-04\n",
            "step 3774 | loss: 0.734619 | lr: 2.189032e-04\n",
            "step 3775 | loss: 0.679719 | lr: 2.187553e-04\n",
            "step 3776 | loss: 0.649974 | lr: 2.186075e-04\n",
            "step 3777 | loss: 0.602616 | lr: 2.184597e-04\n",
            "step 3778 | loss: 0.478049 | lr: 2.183119e-04\n",
            "step 3779 | loss: 0.564658 | lr: 2.181642e-04\n",
            "step 3780 | loss: 0.524485 | lr: 2.180164e-04\n",
            "step 3781 | loss: 0.488973 | lr: 2.178688e-04\n",
            "step 3782 | loss: 0.479808 | lr: 2.177211e-04\n",
            "step 3783 | loss: 0.515921 | lr: 2.175735e-04\n",
            "step 3784 | loss: 0.569914 | lr: 2.174259e-04\n",
            "step 3785 | loss: 0.483143 | lr: 2.172783e-04\n",
            "step 3786 | loss: 0.496052 | lr: 2.171307e-04\n",
            "step 3787 | loss: 0.441962 | lr: 2.169832e-04\n",
            "step 3788 | loss: 0.321861 | lr: 2.168358e-04\n",
            "step 3789 | loss: 0.495057 | lr: 2.166883e-04\n",
            "step 3790 | loss: 0.622029 | lr: 2.165409e-04\n",
            "step 3791 | loss: 0.526456 | lr: 2.163935e-04\n",
            "step 3792 | loss: 0.555383 | lr: 2.162461e-04\n",
            "step 3793 | loss: 0.489416 | lr: 2.160988e-04\n",
            "step 3794 | loss: 0.546619 | lr: 2.159515e-04\n",
            "step 3795 | loss: 0.760781 | lr: 2.158042e-04\n",
            "step 3796 | loss: 0.613308 | lr: 2.156570e-04\n",
            "step 3797 | loss: 0.587379 | lr: 2.155098e-04\n",
            "step 3798 | loss: 0.554231 | lr: 2.153626e-04\n",
            "step 3799 | loss: 0.605127 | lr: 2.152154e-04\n",
            "step 3800 | loss: 0.576018 | lr: 2.150683e-04\n",
            "step 3801 | loss: 0.535521 | lr: 2.149212e-04\n",
            "step 3802 | loss: 0.608500 | lr: 2.147741e-04\n",
            "step 3803 | loss: 0.627732 | lr: 2.146271e-04\n",
            "step 3804 | loss: 0.518335 | lr: 2.144801e-04\n",
            "step 3805 | loss: 0.548737 | lr: 2.143331e-04\n",
            "step 3806 | loss: 0.435988 | lr: 2.141862e-04\n",
            "step 3807 | loss: 0.473517 | lr: 2.140393e-04\n",
            "step 3808 | loss: 0.455531 | lr: 2.138924e-04\n",
            "step 3809 | loss: 0.451229 | lr: 2.137456e-04\n",
            "step 3810 | loss: 0.436550 | lr: 2.135988e-04\n",
            "step 3811 | loss: 0.413035 | lr: 2.134520e-04\n",
            "step 3812 | loss: 0.416867 | lr: 2.133052e-04\n",
            "step 3813 | loss: 0.570635 | lr: 2.131585e-04\n",
            "step 3814 | loss: 0.431505 | lr: 2.130118e-04\n",
            "step 3815 | loss: 0.484865 | lr: 2.128651e-04\n",
            "step 3816 | loss: 0.462364 | lr: 2.127185e-04\n",
            "step 3817 | loss: 0.393761 | lr: 2.125719e-04\n",
            "step 3818 | loss: 0.546136 | lr: 2.124253e-04\n",
            "step 3819 | loss: 0.364029 | lr: 2.122788e-04\n",
            "step 3820 | loss: 0.330029 | lr: 2.121323e-04\n",
            "step 3821 | loss: 0.472815 | lr: 2.119858e-04\n",
            "step 3822 | loss: 0.382016 | lr: 2.118394e-04\n",
            "step 3823 | loss: 0.448554 | lr: 2.116930e-04\n",
            "step 3824 | loss: 0.435794 | lr: 2.115466e-04\n",
            "step 3825 | loss: 0.483513 | lr: 2.114002e-04\n",
            "step 3826 | loss: 0.507304 | lr: 2.112539e-04\n",
            "step 3827 | loss: 0.411654 | lr: 2.111076e-04\n",
            "step 3828 | loss: 0.407327 | lr: 2.109614e-04\n",
            "step 3829 | loss: 0.455068 | lr: 2.108151e-04\n",
            "step 3830 | loss: 0.471535 | lr: 2.106690e-04\n",
            "step 3831 | loss: 0.481183 | lr: 2.105228e-04\n",
            "step 3832 | loss: 0.481322 | lr: 2.103767e-04\n",
            "step 3833 | loss: 0.413689 | lr: 2.102306e-04\n",
            "step 3834 | loss: 0.518934 | lr: 2.100845e-04\n",
            "step 3835 | loss: 0.451982 | lr: 2.099385e-04\n",
            "step 3836 | loss: 0.395649 | lr: 2.097925e-04\n",
            "step 3837 | loss: 0.419831 | lr: 2.096465e-04\n",
            "step 3838 | loss: 0.349599 | lr: 2.095006e-04\n",
            "step 3839 | loss: 0.523158 | lr: 2.093547e-04\n",
            "step 3840 | loss: 0.530526 | lr: 2.092088e-04\n",
            "step 3841 | loss: 0.506360 | lr: 2.090629e-04\n",
            "step 3842 | loss: 0.638664 | lr: 2.089171e-04\n",
            "step 3843 | loss: 0.587161 | lr: 2.087714e-04\n",
            "step 3844 | loss: 0.481012 | lr: 2.086256e-04\n",
            "step 3845 | loss: 0.470903 | lr: 2.084799e-04\n",
            "step 3846 | loss: 0.512579 | lr: 2.083342e-04\n",
            "step 3847 | loss: 0.503447 | lr: 2.081886e-04\n",
            "step 3848 | loss: 0.527799 | lr: 2.080430e-04\n",
            "step 3849 | loss: 0.435763 | lr: 2.078974e-04\n",
            "step 3850 | loss: 0.469780 | lr: 2.077518e-04\n",
            "step 3851 | loss: 0.452427 | lr: 2.076063e-04\n",
            "step 3852 | loss: 0.458451 | lr: 2.074608e-04\n",
            "step 3853 | loss: 0.457406 | lr: 2.073154e-04\n",
            "step 3854 | loss: 0.443824 | lr: 2.071700e-04\n",
            "step 3855 | loss: 0.410990 | lr: 2.070246e-04\n",
            "step 3856 | loss: 0.416027 | lr: 2.068792e-04\n",
            "step 3857 | loss: 0.332602 | lr: 2.067339e-04\n",
            "step 3858 | loss: 0.496554 | lr: 2.065886e-04\n",
            "step 3859 | loss: 0.456920 | lr: 2.064434e-04\n",
            "step 3860 | loss: 0.566752 | lr: 2.062981e-04\n",
            "step 3861 | loss: 0.542097 | lr: 2.061529e-04\n",
            "step 3862 | loss: 0.473904 | lr: 2.060078e-04\n",
            "step 3863 | loss: 0.446014 | lr: 2.058627e-04\n",
            "step 3864 | loss: 0.418652 | lr: 2.057176e-04\n",
            "step 3865 | loss: 0.484965 | lr: 2.055725e-04\n",
            "step 3866 | loss: 0.411124 | lr: 2.054275e-04\n",
            "step 3867 | loss: 0.341385 | lr: 2.052825e-04\n",
            "step 3868 | loss: 0.436075 | lr: 2.051375e-04\n",
            "step 3869 | loss: 0.356072 | lr: 2.049926e-04\n",
            "step 3870 | loss: 0.493833 | lr: 2.048477e-04\n",
            "step 3871 | loss: 0.460792 | lr: 2.047029e-04\n",
            "step 3872 | loss: 0.452038 | lr: 2.045581e-04\n",
            "step 3873 | loss: 0.462273 | lr: 2.044133e-04\n",
            "step 3874 | loss: 0.418366 | lr: 2.042685e-04\n",
            "step 3875 | loss: 0.399245 | lr: 2.041238e-04\n",
            "step 3876 | loss: 0.494404 | lr: 2.039791e-04\n",
            "step 3877 | loss: 0.528937 | lr: 2.038344e-04\n",
            "step 3878 | loss: 0.458009 | lr: 2.036898e-04\n",
            "step 3879 | loss: 0.511399 | lr: 2.035452e-04\n",
            "step 3880 | loss: 0.549954 | lr: 2.034007e-04\n",
            "step 3881 | loss: 0.457553 | lr: 2.032562e-04\n",
            "step 3882 | loss: 0.511102 | lr: 2.031117e-04\n",
            "step 3883 | loss: 0.505148 | lr: 2.029672e-04\n",
            "step 3884 | loss: 0.483880 | lr: 2.028228e-04\n",
            "step 3885 | loss: 0.434608 | lr: 2.026784e-04\n",
            "step 3886 | loss: 0.442599 | lr: 2.025341e-04\n",
            "step 3887 | loss: 0.378554 | lr: 2.023898e-04\n",
            "step 3888 | loss: 0.443483 | lr: 2.022455e-04\n",
            "step 3889 | loss: 0.344359 | lr: 2.021012e-04\n",
            "step 3890 | loss: 0.477938 | lr: 2.019570e-04\n",
            "step 3891 | loss: 0.442136 | lr: 2.018129e-04\n",
            "step 3892 | loss: 0.431753 | lr: 2.016687e-04\n",
            "step 3893 | loss: 0.409744 | lr: 2.015246e-04\n",
            "step 3894 | loss: 0.355442 | lr: 2.013805e-04\n",
            "step 3895 | loss: 0.315486 | lr: 2.012365e-04\n",
            "step 3896 | loss: 0.389912 | lr: 2.010925e-04\n",
            "step 3897 | loss: 0.299749 | lr: 2.009485e-04\n",
            "step 3898 | loss: 0.328828 | lr: 2.008046e-04\n",
            "step 3899 | loss: 0.522169 | lr: 2.006607e-04\n",
            "step 3900 | loss: 0.509143 | lr: 2.005168e-04\n",
            "step 3901 | loss: 0.398293 | lr: 2.003730e-04\n",
            "step 3902 | loss: 0.441165 | lr: 2.002292e-04\n",
            "step 3903 | loss: 0.523257 | lr: 2.000854e-04\n",
            "step 3904 | loss: 0.424207 | lr: 1.999417e-04\n",
            "step 3905 | loss: 0.489052 | lr: 1.997980e-04\n",
            "step 3906 | loss: 0.431103 | lr: 1.996543e-04\n",
            "step 3907 | loss: 0.533594 | lr: 1.995107e-04\n",
            "step 3908 | loss: 0.492689 | lr: 1.993671e-04\n",
            "step 3909 | loss: 0.488783 | lr: 1.992236e-04\n",
            "step 3910 | loss: 0.434155 | lr: 1.990801e-04\n",
            "step 3911 | loss: 0.440465 | lr: 1.989366e-04\n",
            "step 3912 | loss: 0.528162 | lr: 1.987931e-04\n",
            "step 3913 | loss: 0.432043 | lr: 1.986497e-04\n",
            "step 3914 | loss: 0.433127 | lr: 1.985063e-04\n",
            "step 3915 | loss: 0.592572 | lr: 1.983630e-04\n",
            "step 3916 | loss: 0.544958 | lr: 1.982197e-04\n",
            "step 3917 | loss: 0.510078 | lr: 1.980764e-04\n",
            "step 3918 | loss: 0.463616 | lr: 1.979332e-04\n",
            "step 3919 | loss: 0.414796 | lr: 1.977900e-04\n",
            "step 3920 | loss: 0.531944 | lr: 1.976468e-04\n",
            "step 3921 | loss: 0.428772 | lr: 1.975037e-04\n",
            "step 3922 | loss: 0.404448 | lr: 1.973606e-04\n",
            "step 3923 | loss: 0.437198 | lr: 1.972176e-04\n",
            "step 3924 | loss: 0.403169 | lr: 1.970745e-04\n",
            "step 3925 | loss: 0.428063 | lr: 1.969316e-04\n",
            "step 3926 | loss: 0.378590 | lr: 1.967886e-04\n",
            "step 3927 | loss: 0.529227 | lr: 1.966457e-04\n",
            "step 3928 | loss: 0.364334 | lr: 1.965028e-04\n",
            "step 3929 | loss: 0.434543 | lr: 1.963600e-04\n",
            "step 3930 | loss: 0.354990 | lr: 1.962172e-04\n",
            "step 3931 | loss: 0.383536 | lr: 1.960744e-04\n",
            "step 3932 | loss: 0.352153 | lr: 1.959317e-04\n",
            "step 3933 | loss: 0.363599 | lr: 1.957890e-04\n",
            "step 3934 | loss: 0.400081 | lr: 1.956463e-04\n",
            "step 3935 | loss: 0.369802 | lr: 1.955037e-04\n",
            "step 3936 | loss: 0.321152 | lr: 1.953611e-04\n",
            "step 3937 | loss: 0.426360 | lr: 1.952186e-04\n",
            "step 3938 | loss: 0.442064 | lr: 1.950760e-04\n",
            "step 3939 | loss: 0.521173 | lr: 1.949336e-04\n",
            "step 3940 | loss: 0.467506 | lr: 1.947911e-04\n",
            "step 3941 | loss: 0.446422 | lr: 1.946487e-04\n",
            "step 3942 | loss: 0.395269 | lr: 1.945063e-04\n",
            "step 3943 | loss: 0.328049 | lr: 1.943640e-04\n",
            "step 3944 | loss: 0.385410 | lr: 1.942217e-04\n",
            "step 3945 | loss: 0.355122 | lr: 1.940795e-04\n",
            "step 3946 | loss: 0.333762 | lr: 1.939372e-04\n",
            "step 3947 | loss: 0.334794 | lr: 1.937951e-04\n",
            "step 3948 | loss: 0.373091 | lr: 1.936529e-04\n",
            "step 3949 | loss: 0.402060 | lr: 1.935108e-04\n",
            "step 3950 | loss: 0.326556 | lr: 1.933687e-04\n",
            "step 3951 | loss: 0.350621 | lr: 1.932267e-04\n",
            "step 3952 | loss: 0.307413 | lr: 1.930847e-04\n",
            "step 3953 | loss: 0.249396 | lr: 1.929427e-04\n",
            "step 3954 | loss: 0.370386 | lr: 1.928008e-04\n",
            "step 3955 | loss: 0.483078 | lr: 1.926589e-04\n",
            "step 3956 | loss: 0.419229 | lr: 1.925170e-04\n",
            "step 3957 | loss: 0.414990 | lr: 1.923752e-04\n",
            "step 3958 | loss: 0.352909 | lr: 1.922335e-04\n",
            "step 3959 | loss: 0.398140 | lr: 1.920917e-04\n",
            "step 3960 | loss: 0.598884 | lr: 1.919500e-04\n",
            "step 3961 | loss: 0.469529 | lr: 1.918083e-04\n",
            "step 3962 | loss: 0.429449 | lr: 1.916667e-04\n",
            "step 3963 | loss: 0.386116 | lr: 1.915251e-04\n",
            "step 3964 | loss: 0.405240 | lr: 1.913836e-04\n",
            "step 3965 | loss: 0.395148 | lr: 1.912421e-04\n",
            "step 3966 | loss: 0.346479 | lr: 1.911006e-04\n",
            "step 3967 | loss: 0.447197 | lr: 1.909591e-04\n",
            "step 3968 | loss: 0.465524 | lr: 1.908177e-04\n",
            "step 3969 | loss: 0.358054 | lr: 1.906764e-04\n",
            "step 3970 | loss: 0.372591 | lr: 1.905350e-04\n",
            "step 3971 | loss: 0.278643 | lr: 1.903938e-04\n",
            "step 3972 | loss: 0.322144 | lr: 1.902525e-04\n",
            "step 3973 | loss: 0.307891 | lr: 1.901113e-04\n",
            "step 3974 | loss: 0.315578 | lr: 1.899701e-04\n",
            "step 3975 | loss: 0.330278 | lr: 1.898290e-04\n",
            "step 3976 | loss: 0.308211 | lr: 1.896879e-04\n",
            "step 3977 | loss: 0.299748 | lr: 1.895468e-04\n",
            "step 3978 | loss: 0.444512 | lr: 1.894058e-04\n",
            "step 3979 | loss: 0.315276 | lr: 1.892648e-04\n",
            "step 3980 | loss: 0.355953 | lr: 1.891239e-04\n",
            "step 3981 | loss: 0.336619 | lr: 1.889829e-04\n",
            "step 3982 | loss: 0.296590 | lr: 1.888421e-04\n",
            "step 3983 | loss: 0.383483 | lr: 1.887012e-04\n",
            "step 3984 | loss: 0.289455 | lr: 1.885604e-04\n",
            "step 3985 | loss: 0.240417 | lr: 1.884197e-04\n",
            "step 3986 | loss: 0.342040 | lr: 1.882790e-04\n",
            "step 3987 | loss: 0.270959 | lr: 1.881383e-04\n",
            "step 3988 | loss: 0.322529 | lr: 1.879977e-04\n",
            "step 3989 | loss: 0.316749 | lr: 1.878571e-04\n",
            "step 3990 | loss: 0.335246 | lr: 1.877165e-04\n",
            "step 3991 | loss: 0.349629 | lr: 1.875760e-04\n",
            "step 3992 | loss: 0.322368 | lr: 1.874355e-04\n",
            "step 3993 | loss: 0.326283 | lr: 1.872950e-04\n",
            "step 3994 | loss: 0.356567 | lr: 1.871546e-04\n",
            "step 3995 | loss: 0.354509 | lr: 1.870143e-04\n",
            "step 3996 | loss: 0.346983 | lr: 1.868739e-04\n",
            "step 3997 | loss: 0.343609 | lr: 1.867336e-04\n",
            "step 3998 | loss: 0.277792 | lr: 1.865934e-04\n",
            "step 3999 | loss: 0.386240 | lr: 1.864532e-04\n",
            "step 4000 | loss: 0.338907 | lr: 1.863130e-04\n",
            "step 4001 | loss: 0.289789 | lr: 1.861729e-04\n",
            "step 4002 | loss: 0.289633 | lr: 1.860328e-04\n",
            "step 4003 | loss: 0.251684 | lr: 1.858927e-04\n",
            "step 4004 | loss: 0.341873 | lr: 1.857527e-04\n",
            "step 4005 | loss: 0.338696 | lr: 1.856127e-04\n",
            "step 4006 | loss: 0.344044 | lr: 1.854728e-04\n",
            "step 4007 | loss: 0.443248 | lr: 1.853329e-04\n",
            "step 4008 | loss: 0.449450 | lr: 1.851930e-04\n",
            "step 4009 | loss: 0.353133 | lr: 1.850532e-04\n",
            "step 4010 | loss: 0.376189 | lr: 1.849134e-04\n",
            "step 4011 | loss: 0.376073 | lr: 1.847737e-04\n",
            "step 4012 | loss: 0.377478 | lr: 1.846340e-04\n",
            "step 4013 | loss: 0.393163 | lr: 1.844943e-04\n",
            "step 4014 | loss: 0.343468 | lr: 1.843547e-04\n",
            "step 4015 | loss: 0.347874 | lr: 1.842151e-04\n",
            "step 4016 | loss: 0.319035 | lr: 1.840755e-04\n",
            "step 4017 | loss: 0.340453 | lr: 1.839360e-04\n",
            "step 4018 | loss: 0.327580 | lr: 1.837966e-04\n",
            "step 4019 | loss: 0.327270 | lr: 1.836572e-04\n",
            "step 4020 | loss: 0.323311 | lr: 1.835178e-04\n",
            "step 4021 | loss: 0.299735 | lr: 1.833784e-04\n",
            "step 4022 | loss: 0.248211 | lr: 1.832391e-04\n",
            "step 4023 | loss: 0.336130 | lr: 1.830999e-04\n",
            "step 4024 | loss: 0.338923 | lr: 1.829606e-04\n",
            "step 4025 | loss: 0.442344 | lr: 1.828214e-04\n",
            "step 4026 | loss: 0.403102 | lr: 1.826823e-04\n",
            "step 4027 | loss: 0.373093 | lr: 1.825432e-04\n",
            "step 4028 | loss: 0.342299 | lr: 1.824041e-04\n",
            "step 4029 | loss: 0.350860 | lr: 1.822651e-04\n",
            "step 4030 | loss: 0.381443 | lr: 1.821261e-04\n",
            "step 4031 | loss: 0.312240 | lr: 1.819872e-04\n",
            "step 4032 | loss: 0.266037 | lr: 1.818483e-04\n",
            "step 4033 | loss: 0.325032 | lr: 1.817094e-04\n",
            "step 4034 | loss: 0.263614 | lr: 1.815706e-04\n",
            "step 4035 | loss: 0.373807 | lr: 1.814318e-04\n",
            "step 4036 | loss: 0.348431 | lr: 1.812931e-04\n",
            "step 4037 | loss: 0.342246 | lr: 1.811544e-04\n",
            "step 4038 | loss: 0.359319 | lr: 1.810157e-04\n",
            "step 4039 | loss: 0.300219 | lr: 1.808771e-04\n",
            "step 4040 | loss: 0.303912 | lr: 1.807385e-04\n",
            "step 4041 | loss: 0.345455 | lr: 1.806000e-04\n",
            "step 4042 | loss: 0.378926 | lr: 1.804615e-04\n",
            "step 4043 | loss: 0.353813 | lr: 1.803230e-04\n",
            "step 4044 | loss: 0.402112 | lr: 1.801846e-04\n",
            "step 4045 | loss: 0.430121 | lr: 1.800462e-04\n",
            "step 4046 | loss: 0.351770 | lr: 1.799079e-04\n",
            "step 4047 | loss: 0.425510 | lr: 1.797696e-04\n",
            "step 4048 | loss: 0.374900 | lr: 1.796314e-04\n",
            "step 4049 | loss: 0.348174 | lr: 1.794932e-04\n",
            "step 4050 | loss: 0.337000 | lr: 1.793550e-04\n",
            "step 4051 | loss: 0.305594 | lr: 1.792169e-04\n",
            "step 4052 | loss: 0.287110 | lr: 1.790788e-04\n",
            "step 4053 | loss: 0.308423 | lr: 1.789407e-04\n",
            "step 4054 | loss: 0.246907 | lr: 1.788027e-04\n",
            "step 4055 | loss: 0.309994 | lr: 1.786648e-04\n",
            "step 4056 | loss: 0.285195 | lr: 1.785269e-04\n",
            "step 4057 | loss: 0.294842 | lr: 1.783890e-04\n",
            "step 4058 | loss: 0.267937 | lr: 1.782511e-04\n",
            "step 4059 | loss: 0.261221 | lr: 1.781133e-04\n",
            "step 4060 | loss: 0.229465 | lr: 1.779756e-04\n",
            "step 4061 | loss: 0.285693 | lr: 1.778379e-04\n",
            "step 4062 | loss: 0.233729 | lr: 1.777002e-04\n",
            "step 4063 | loss: 0.257993 | lr: 1.775626e-04\n",
            "step 4064 | loss: 0.418056 | lr: 1.774250e-04\n",
            "step 4065 | loss: 0.411912 | lr: 1.772874e-04\n",
            "step 4066 | loss: 0.282847 | lr: 1.771499e-04\n",
            "step 4067 | loss: 0.347941 | lr: 1.770125e-04\n",
            "step 4068 | loss: 0.373756 | lr: 1.768751e-04\n",
            "step 4069 | loss: 0.324421 | lr: 1.767377e-04\n",
            "step 4070 | loss: 0.383786 | lr: 1.766003e-04\n",
            "step 4071 | loss: 0.329056 | lr: 1.764630e-04\n",
            "step 4072 | loss: 0.378042 | lr: 1.763258e-04\n",
            "step 4073 | loss: 0.334019 | lr: 1.761886e-04\n",
            "step 4074 | loss: 0.329224 | lr: 1.760514e-04\n",
            "step 4075 | loss: 0.299360 | lr: 1.759143e-04\n",
            "step 4076 | loss: 0.305274 | lr: 1.757772e-04\n",
            "step 4077 | loss: 0.377601 | lr: 1.756402e-04\n",
            "step 4078 | loss: 0.307037 | lr: 1.755032e-04\n",
            "step 4079 | loss: 0.316849 | lr: 1.753662e-04\n",
            "step 4080 | loss: 0.467701 | lr: 1.752293e-04\n",
            "step 4081 | loss: 0.395419 | lr: 1.750924e-04\n",
            "step 4082 | loss: 0.369518 | lr: 1.749556e-04\n",
            "step 4083 | loss: 0.338527 | lr: 1.748188e-04\n",
            "step 4084 | loss: 0.322107 | lr: 1.746821e-04\n",
            "step 4085 | loss: 0.396700 | lr: 1.745454e-04\n",
            "step 4086 | loss: 0.305585 | lr: 1.744087e-04\n",
            "step 4087 | loss: 0.288902 | lr: 1.742721e-04\n",
            "step 4088 | loss: 0.312508 | lr: 1.741355e-04\n",
            "step 4089 | loss: 0.265641 | lr: 1.739990e-04\n",
            "step 4090 | loss: 0.279950 | lr: 1.738625e-04\n",
            "step 4091 | loss: 0.243732 | lr: 1.737261e-04\n",
            "step 4092 | loss: 0.363017 | lr: 1.735897e-04\n",
            "step 4093 | loss: 0.272032 | lr: 1.734533e-04\n",
            "step 4094 | loss: 0.297817 | lr: 1.733170e-04\n",
            "step 4095 | loss: 0.262356 | lr: 1.731807e-04\n",
            "step 4096 | loss: 0.293768 | lr: 1.730445e-04\n",
            "step 4097 | loss: 0.264196 | lr: 1.729083e-04\n",
            "step 4098 | loss: 0.279003 | lr: 1.727722e-04\n",
            "step 4099 | loss: 0.296366 | lr: 1.726361e-04\n",
            "step 4100 | loss: 0.284160 | lr: 1.725000e-04\n",
            "step 4101 | loss: 0.266086 | lr: 1.723640e-04\n",
            "step 4102 | loss: 0.345015 | lr: 1.722280e-04\n",
            "step 4103 | loss: 0.351897 | lr: 1.720921e-04\n",
            "step 4104 | loss: 0.402276 | lr: 1.719562e-04\n",
            "step 4105 | loss: 0.374183 | lr: 1.718204e-04\n",
            "step 4106 | loss: 0.336144 | lr: 1.716846e-04\n",
            "step 4107 | loss: 0.301603 | lr: 1.715488e-04\n",
            "step 4108 | loss: 0.222327 | lr: 1.714131e-04\n",
            "step 4109 | loss: 0.278480 | lr: 1.712774e-04\n",
            "step 4110 | loss: 0.256464 | lr: 1.711418e-04\n",
            "step 4111 | loss: 0.235727 | lr: 1.710062e-04\n",
            "step 4112 | loss: 0.254911 | lr: 1.708707e-04\n",
            "step 4113 | loss: 0.279167 | lr: 1.707352e-04\n",
            "step 4114 | loss: 0.284124 | lr: 1.705998e-04\n",
            "step 4115 | loss: 0.271479 | lr: 1.704644e-04\n",
            "step 4116 | loss: 0.249366 | lr: 1.703290e-04\n",
            "step 4117 | loss: 0.220647 | lr: 1.701937e-04\n",
            "step 4118 | loss: 0.179691 | lr: 1.700584e-04\n",
            "step 4119 | loss: 0.243965 | lr: 1.699232e-04\n",
            "step 4120 | loss: 0.321892 | lr: 1.697880e-04\n",
            "step 4121 | loss: 0.293698 | lr: 1.696529e-04\n",
            "step 4122 | loss: 0.271559 | lr: 1.695178e-04\n",
            "step 4123 | loss: 0.269527 | lr: 1.693827e-04\n",
            "step 4124 | loss: 0.328019 | lr: 1.692477e-04\n",
            "step 4125 | loss: 0.448477 | lr: 1.691128e-04\n",
            "step 4126 | loss: 0.336670 | lr: 1.689778e-04\n",
            "step 4127 | loss: 0.318489 | lr: 1.688430e-04\n",
            "step 4128 | loss: 0.277993 | lr: 1.687081e-04\n",
            "step 4129 | loss: 0.316935 | lr: 1.685734e-04\n",
            "step 4130 | loss: 0.314442 | lr: 1.684386e-04\n",
            "step 4131 | loss: 0.277360 | lr: 1.683039e-04\n",
            "step 4132 | loss: 0.326427 | lr: 1.681693e-04\n",
            "step 4133 | loss: 0.342123 | lr: 1.680347e-04\n",
            "step 4134 | loss: 0.250682 | lr: 1.679001e-04\n",
            "step 4135 | loss: 0.251158 | lr: 1.677656e-04\n",
            "step 4136 | loss: 0.219671 | lr: 1.676311e-04\n",
            "step 4137 | loss: 0.228426 | lr: 1.674967e-04\n",
            "step 4138 | loss: 0.232082 | lr: 1.673623e-04\n",
            "step 4139 | loss: 0.225893 | lr: 1.672280e-04\n",
            "step 4140 | loss: 0.270514 | lr: 1.670937e-04\n",
            "step 4141 | loss: 0.224622 | lr: 1.669594e-04\n",
            "step 4142 | loss: 0.201147 | lr: 1.668252e-04\n",
            "step 4143 | loss: 0.308444 | lr: 1.666911e-04\n",
            "step 4144 | loss: 0.242858 | lr: 1.665569e-04\n",
            "step 4145 | loss: 0.284439 | lr: 1.664229e-04\n",
            "step 4146 | loss: 0.249542 | lr: 1.662889e-04\n",
            "step 4147 | loss: 0.222146 | lr: 1.661549e-04\n",
            "step 4148 | loss: 0.292351 | lr: 1.660209e-04\n",
            "step 4149 | loss: 0.201805 | lr: 1.658871e-04\n",
            "step 4150 | loss: 0.190579 | lr: 1.657532e-04\n",
            "step 4151 | loss: 0.220052 | lr: 1.656194e-04\n",
            "step 4152 | loss: 0.189045 | lr: 1.654857e-04\n",
            "step 4153 | loss: 0.204647 | lr: 1.653520e-04\n",
            "step 4154 | loss: 0.244838 | lr: 1.652183e-04\n",
            "step 4155 | loss: 0.245607 | lr: 1.650847e-04\n",
            "step 4156 | loss: 0.247824 | lr: 1.649511e-04\n",
            "step 4157 | loss: 0.242035 | lr: 1.648176e-04\n",
            "step 4158 | loss: 0.220915 | lr: 1.646841e-04\n",
            "step 4159 | loss: 0.255107 | lr: 1.645507e-04\n",
            "step 4160 | loss: 0.250806 | lr: 1.644173e-04\n",
            "step 4161 | loss: 0.252584 | lr: 1.642840e-04\n",
            "step 4162 | loss: 0.255738 | lr: 1.641507e-04\n",
            "step 4163 | loss: 0.222054 | lr: 1.640174e-04\n",
            "step 4164 | loss: 0.290140 | lr: 1.638842e-04\n",
            "step 4165 | loss: 0.246263 | lr: 1.637511e-04\n",
            "step 4166 | loss: 0.223209 | lr: 1.636179e-04\n",
            "step 4167 | loss: 0.205208 | lr: 1.634849e-04\n",
            "step 4168 | loss: 0.187910 | lr: 1.633519e-04\n",
            "step 4169 | loss: 0.259021 | lr: 1.632189e-04\n",
            "step 4170 | loss: 0.270655 | lr: 1.630860e-04\n",
            "step 4171 | loss: 0.279994 | lr: 1.629531e-04\n",
            "step 4172 | loss: 0.337638 | lr: 1.628202e-04\n",
            "step 4173 | loss: 0.311702 | lr: 1.626875e-04\n",
            "step 4174 | loss: 0.237996 | lr: 1.625547e-04\n",
            "step 4175 | loss: 0.238079 | lr: 1.624220e-04\n",
            "step 4176 | loss: 0.255483 | lr: 1.622894e-04\n",
            "step 4177 | loss: 0.253539 | lr: 1.621568e-04\n",
            "step 4178 | loss: 0.273164 | lr: 1.620242e-04\n",
            "step 4179 | loss: 0.233183 | lr: 1.618917e-04\n",
            "step 4180 | loss: 0.245232 | lr: 1.617592e-04\n",
            "step 4181 | loss: 0.240420 | lr: 1.616268e-04\n",
            "step 4182 | loss: 0.247036 | lr: 1.614944e-04\n",
            "step 4183 | loss: 0.240290 | lr: 1.613621e-04\n",
            "step 4184 | loss: 0.254360 | lr: 1.612298e-04\n",
            "step 4185 | loss: 0.237977 | lr: 1.610976e-04\n",
            "step 4186 | loss: 0.230967 | lr: 1.609654e-04\n",
            "step 4187 | loss: 0.202961 | lr: 1.608333e-04\n",
            "step 4188 | loss: 0.236564 | lr: 1.607012e-04\n",
            "step 4189 | loss: 0.255005 | lr: 1.605692e-04\n",
            "step 4190 | loss: 0.304327 | lr: 1.604372e-04\n",
            "step 4191 | loss: 0.276848 | lr: 1.603052e-04\n",
            "step 4192 | loss: 0.246285 | lr: 1.601733e-04\n",
            "step 4193 | loss: 0.228037 | lr: 1.600414e-04\n",
            "step 4194 | loss: 0.233931 | lr: 1.599096e-04\n",
            "step 4195 | loss: 0.251581 | lr: 1.597779e-04\n",
            "step 4196 | loss: 0.219130 | lr: 1.596462e-04\n",
            "step 4197 | loss: 0.222249 | lr: 1.595145e-04\n",
            "step 4198 | loss: 0.232191 | lr: 1.593829e-04\n",
            "step 4199 | loss: 0.210927 | lr: 1.592513e-04\n",
            "step 4200 | loss: 0.241733 | lr: 1.591198e-04\n",
            "step 4201 | loss: 0.226193 | lr: 1.589883e-04\n",
            "step 4202 | loss: 0.248306 | lr: 1.588569e-04\n",
            "step 4203 | loss: 0.236216 | lr: 1.587255e-04\n",
            "step 4204 | loss: 0.202364 | lr: 1.585941e-04\n",
            "step 4205 | loss: 0.232128 | lr: 1.584629e-04\n",
            "step 4206 | loss: 0.240110 | lr: 1.583316e-04\n",
            "step 4207 | loss: 0.271134 | lr: 1.582004e-04\n",
            "step 4208 | loss: 0.243365 | lr: 1.580693e-04\n",
            "step 4209 | loss: 0.244085 | lr: 1.579382e-04\n",
            "step 4210 | loss: 0.308196 | lr: 1.578071e-04\n",
            "step 4211 | loss: 0.257365 | lr: 1.576761e-04\n",
            "step 4212 | loss: 0.281217 | lr: 1.575452e-04\n",
            "step 4213 | loss: 0.277696 | lr: 1.574143e-04\n",
            "step 4214 | loss: 0.268870 | lr: 1.572834e-04\n",
            "step 4215 | loss: 0.243542 | lr: 1.571526e-04\n",
            "step 4216 | loss: 0.240313 | lr: 1.570218e-04\n",
            "step 4217 | loss: 0.220129 | lr: 1.568911e-04\n",
            "step 4218 | loss: 0.254361 | lr: 1.567605e-04\n",
            "step 4219 | loss: 0.216590 | lr: 1.566298e-04\n",
            "step 4220 | loss: 0.250834 | lr: 1.564993e-04\n",
            "step 4221 | loss: 0.217779 | lr: 1.563687e-04\n",
            "step 4222 | loss: 0.233869 | lr: 1.562383e-04\n",
            "step 4223 | loss: 0.196546 | lr: 1.561078e-04\n",
            "step 4224 | loss: 0.202282 | lr: 1.559775e-04\n",
            "step 4225 | loss: 0.188195 | lr: 1.558471e-04\n",
            "step 4226 | loss: 0.201394 | lr: 1.557168e-04\n",
            "step 4227 | loss: 0.145910 | lr: 1.555866e-04\n",
            "step 4228 | loss: 0.172790 | lr: 1.554564e-04\n",
            "step 4229 | loss: 0.295056 | lr: 1.553263e-04\n",
            "step 4230 | loss: 0.303716 | lr: 1.551962e-04\n",
            "step 4231 | loss: 0.221216 | lr: 1.550662e-04\n",
            "step 4232 | loss: 0.264087 | lr: 1.549362e-04\n",
            "step 4233 | loss: 0.306796 | lr: 1.548062e-04\n",
            "step 4234 | loss: 0.285159 | lr: 1.546763e-04\n",
            "step 4235 | loss: 0.270121 | lr: 1.545465e-04\n",
            "step 4236 | loss: 0.247895 | lr: 1.544167e-04\n",
            "step 4237 | loss: 0.279772 | lr: 1.542870e-04\n",
            "step 4238 | loss: 0.240416 | lr: 1.541573e-04\n",
            "step 4239 | loss: 0.234425 | lr: 1.540276e-04\n",
            "step 4240 | loss: 0.257128 | lr: 1.538980e-04\n",
            "step 4241 | loss: 0.230701 | lr: 1.537685e-04\n",
            "step 4242 | loss: 0.311739 | lr: 1.536390e-04\n",
            "step 4243 | loss: 0.236410 | lr: 1.535095e-04\n",
            "step 4244 | loss: 0.238814 | lr: 1.533801e-04\n",
            "step 4245 | loss: 0.324809 | lr: 1.532507e-04\n",
            "step 4246 | loss: 0.264055 | lr: 1.531214e-04\n",
            "step 4247 | loss: 0.260175 | lr: 1.529922e-04\n",
            "step 4248 | loss: 0.248170 | lr: 1.528630e-04\n",
            "step 4249 | loss: 0.268721 | lr: 1.527338e-04\n",
            "step 4250 | loss: 0.302402 | lr: 1.526047e-04\n",
            "step 4251 | loss: 0.247325 | lr: 1.524756e-04\n",
            "step 4252 | loss: 0.229541 | lr: 1.523466e-04\n",
            "step 4253 | loss: 0.232969 | lr: 1.522177e-04\n",
            "step 4254 | loss: 0.185603 | lr: 1.520888e-04\n",
            "step 4255 | loss: 0.188718 | lr: 1.519599e-04\n",
            "step 4256 | loss: 0.171441 | lr: 1.518311e-04\n",
            "step 4257 | loss: 0.232949 | lr: 1.517023e-04\n",
            "step 4258 | loss: 0.193160 | lr: 1.515736e-04\n",
            "step 4259 | loss: 0.184547 | lr: 1.514449e-04\n",
            "step 4260 | loss: 0.176089 | lr: 1.513163e-04\n",
            "step 4261 | loss: 0.193721 | lr: 1.511878e-04\n",
            "step 4262 | loss: 0.159008 | lr: 1.510593e-04\n",
            "step 4263 | loss: 0.178414 | lr: 1.509308e-04\n",
            "step 4264 | loss: 0.201708 | lr: 1.508024e-04\n",
            "step 4265 | loss: 0.178651 | lr: 1.506740e-04\n",
            "step 4266 | loss: 0.220706 | lr: 1.505457e-04\n",
            "step 4267 | loss: 0.262109 | lr: 1.504174e-04\n",
            "step 4268 | loss: 0.245499 | lr: 1.502892e-04\n",
            "step 4269 | loss: 0.290845 | lr: 1.501610e-04\n",
            "step 4270 | loss: 0.279597 | lr: 1.500329e-04\n",
            "step 4271 | loss: 0.242728 | lr: 1.499049e-04\n",
            "step 4272 | loss: 0.218165 | lr: 1.497769e-04\n",
            "step 4273 | loss: 0.189887 | lr: 1.496489e-04\n",
            "step 4274 | loss: 0.201623 | lr: 1.495210e-04\n",
            "step 4275 | loss: 0.193584 | lr: 1.493931e-04\n",
            "step 4276 | loss: 0.175574 | lr: 1.492653e-04\n",
            "step 4277 | loss: 0.205627 | lr: 1.491375e-04\n",
            "step 4278 | loss: 0.207952 | lr: 1.490098e-04\n",
            "step 4279 | loss: 0.222121 | lr: 1.488822e-04\n",
            "step 4280 | loss: 0.185354 | lr: 1.487545e-04\n",
            "step 4281 | loss: 0.196305 | lr: 1.486270e-04\n",
            "step 4282 | loss: 0.169888 | lr: 1.484995e-04\n",
            "step 4283 | loss: 0.160591 | lr: 1.483720e-04\n",
            "step 4284 | loss: 0.190314 | lr: 1.482446e-04\n",
            "step 4285 | loss: 0.246985 | lr: 1.481172e-04\n",
            "step 4286 | loss: 0.226772 | lr: 1.479899e-04\n",
            "step 4287 | loss: 0.222081 | lr: 1.478627e-04\n",
            "step 4288 | loss: 0.207032 | lr: 1.477355e-04\n",
            "step 4289 | loss: 0.253818 | lr: 1.476083e-04\n",
            "step 4290 | loss: 0.319982 | lr: 1.474812e-04\n",
            "step 4291 | loss: 0.227940 | lr: 1.473541e-04\n",
            "step 4292 | loss: 0.222616 | lr: 1.472271e-04\n",
            "step 4293 | loss: 0.197797 | lr: 1.471002e-04\n",
            "step 4294 | loss: 0.232740 | lr: 1.469733e-04\n",
            "step 4295 | loss: 0.207566 | lr: 1.468464e-04\n",
            "step 4296 | loss: 0.187941 | lr: 1.467196e-04\n",
            "step 4297 | loss: 0.220914 | lr: 1.465929e-04\n",
            "step 4298 | loss: 0.220627 | lr: 1.464662e-04\n",
            "step 4299 | loss: 0.168075 | lr: 1.463395e-04\n",
            "step 4300 | loss: 0.169874 | lr: 1.462130e-04\n",
            "step 4301 | loss: 0.161419 | lr: 1.460864e-04\n",
            "step 4302 | loss: 0.170285 | lr: 1.459599e-04\n",
            "step 4303 | loss: 0.157898 | lr: 1.458335e-04\n",
            "step 4304 | loss: 0.154180 | lr: 1.457071e-04\n",
            "step 4305 | loss: 0.181484 | lr: 1.455807e-04\n",
            "step 4306 | loss: 0.162790 | lr: 1.454545e-04\n",
            "step 4307 | loss: 0.156669 | lr: 1.453282e-04\n",
            "step 4308 | loss: 0.213485 | lr: 1.452020e-04\n",
            "step 4309 | loss: 0.174319 | lr: 1.450759e-04\n",
            "step 4310 | loss: 0.194311 | lr: 1.449498e-04\n",
            "step 4311 | loss: 0.183010 | lr: 1.448238e-04\n",
            "step 4312 | loss: 0.164719 | lr: 1.446978e-04\n",
            "step 4313 | loss: 0.200274 | lr: 1.445719e-04\n",
            "step 4314 | loss: 0.130836 | lr: 1.444460e-04\n",
            "step 4315 | loss: 0.132513 | lr: 1.443202e-04\n",
            "step 4316 | loss: 0.169153 | lr: 1.441944e-04\n",
            "step 4317 | loss: 0.142735 | lr: 1.440687e-04\n",
            "step 4318 | loss: 0.149188 | lr: 1.439430e-04\n",
            "step 4319 | loss: 0.176084 | lr: 1.438174e-04\n",
            "step 4320 | loss: 0.163923 | lr: 1.436919e-04\n",
            "step 4321 | loss: 0.195673 | lr: 1.435664e-04\n",
            "step 4322 | loss: 0.176117 | lr: 1.434409e-04\n",
            "step 4323 | loss: 0.176306 | lr: 1.433155e-04\n",
            "step 4324 | loss: 0.186808 | lr: 1.431901e-04\n",
            "step 4325 | loss: 0.189201 | lr: 1.430648e-04\n",
            "step 4326 | loss: 0.155361 | lr: 1.429396e-04\n",
            "step 4327 | loss: 0.169287 | lr: 1.428144e-04\n",
            "step 4328 | loss: 0.151489 | lr: 1.426893e-04\n",
            "step 4329 | loss: 0.243369 | lr: 1.425642e-04\n",
            "step 4330 | loss: 0.199950 | lr: 1.424391e-04\n",
            "step 4331 | loss: 0.160967 | lr: 1.423141e-04\n",
            "step 4332 | loss: 0.153698 | lr: 1.421892e-04\n",
            "step 4333 | loss: 0.140308 | lr: 1.420643e-04\n",
            "step 4334 | loss: 0.207773 | lr: 1.419395e-04\n",
            "step 4335 | loss: 0.212190 | lr: 1.418147e-04\n",
            "step 4336 | loss: 0.209934 | lr: 1.416900e-04\n",
            "step 4337 | loss: 0.250204 | lr: 1.415653e-04\n",
            "step 4338 | loss: 0.247604 | lr: 1.414407e-04\n",
            "step 4339 | loss: 0.209473 | lr: 1.413161e-04\n",
            "step 4340 | loss: 0.216780 | lr: 1.411916e-04\n",
            "step 4341 | loss: 0.204229 | lr: 1.410671e-04\n",
            "step 4342 | loss: 0.198487 | lr: 1.409427e-04\n",
            "step 4343 | loss: 0.218446 | lr: 1.408184e-04\n",
            "step 4344 | loss: 0.166635 | lr: 1.406941e-04\n",
            "step 4345 | loss: 0.163072 | lr: 1.405698e-04\n",
            "step 4346 | loss: 0.164948 | lr: 1.404456e-04\n",
            "step 4347 | loss: 0.182951 | lr: 1.403215e-04\n",
            "step 4348 | loss: 0.182716 | lr: 1.401974e-04\n",
            "step 4349 | loss: 0.170033 | lr: 1.400734e-04\n",
            "step 4350 | loss: 0.169541 | lr: 1.399494e-04\n",
            "step 4351 | loss: 0.153422 | lr: 1.398254e-04\n",
            "step 4352 | loss: 0.135715 | lr: 1.397016e-04\n",
            "step 4353 | loss: 0.187162 | lr: 1.395777e-04\n",
            "step 4354 | loss: 0.215503 | lr: 1.394540e-04\n",
            "step 4355 | loss: 0.256394 | lr: 1.393303e-04\n",
            "step 4356 | loss: 0.240262 | lr: 1.392066e-04\n",
            "step 4357 | loss: 0.212003 | lr: 1.390830e-04\n",
            "step 4358 | loss: 0.182829 | lr: 1.389594e-04\n",
            "step 4359 | loss: 0.194352 | lr: 1.388359e-04\n",
            "step 4360 | loss: 0.197728 | lr: 1.387125e-04\n",
            "step 4361 | loss: 0.161214 | lr: 1.385891e-04\n",
            "step 4362 | loss: 0.165321 | lr: 1.384657e-04\n",
            "step 4363 | loss: 0.169629 | lr: 1.383424e-04\n",
            "step 4364 | loss: 0.151886 | lr: 1.382192e-04\n",
            "step 4365 | loss: 0.173436 | lr: 1.380960e-04\n",
            "step 4366 | loss: 0.183509 | lr: 1.379729e-04\n",
            "step 4367 | loss: 0.204961 | lr: 1.378498e-04\n",
            "step 4368 | loss: 0.215449 | lr: 1.377268e-04\n",
            "step 4369 | loss: 0.177327 | lr: 1.376038e-04\n",
            "step 4370 | loss: 0.158384 | lr: 1.374809e-04\n",
            "step 4371 | loss: 0.169524 | lr: 1.373580e-04\n",
            "step 4372 | loss: 0.186534 | lr: 1.372352e-04\n",
            "step 4373 | loss: 0.164158 | lr: 1.371125e-04\n",
            "step 4374 | loss: 0.179529 | lr: 1.369898e-04\n",
            "step 4375 | loss: 0.224043 | lr: 1.368671e-04\n",
            "step 4376 | loss: 0.206556 | lr: 1.367446e-04\n",
            "step 4377 | loss: 0.209889 | lr: 1.366220e-04\n",
            "step 4378 | loss: 0.194203 | lr: 1.364995e-04\n",
            "step 4379 | loss: 0.173664 | lr: 1.363771e-04\n",
            "step 4380 | loss: 0.169778 | lr: 1.362547e-04\n",
            "step 4381 | loss: 0.163634 | lr: 1.361324e-04\n",
            "step 4382 | loss: 0.141680 | lr: 1.360102e-04\n",
            "step 4383 | loss: 0.171322 | lr: 1.358879e-04\n",
            "step 4384 | loss: 0.201972 | lr: 1.357658e-04\n",
            "step 4385 | loss: 0.171985 | lr: 1.356437e-04\n",
            "step 4386 | loss: 0.162957 | lr: 1.355216e-04\n",
            "step 4387 | loss: 0.168915 | lr: 1.353996e-04\n",
            "step 4388 | loss: 0.147575 | lr: 1.352777e-04\n",
            "step 4389 | loss: 0.147520 | lr: 1.351558e-04\n",
            "step 4390 | loss: 0.150881 | lr: 1.350340e-04\n",
            "step 4391 | loss: 0.170108 | lr: 1.349122e-04\n",
            "step 4392 | loss: 0.138487 | lr: 1.347905e-04\n",
            "step 4393 | loss: 0.140539 | lr: 1.346688e-04\n",
            "step 4394 | loss: 0.220559 | lr: 1.345472e-04\n",
            "step 4395 | loss: 0.217746 | lr: 1.344257e-04\n",
            "step 4396 | loss: 0.160230 | lr: 1.343042e-04\n",
            "step 4397 | loss: 0.169253 | lr: 1.341827e-04\n",
            "step 4398 | loss: 0.208114 | lr: 1.340613e-04\n",
            "step 4399 | loss: 0.196810 | lr: 1.339400e-04\n",
            "step 4400 | loss: 0.203790 | lr: 1.338187e-04\n",
            "step 4401 | loss: 0.185059 | lr: 1.336975e-04\n",
            "step 4402 | loss: 0.213910 | lr: 1.335763e-04\n",
            "step 4403 | loss: 0.187606 | lr: 1.334552e-04\n",
            "step 4404 | loss: 0.190859 | lr: 1.333342e-04\n",
            "step 4405 | loss: 0.193742 | lr: 1.332132e-04\n",
            "step 4406 | loss: 0.205120 | lr: 1.330922e-04\n",
            "step 4407 | loss: 0.221993 | lr: 1.329713e-04\n",
            "step 4408 | loss: 0.151575 | lr: 1.328505e-04\n",
            "step 4409 | loss: 0.155771 | lr: 1.327297e-04\n",
            "step 4410 | loss: 0.209728 | lr: 1.326090e-04\n",
            "step 4411 | loss: 0.177170 | lr: 1.324883e-04\n",
            "step 4412 | loss: 0.191207 | lr: 1.323677e-04\n",
            "step 4413 | loss: 0.161129 | lr: 1.322471e-04\n",
            "step 4414 | loss: 0.192905 | lr: 1.321266e-04\n",
            "step 4415 | loss: 0.208349 | lr: 1.320062e-04\n",
            "step 4416 | loss: 0.178198 | lr: 1.318858e-04\n",
            "step 4417 | loss: 0.182123 | lr: 1.317654e-04\n",
            "step 4418 | loss: 0.153962 | lr: 1.316451e-04\n",
            "step 4419 | loss: 0.157216 | lr: 1.315249e-04\n",
            "step 4420 | loss: 0.126647 | lr: 1.314047e-04\n",
            "step 4421 | loss: 0.128509 | lr: 1.312846e-04\n",
            "step 4422 | loss: 0.173255 | lr: 1.311646e-04\n",
            "step 4423 | loss: 0.142139 | lr: 1.310446e-04\n",
            "step 4424 | loss: 0.134469 | lr: 1.309246e-04\n",
            "step 4425 | loss: 0.125936 | lr: 1.308047e-04\n",
            "step 4426 | loss: 0.158383 | lr: 1.306849e-04\n",
            "step 4427 | loss: 0.115791 | lr: 1.305651e-04\n",
            "step 4428 | loss: 0.116518 | lr: 1.304454e-04\n",
            "step 4429 | loss: 0.128623 | lr: 1.303257e-04\n",
            "step 4430 | loss: 0.121715 | lr: 1.302061e-04\n",
            "step 4431 | loss: 0.119018 | lr: 1.300865e-04\n",
            "step 4432 | loss: 0.149787 | lr: 1.299670e-04\n",
            "step 4433 | loss: 0.150848 | lr: 1.298476e-04\n",
            "step 4434 | loss: 0.188226 | lr: 1.297282e-04\n",
            "step 4435 | loss: 0.173535 | lr: 1.296089e-04\n",
            "step 4436 | loss: 0.178695 | lr: 1.294896e-04\n",
            "step 4437 | loss: 0.170689 | lr: 1.293704e-04\n",
            "step 4438 | loss: 0.143846 | lr: 1.292512e-04\n",
            "step 4439 | loss: 0.134614 | lr: 1.291321e-04\n",
            "step 4440 | loss: 0.144310 | lr: 1.290131e-04\n",
            "step 4441 | loss: 0.130973 | lr: 1.288941e-04\n",
            "step 4442 | loss: 0.162429 | lr: 1.287751e-04\n",
            "step 4443 | loss: 0.155204 | lr: 1.286562e-04\n",
            "step 4444 | loss: 0.172424 | lr: 1.285374e-04\n",
            "step 4445 | loss: 0.156837 | lr: 1.284187e-04\n",
            "step 4446 | loss: 0.158105 | lr: 1.282999e-04\n",
            "step 4447 | loss: 0.133310 | lr: 1.281813e-04\n",
            "step 4448 | loss: 0.109863 | lr: 1.280627e-04\n",
            "step 4449 | loss: 0.143663 | lr: 1.279442e-04\n",
            "step 4450 | loss: 0.178924 | lr: 1.278257e-04\n",
            "step 4451 | loss: 0.181846 | lr: 1.277072e-04\n",
            "step 4452 | loss: 0.160208 | lr: 1.275889e-04\n",
            "step 4453 | loss: 0.178061 | lr: 1.274706e-04\n",
            "step 4454 | loss: 0.204213 | lr: 1.273523e-04\n",
            "step 4455 | loss: 0.253889 | lr: 1.272341e-04\n",
            "step 4456 | loss: 0.181681 | lr: 1.271160e-04\n",
            "step 4457 | loss: 0.169509 | lr: 1.269979e-04\n",
            "step 4458 | loss: 0.142871 | lr: 1.268799e-04\n",
            "step 4459 | loss: 0.163264 | lr: 1.267619e-04\n",
            "step 4460 | loss: 0.156804 | lr: 1.266440e-04\n",
            "step 4461 | loss: 0.142069 | lr: 1.265261e-04\n",
            "step 4462 | loss: 0.166927 | lr: 1.264083e-04\n",
            "step 4463 | loss: 0.160167 | lr: 1.262906e-04\n",
            "step 4464 | loss: 0.127985 | lr: 1.261729e-04\n",
            "step 4465 | loss: 0.134379 | lr: 1.260553e-04\n",
            "step 4466 | loss: 0.127258 | lr: 1.259377e-04\n",
            "step 4467 | loss: 0.133546 | lr: 1.258202e-04\n",
            "step 4468 | loss: 0.119125 | lr: 1.257027e-04\n",
            "step 4469 | loss: 0.117057 | lr: 1.255853e-04\n",
            "step 4470 | loss: 0.136186 | lr: 1.254680e-04\n",
            "step 4471 | loss: 0.115879 | lr: 1.253507e-04\n",
            "step 4472 | loss: 0.125477 | lr: 1.252335e-04\n",
            "step 4473 | loss: 0.149886 | lr: 1.251163e-04\n",
            "step 4474 | loss: 0.127600 | lr: 1.249992e-04\n",
            "step 4475 | loss: 0.137645 | lr: 1.248822e-04\n",
            "step 4476 | loss: 0.120972 | lr: 1.247652e-04\n",
            "step 4477 | loss: 0.123900 | lr: 1.246482e-04\n",
            "step 4478 | loss: 0.140919 | lr: 1.245314e-04\n",
            "step 4479 | loss: 0.099630 | lr: 1.244145e-04\n",
            "\n",
            "âœ… Target loss 0.0999 reached at step 4479\n",
            "The Final loss is :  tensor(0.0996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "> Is to become her husband and her father:\n",
            "The which will I; not all so much for love\n",
            "As for another secret close intent,\n",
            "\n",
            "> !\n",
            "Cursed the blood that let this blood from hence!\n",
            "More direful hap betide that hated wretch,\n",
            "That makes us\n",
            "> , set down the corse; or, by Saint Paul,\n",
            "I'll make a corse of him that disobeys.\n",
            "\n",
            "G\n",
            "> .\n",
            "If thou delight to view thy heinous deeds,\n",
            "Behold this pattern of thy butcheries.\n",
            "O, gentlemen, see, see!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-sEZhaN79KD"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}